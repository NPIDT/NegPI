{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing steps\n",
    "This notebook presents the basic cross-validation for the model:\n",
    "1. Basic cleaning\n",
    "2. Hashtag decomposition\n",
    "3. Handling competitors' names\n",
    "4. Handling questions\n",
    "5. Adding bigrams\n",
    "6. Adding indicator on whether BA emitted the tweet or not\n",
    "7. Neighborhood building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrg-researchlab/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ark_tweet_nlp import CMUTweetTagger\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import pickle\n",
    "import nltk.data\n",
    "from multiprocessing import Pool\n",
    "import enchant\n",
    "from spellchecker import SpellChecker\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loading and formatting the data\"\"\"\n",
    "\n",
    "data = pd.read_csv('./labeled_ba.csv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Collapsing repetitions\"\"\"\n",
    "\n",
    "def collapse_repeat(s):\n",
    "    if len(s)<=2:\n",
    "        return s\n",
    "    else:\n",
    "        ind_trm = []\n",
    "        k = 0\n",
    "        while k <= len(s) - 2:\n",
    "            if s[k]==s[k+1]:\n",
    "                i = k + 2\n",
    "                while i<len(s) and s[i]==s[k]:\n",
    "                    ind_trm.append(i)\n",
    "                    i += 1\n",
    "                k = i\n",
    "            else:\n",
    "                k += 1\n",
    "        if len(ind_trm)==0:\n",
    "            return s\n",
    "        else:\n",
    "            return ''.join([s[i] for i in range(len(s)) if i not in ind_trm])\n",
    "        \n",
    "\"\"\"Handling special negative expressions\"\"\"\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "neg_pats_1 = {r'havent\\s': ' have not ', r'wouldnt\\s': ' would not ', r'couldnt\\s': ' could not ', r'cant\\s': ' can not ', r'wont\\s': ' will not ',\n",
    "              r'didnt\\s': ' did not ', r'dont\\s': ' do not ', r'shouldnt\\s': ' should not '}\n",
    "\n",
    "def handle_neg(text):\n",
    "    first = text.lower().replace(\"'ll\",' will').replace(\"'re\",' are').replace(\"'ve\", ' have').replace(\"'m\",' am').replace(\"'d\", ' would')\n",
    "    second = first.replace(\"haven't\",'havent').replace(\"wouldn't\",'wouldnt').replace(\"couldn't\",'couldnt').replace(\"can't\",\"cant\").replace(\"won't\",'wont').replace(\"didn't\",'didnt').replace(\"shouldn't\",'shouldnt').replace(\"don't\",'dont')\n",
    "    for key, value in neg_pats_1.items():\n",
    "        second = re.sub(key, value, second)\n",
    "    second = second.replace('wont',' will not').replace('dont',' do not').replace('cant',' can not').replace('havent',' have not').replace('wouldnt',' would not')\n",
    "    third = ' '.join([sent.capitalize() for sent in sent_tokenizer.tokenize(second)])\n",
    "    return third\n",
    "\n",
    "\"\"\"Building company designations\"\"\"\n",
    "companies = [['british','airways'],['iceland','air'],['vueling'],['easy','jet'],['air','canada'],['american','airways'],['united','airlines'],['qatar','airways'],\n",
    "             ['virgin','atlantic'],['qatar','airways'],['norwegian','air'],['virgin'],['qatar']]\n",
    "\n",
    "def generate_versions(name):\n",
    "    if len(list(name))>=2:\n",
    "        return [''.join(list(name)), '_'.join(list(name)), '@'+'_'.join(list(name)), '@'+''.join(list(name)), '#'+'_'.join(list(name)), '#'+''.join(list(name)),\n",
    "               ' '.join(list(name))]\n",
    "    else:\n",
    "        return ['@'+name[0],name[0],'#'+name[0]]\n",
    "\n",
    "ba = generate_versions(companies[0])\n",
    "\n",
    "comp_des = []\n",
    "for c in companies[1:]:\n",
    "    comp_des += generate_versions(c)\n",
    "    \n",
    "\"\"\"Function to replace British Airways indicators as well as its competitors' indicators (as far as we know from reading the tweets)\"\"\"\n",
    "pat_ba = re.compile('\\s(' + '|'.join(ba) + ')\\s')\n",
    "pat_comp = re.compile(r'\\s(' + '|'.join(comp_des) + ')\\s')\n",
    "\n",
    "def replace_ba(string):\n",
    "    ret_str = sent_tokenizer.tokenize(' '.join(re.sub(pat_ba, ' ba ', string.lower()+' ').replace('@ba','ba').replace('#ba','ba').split()))\n",
    "    ret_str = [sent.capitalize() for sent in ret_str]\n",
    "    return ' '.join(ret_str)\n",
    "            \n",
    "\n",
    "def replace_comp(string):\n",
    "    ret_str = sent_tokenizer.tokenize(' '.join(re.sub(pat_comp, ' no_ba ', string.lower()+' ').replace('@no_ba','no_ba').replace('#no_ba','no_ba').split()))\n",
    "    ret_str = [sent.capitalize() for sent in ret_str]\n",
    "    return ' '.join(ret_str)\n",
    "\n",
    "\"\"\"Preprocessing the data\"\"\"\n",
    "data['text'] = data.text.apply(collapse_repeat)\n",
    "data['text'] = data.text.apply(lambda x: x.replace('\"',' '))\n",
    "data['text'] = data.text.apply(replace_ba).apply(replace_comp)\n",
    "data['text'] = data.text.apply(handle_neg)\n",
    "data['text'] = data.text.apply(lambda x: x.replace(' i ', ' I '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Provenance from BA variable\"\"\"\n",
    "import re\n",
    "\n",
    "ba_pat = re.compile(\"\\^\\w+\")\n",
    "def find_ba_reply(text):\n",
    "    s = [m.start() for m in re.finditer(ba_pat, text)]\n",
    "    return(len(s)>0)\n",
    "\n",
    "data['from_ba'] = data.text.apply(find_ba_reply).apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f0543e94674173963f1ad07e33db5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"POS-tagging\"\"\"\n",
    "def tagg(x):\n",
    "    return(pd.DataFrame(CMUTweetTagger.runtagger_parse([x])[0], columns=['word','tag','score']))\n",
    "\n",
    "tag_text = []\n",
    "N = data.shape[0]\n",
    "p = Pool(30)\n",
    "\n",
    "for k in tqdm(range(N//500+1)):\n",
    "    tag_text += p.map(tagg, list(data.text.iloc[500*k:min(N,500*(k+1))]))\n",
    "    \n",
    "data['tag_df'] = pd.Series(tag_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashtag decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d52b0c4007044aa5baa40cda1648aba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Building corpus', max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384321ddc94e4cf0a92c7f888d6d5159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Removing concatenations of basics', max=12235), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507a28cfb5f04f8b934f2130774a022f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10187), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bccba83cce714ad98b681dba4e71ad4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2011), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4efb9f2e9ad54e388229ffef552436c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d95face686c4983ba6fb9dea69f3e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A total of 2424 hastags\n"
     ]
    }
   ],
   "source": [
    "\"\"\"1. Building corpus\"\"\"\n",
    "corpus = set()\n",
    "\n",
    "for k in tqdm(range(data.shape[0]), desc='Building corpus'):\n",
    "    local = data.tag_df.iloc[k]\n",
    "    for i in range(local.shape[0]):\n",
    "        if any(str(k) in local.word.iloc[i] for k in range(10)):\n",
    "            continue\n",
    "        if '#' in local.word.iloc[i]:\n",
    "            continue\n",
    "        if local.tag.iloc[i] in ['#','U','&',',','O','$','D','!','^','@']:\n",
    "            continue\n",
    "        if \"'\" in local.word.iloc[i]:\n",
    "            continue\n",
    "        if '_' in local.word.iloc[i]:\n",
    "            continue\n",
    "        w = re.split('\\W+', local.word.iloc[i].replace(\"n't\",\" not\"))\n",
    "        corpus.update(set(w))\n",
    "\n",
    "corpus.remove('')\n",
    "corpus = set([e for e in corpus])\n",
    "\n",
    "list_corpus = list(corpus)\n",
    "to_remove = []\n",
    "keepers = ['an','of','air','ways','lines','ing','lon','un','don',]\n",
    "\n",
    "for i in tqdm(range(len(list_corpus)), desc='Removing concatenations of basics'):\n",
    "    w_i = list_corpus[i]\n",
    "    if w_i in keepers or len(w_i)<3:\n",
    "        continue\n",
    "    for j in range(i+1, len(list_corpus)):\n",
    "        w_j = list_corpus[j]\n",
    "        if w_j in keepers:\n",
    "            continue\n",
    "        if w_i+w_j in corpus:\n",
    "            to_remove.append(w_i+w_j)\n",
    "        if w_j+w_i in corpus:\n",
    "            to_remove.append(w_j+w_i)\n",
    "            \n",
    "corpus = set([e.replace(\"'\",'') for e in corpus if len(e)>1])\n",
    "corpus.remove('baagain')\n",
    "\n",
    "\"\"\"Clean the corpus\"\"\"\n",
    "eng_dict = enchant.Dict('en')\n",
    "remove_from_corpus = set([w for w in corpus if not eng_dict.check(w)])\n",
    "\n",
    "spell = SpellChecker(distance=2)\n",
    "spell.word_frequency.load_words(list(corpus.difference(remove_from_corpus)))\n",
    "misspelled = spell.unknown(list(remove_from_corpus))\n",
    "\n",
    "corrected = {w: w for w in tqdm(corpus.difference(remove_from_corpus))}\n",
    "corrected.update({w: spell.correction(w) for w in tqdm(remove_from_corpus)})\n",
    "\n",
    "corrected['flyng'] = 'flying'\n",
    "corrected['bycott'] = 'boycott'\n",
    "corrected['flyng'] = 'flying'\n",
    "corrected['bycott'] = 'boycott'\n",
    "\n",
    "for k in tqdm(range(data.shape[0])):\n",
    "    local = data.tag_df.at[k]\n",
    "    local['word'] = local.word.apply(lambda x: corrected[x] if x in corrected.keys() else x)\n",
    "    data['tag_df'].at[k] = deepcopy(local)\n",
    "    \n",
    "\"\"\"Harvesting the hashtags\"\"\"\n",
    "hashtag_set = set()\n",
    "\n",
    "for k in tqdm(range(data.shape[0])):\n",
    "    local = data.tag_df.iloc[k]\n",
    "    for i in range(local.shape[0]):\n",
    "        if local.tag.iloc[i]=='#' or '#' in local.word.iloc[i]:\n",
    "            hashtag_set.add(local.word.iloc[i])\n",
    "            \n",
    "print('A total of {} hastags'.format(len(hashtag_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ce83bf97b943a48cc443d15a4f7048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Extracting valid splits', max=2424), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd1eaf1899d4557827c5bb6c1a0d4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Histogram of number of splits', max=2424), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counts histogram :\n",
      "1      940\n",
      "0      821\n",
      "2      327\n",
      "8       12\n",
      "10      23\n",
      "4       83\n",
      "5       18\n",
      "6       43\n",
      "9        5\n",
      "15       1\n",
      "30       6\n",
      "20      13\n",
      "3      102\n",
      "40       4\n",
      "12       6\n",
      "7        6\n",
      "14       3\n",
      "24       1\n",
      "16       1\n",
      "130      1\n",
      "18       3\n",
      "50       2\n",
      "60       2\n",
      "11       1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"2. Function to find the splits with only known words\"\"\"\n",
    "proper = ['british','airways','iceland','air','qatar','vueling','norwegian','air','american','canada','united','airlines','ryanair','easy','jet','virgin','atlantic']\n",
    "new_corpus = set(list(corrected.keys())).union(proper)\n",
    "for w in proper:\n",
    "    corrected[w] = w\n",
    "corrected['flyijng'] = 'flying'\n",
    "corrected['safair'] = 'safair'\n",
    "\n",
    "\n",
    "def valid_split(ht):\n",
    "    \n",
    "    if ht in new_corpus:\n",
    "        return [[ht]]\n",
    "    \n",
    "    components = [e for e in new_corpus if e in ht]\n",
    "    queue = [[ht]]\n",
    "    candidates = []\n",
    "    \n",
    "    while len(queue)>0:\n",
    "        # Dequeuing\n",
    "        next_c = []\n",
    "        base = queue[-1][:-1]\n",
    "        suite = queue[-1][-1]\n",
    "        del queue[-1]\n",
    "        \n",
    "        # Looking for news candidates\n",
    "        for c in components:\n",
    "            if c==suite[:len(c)] and len(c)<len(suite):\n",
    "                next_c.append(base + [suite[:len(c)], suite[len(c):]])\n",
    "                candidates.append(next_c[-1])\n",
    "            elif c==suite[:len(c)] and len(c)==len(suite):\n",
    "                # If no more characters in string add words\n",
    "                candidates.append(base + [suite[:len(c)]])\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        # Growing the queue\n",
    "        check = [e for e in next_c if not any(f not in new_corpus for f in e[:-1])]\n",
    "        queue += check\n",
    "        \n",
    "    return [[corrected[f] for f in e] for e in candidates if e[-1] in new_corpus]\n",
    "\n",
    "hashtag_list = list(hashtag_set)\n",
    "candidate_lists = [valid_split(e.replace('#','')) for e in tqdm(hashtag_list, desc='Extracting valid splits')]\n",
    "\n",
    "\"\"\"Dropping duplicates in candidates\"\"\"\n",
    "candidate_lists = [[list(a) for a in list(set([tuple(e) for e in f]))] for f in candidate_lists]\n",
    "\n",
    "\"\"\"Checking how many hashtags have more than one valid split\"\"\"\n",
    "counts = {}\n",
    "for k in tqdm(range(len(candidate_lists)), desc='Histogram of number of splits'):\n",
    "    try:\n",
    "        counts[len(candidate_lists[k])] += 1\n",
    "    except:\n",
    "        counts[len(candidate_lists[k])] = 1\n",
    "        \n",
    "print('Counts histogram :\\n{}'.format(pd.Series(counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ac31ee1093405f8a9c6017564d6bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Building the co-occurrence matrix', max=11684), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrg-researchlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n"
     ]
    }
   ],
   "source": [
    "\"\"\"3. Computing the cooccurrence matrix\"\"\"\n",
    "coocc = {}\n",
    "\n",
    "for k in tqdm(range(data.shape[0]), desc='Building the co-occurrence matrix'):\n",
    "    local = data.tag_df.iloc[k]\n",
    "    local = local[local.tag.isin(['#','U','&',',','O','$','D','!','^','@']).apply(lambda x: not x)]\n",
    "    local = local[local.word.apply(lambda x: '#' not in x)]\n",
    "    if local.shape[0]>1:\n",
    "        local = list(local.word)\n",
    "        for i in range(1, len(local)):\n",
    "            w1 = local[i-1]\n",
    "            w2 = local[i]\n",
    "            try:\n",
    "                coocc[w1][w2] += 1\n",
    "            except:\n",
    "                try:\n",
    "                    coocc[w1][w2] = 1\n",
    "                except:\n",
    "                    coocc[w1] = {w2: 1}\n",
    "                    \n",
    "coocc = pd.DataFrame(coocc).fillna(0.)\n",
    "relevant = list(corpus.intersection(list(coocc.index)))\n",
    "coocc = coocc.loc[relevant, relevant].fillna(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f638859c124ae4bab0af8cd081bca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Computing EV for candidates', max=2424), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"4. Computing External Value for the various configurations as the product of \n",
    "left-side value and right-side value\"\"\"\n",
    "\n",
    "dict_hdf_ev = {}\n",
    "for k in tqdm(range(len(hashtag_list)), desc='Computing EV for candidates'):\n",
    "    dict_hdf_ev[hashtag_list[k]] = {}\n",
    "    for e in candidate_lists[k]:\n",
    "        ev = []\n",
    "        for w in e:\n",
    "            try:\n",
    "                ev.append(coocc[w].sum() * coocc.loc[w].sum())\n",
    "            except:\n",
    "                ev.append(0.)\n",
    "        dict_hdf_ev[hashtag_list[k]][tuple(e)] = ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673189963712444daa174acb80ac345e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Computing the Mutual Information between characters', max=116…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrg-researchlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:32: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c247ec7ba444868576dbd2b052e034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Computing IV for candidates', max=2424), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"5. Computing Internal value for the various splits\"\"\"\n",
    "\n",
    "\"\"\"First, computing the Mutual Information scores of the splits\"\"\"\n",
    "\n",
    "mi_matrix = {}\n",
    "counts = {}\n",
    "\n",
    "for k in tqdm(range(data.shape[0]), desc='Computing the Mutual Information between characters'):\n",
    "    local = data.tag_df.iloc[k]\n",
    "    local = local[local.tag.isin(['#','U',',','$','!']).apply(lambda x: not x)]\n",
    "    local = local[local.word.apply(lambda x: '#' not in x)]\n",
    "    if local.shape[0]>1:\n",
    "        local = [w for w in list(local.word) if w!=''] \n",
    "        for i in range(1, len(local)):\n",
    "            c1 = local[i-1][-1]\n",
    "            c2 = local[i][0]\n",
    "            # Updating counts\n",
    "            try:\n",
    "                counts[c1] += 1\n",
    "            except:\n",
    "                counts[c1] = 1\n",
    "            # Updating follow ups\n",
    "            try:\n",
    "                mi_matrix[c1][c2] += 1\n",
    "            except:\n",
    "                try:\n",
    "                    mi_matrix[c1][c2] = 1\n",
    "                except:\n",
    "                    mi_matrix[c1] = {c2: 1}\n",
    "                            \n",
    "mi_matrix = pd.DataFrame(mi_matrix)\n",
    "mi_matrix = mi_matrix.loc[list(mi_matrix.index), list(mi_matrix.index)]\n",
    "mi_df = (mi_matrix + mi_matrix.T)\n",
    "mi_df.fillna(0., inplace=True)        \n",
    "\n",
    "\"\"\"Computing the probabilities\"\"\"\n",
    "base_proba = pd.Series(counts)\n",
    "pair_proba = mi_df/np.sum(base_proba)\n",
    "base_proba /= np.sum(base_proba)\n",
    "\n",
    "\"\"\"Computing Mutual Information\"\"\"\n",
    "mi_matrix = pair_proba * pd.DataFrame(1/np.array(base_proba).reshape((-1,1)).dot(np.array(base_proba).reshape((1,-1))), index=list(base_proba.index), columns=list(base_proba.index))\n",
    "mi_matrix.fillna(0., inplace=True)\n",
    "\n",
    "\n",
    "\"\"\"Second for every candidate split, we compute the Boundary scores\"\"\"\n",
    "dict_hdf_iv = {}\n",
    "for k in tqdm(range(len(hashtag_list)), desc='Computing IV for candidates'):\n",
    "    dict_hdf_iv[hashtag_list[k]] = {}\n",
    "    for e in candidate_lists[k]:\n",
    "        if len(e)<=1:\n",
    "            dict_hdf_iv[hashtag_list[k]][tuple(e)] = []\n",
    "        else:\n",
    "            iv = []\n",
    "            for i in range(1, len(e)):\n",
    "                iv.append(mi_matrix.loc[e[i-1][-1], e[i][0]])\n",
    "            dict_hdf_iv[hashtag_list[k]][tuple(e)] = iv\n",
    "            \n",
    "\n",
    "\"\"\"Computing the Word Rank score as the product of \n",
    "- the geometric mean of EV (intuition : high EV reflects high word importance in the \n",
    "cooccurrence graph)\n",
    "- a decreasing function of maximum IV (intuition : IV is large when split is very likely, \n",
    "large min IV indicates high probability of split validity)\"\"\"\n",
    "\n",
    "dict_EV = {}\n",
    "for ht in dict_hdf_ev.keys():\n",
    "    dict_EV[ht] = {}\n",
    "    if len(dict_hdf_ev[ht])>=1:\n",
    "        for c in dict_hdf_ev[ht]:\n",
    "            dict_EV[ht][c] = np.prod(dict_hdf_ev[ht][c])**(1./len(dict_hdf_ev[ht][c]))        \n",
    "\n",
    "dict_IV = {}\n",
    "for ht in dict_hdf_iv.keys():\n",
    "    dict_IV[ht] = {}\n",
    "    if len(dict_hdf_iv[ht])>=1:\n",
    "        for c in dict_hdf_iv[ht]:\n",
    "            if len(c)==1:\n",
    "                dict_IV[ht][c] = 100.\n",
    "            else:\n",
    "                dict_IV[ht][c] = np.min(dict_hdf_iv[ht][c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358ab012ae6a49ae8b6ef430e15743ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2424), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"6. Given the results, and the large presence of small letters words due to the errors\n",
    "in tweets, we'll two criteria to select the best split :\n",
    "1. Largest IV\n",
    "2. In case of ties, choose the split with smallest number of tokens\"\"\"\n",
    "split_hashtags = {}\n",
    "\n",
    "for k in tqdm(range(len(hashtag_list))):\n",
    "    ht = hashtag_list[k]\n",
    "    if len(list(dict_IV[ht].keys()))==0:\n",
    "        split_hashtags[ht] = ht.replace('#','')\n",
    "    elif len(list(dict_IV[ht].keys()))==1:\n",
    "        split_hashtags[ht] = ', '.join(list(dict_IV[ht].keys())[0])\n",
    "    else:\n",
    "        loc_dict = dict_IV[ht]\n",
    "        loc_iv = {', '.join(c): loc_dict[c] for c in loc_dict.keys()}\n",
    "        loc_iv_df = pd.DataFrame([pd.Series(loc_iv),\n",
    "                                  pd.Series([len(c.split(', ')) for c in pd.Series(loc_iv).index], index=pd.Series(loc_iv).index)],\n",
    "                                 index=['iv','len'])\n",
    "        loc_iv_df.sort_values(['iv','len'], ascending=[False,True], axis=1, inplace=True)\n",
    "        split_hashtags[ht] = loc_iv_df.columns[0]\n",
    "        \n",
    "\n",
    "\"\"\"Normalizing the hashtags and correcting the relevant ones\"\"\"\n",
    "#\"\"\"Correcting relevant hashtags\"\"\"\n",
    "split_hashtags['#ineverflywithbaagain'] = 'i, never, fly, with, ba, again'\n",
    "split_hashtags['#boycottryanair'] = 'boycott, no_ba'\n",
    "split_hashtags['#neverflybritish'] = 'never, fly, ba'\n",
    "split_hashtags['#boycottbritish_airways'] = 'boycott, ba'\n",
    "\n",
    "for key, value in split_hashtags.items():\n",
    "    \"\"\"Normalizing airlines\"\"\"\n",
    "    split_hashtags[key] = value.replace('british, airways','ba').replace('ryanair','no_ba').replace('americanairlines','no_ba').replace('vueling','no_ba').replace('iberia,','no_ba,').replace(' no,',' not,').replace('norwegian','no_ba').replace('virgin, atlantic', 'no_ba').replace('virgin,','no_ba,')\n",
    "    \"\"\"Normalizing verbs with negatives\"\"\"\n",
    "    split_hashtags[key] = split_hashtags[key].replace(' wont,',' will, not,').replace(' cant,',' can, not,').replace(' dont,',' do, not,').replace(' wouldnt,',' would, not,').replace(' shouldnt,',' should, not,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9336c7ba52d43d0942567ccd7712a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Tagging all the split hashtags\"\"\"\n",
    "hashtag_list = []\n",
    "tokenized_hashtags = []\n",
    "\n",
    "for ht in split_hashtags.keys():\n",
    "    hashtag_list.append(ht)\n",
    "    tokenized_hashtags.append(split_hashtags[ht].replace(',',''))\n",
    "    \n",
    "H = len(tokenized_hashtags)\n",
    "tagged_hashtags = []\n",
    "p = Pool(40)\n",
    "\n",
    "for k in tqdm(range(H//500+1)):\n",
    "    tagged_hashtags += p.map(tagg, tokenized_hashtags[500*k:min(H,500*(k+1))])\n",
    "\n",
    "tagged_hashtags = {hashtag_list[k]: tagged_hashtags[k] for k in range(len(hashtag_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776b9fae1e604d8eb9b1f97bfeddce1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Additional cleaning to remove some special characters\"\"\"\n",
    "for k in tqdm(range(data.shape[0])):\n",
    "    local = data.tag_df.iloc[k]\n",
    "    local['word'] = local['word'].apply(lambda x: x.replace('-','').replace('/','').replace('\\\\','').replace(\"'\",'').replace(' ','').lower())\n",
    "    data['tag_df'].at[k] = deepcopy(local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9602ee7ff64abc8c964d1fb557a9d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Collecting verbs, adverbs, and adjectives\"\"\"\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "unique_verbs = set()\n",
    "unique_adverbs = set()\n",
    "unique_adjectives = set()\n",
    "\n",
    "for k in tqdm(range(data.shape[0])):\n",
    "    local = data.tag_df.iloc[k]\n",
    "    local = local[local.word.apply(lambda x: x.count('-')<=1 and x.count('/')<=1 and not(any(str(k) in x for k in range(10))) and not('#' in x))]\n",
    "    unique_verbs.update(list(local[local.tag=='V'].word.apply(lambda x: lmtzr.lemmatize(x.replace('-','').replace('/','').replace('\\\\',''),'v'))))\n",
    "    unique_adverbs.update(list(local[local.tag=='R'].word.apply(lambda x: x.replace('-','').replace('/','').replace('\\\\',''))))\n",
    "    unique_adjectives.update(list(local[local.tag=='A'].word.apply(lambda x: x.replace('-','').replace('/','').replace('\\\\',''))))\n",
    "    \n",
    "\"\"\"Lemmatizing all verbs\"\"\"\n",
    "lem_verbs = {verb: lmtzr.lemmatize(verb, 'v') for verb in unique_verbs}\n",
    "lem_adj = {adj: lmtzr.lemmatize(adj, 'a') for adj in unique_adjectives}\n",
    "lem_adv = {adv: adv for adv in unique_adverbs}\n",
    "lem_words = dict()\n",
    "lem_words.update(lem_adv)\n",
    "lem_words.update(lem_adj)\n",
    "lem_words.update(lem_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7855c7c334624a828f70cd4a4ac949fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building the neighborhoods\"\"\"\n",
    "unique_corpus = unique_verbs.union(unique_adverbs).union(unique_adjectives)\n",
    "\n",
    "def build_neighborhood(local):\n",
    "    #Instantiating lemmatizer\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    \n",
    "    try:\n",
    "        #Removing non interesting tags, tweet specific tags (emojis, existentials, numbers, URLs, the &, punctuation, unknown, @ mentions, determinants)\n",
    "        local = local[local.word.apply(lambda x: x.count('-')<=1 and x.count('/')<=1 and not(any(str(k) in x for k in range(10))) and not('#' in x))]\n",
    "        local = local[local.tag.isin(['U','&',',','$','!','^','#']).apply(lambda x: not x)]\n",
    "        if local.shape[0]==0:\n",
    "            return([],[],[])\n",
    "        else: \n",
    "            #Lemmatizing the words to remove the verb and adverb tokens to be considered\n",
    "            local['word'] = local.word.apply(lambda x: x.replace('-',' ').replace('/',' ').replace('\\\\','').replace(\"'\",''))\n",
    "            #Extracting the verb and adverb patterns\n",
    "            local_words_5 = []\n",
    "            local_words_7 = []\n",
    "            local_words_9 = []\n",
    "            for i in range(local.shape[0]):\n",
    "                w = local.word.iloc[i]\n",
    "                if w in unique_corpus or local.tag.iloc[i] in ['V','R']:#,'N']:\n",
    "                    neighborhood_5 = local.iloc[max(0,i-2):min(local.shape[0],i+3)]\n",
    "                    neighborhood_7 = local.iloc[max(0,i-3):min(local.shape[0],i+4)]\n",
    "                    neighborhood_9 = local.iloc[max(0,i-4):min(local.shape[0],i+5)]\n",
    "                    local_words_5.append(list(neighborhood_5.word))\n",
    "                    local_words_7.append(list(neighborhood_7.word))\n",
    "                    local_words_9.append(list(neighborhood_9.word))\n",
    "                #If the word is a verb, add it to the bank of verbs\n",
    "                if (local.tag.iloc[i]=='V') and ('#' not in w) and ('&' not in w) and not(any(str(k) in w for k in range(10))) and ('-' not in w) and ('/' not in w):\n",
    "                    if type(w)==list:\n",
    "                        unique_verbs.update(set(w))\n",
    "                    else:\n",
    "                        unique_verbs.add(w)\n",
    "            return(local_words_5, local_words_7, local_words_9)\n",
    "    except:\n",
    "        return ([],[],[])\n",
    "\n",
    "neighborhoods = []\n",
    "p = Pool(40)\n",
    "for k in tqdm(range(N//500+1)):\n",
    "    neighborhoods += p.map(build_neighborhood, list(data.tag_df.iloc[500*k:min(500*(k+1),N)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb477bb2bc74065b3b04e60cf753875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26177bd444744da3a5dee9793338e33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ed293622c4447483d251970a4fb58f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "actual_words_5 = [[[lem_words[w.lower()] if w.lower() in lem_words.keys() else w.lower() for w in v] for v in neighborhoods[k][0]] for k in tqdm(range(len(neighborhoods)))] \n",
    "actual_words_7 = [[[lem_words[w.lower()] if w.lower() in lem_words.keys() else w.lower() for w in v] for v in neighborhoods[k][1]] for k in tqdm(range(len(neighborhoods)))]\n",
    "actual_words_9 = [[[lem_words[w.lower()] if w.lower() in lem_words.keys() else w.lower() for w in v] for v in neighborhoods[k][2]] for k in tqdm(range(len(neighborhoods)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec2d762df56436b86327acbacf7b9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8435bf340cc04f2b9e0e1ae5102cdfaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building hashtag neighbordhoods, and correcting some of them\"\"\"\n",
    "tagged_hashtags['#neveragainonba'] = pd.DataFrame([['never','again','on','ba'],['R','R','P','N'],[1.,1.,1.,1.]], index=['word','tag','score']).T\n",
    "tagged_hashtags['#boycottryanair'] = pd.DataFrame([['boycott','no_ba'],['R','N'],[1.,1.]], index=['word','tag','score']).T\n",
    "tagged_hashtags['#neverflybritish'] = pd.DataFrame([['never','fly','ba'],['R','V','N'],[1.,1.,1.]], index=['word','tag','score']).T\n",
    "tagged_hashtags['#ineverflywithbaagain'] = pd.DataFrame([['i','never','fly','with','ba','again'],['P','R','V','P','N','R'],[1.,1.,1.,1.,1.,1.]], index=['word','tag','score']).T\n",
    "\n",
    "hashtag_neighborhoods = []\n",
    "for k in tqdm(range(data.shape[0])):\n",
    "    local = data.tag_df.iloc[k]\n",
    "    local = local[(local.tag=='#') | (local.word.apply(lambda x: '#' in x))]\n",
    "    if local.shape[0]==0:\n",
    "        hashtag_neighborhoods.append([])\n",
    "    else:\n",
    "        local_list = []\n",
    "        for w in list(local.word):\n",
    "            try:\n",
    "                words = list(tagged_hashtags[w].word.apply(lambda x: x.replace(\"'\",'')))\n",
    "                words = [lmtzr.lemmatize(words[i], 'v') if tagged_hashtags[w].tag.iloc[i]=='V' else words[i] for i in range(len(words)) ]\n",
    "                local_list.append((', '.join(words).replace('dont,', 'do, not,').replace('wont,','will, not,')).split(','))\n",
    "            except:\n",
    "                pass\n",
    "        hashtag_neighborhoods.append(local_list)\n",
    "        \n",
    "hashtag_sents = []\n",
    "for k in tqdm(range(len(hashtag_neighborhoods))):\n",
    "    local = []\n",
    "    for e in hashtag_neighborhoods[k]:\n",
    "        local.append([w.replace(' ','') for w in e])\n",
    "    hashtag_sents.append(local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a177af1447407eb110351b54d6d710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a044e5737b640ca8ca27857acf0ae75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93387d49cc64eba88d251cba7d68db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building full representation\"\"\"\n",
    "full_representation = []\n",
    "for k in tqdm(range(data.shape[0])):\n",
    "    local = data.tag_df.iloc[k]\n",
    "    if local.shape[0]>0:\n",
    "        local['word'] = local.word.apply(lambda x: x.replace('-',' ').replace('/',' ').replace('\\\\','').replace(\"'\",''))\n",
    "        local['word'] = local.T.apply(lambda x: (lmtzr.lemmatize(x['word'].replace(\"'\",''), x['tag'].lower()) if x['tag'] in ['V','R','A','N'] else x['word'].lower()) if type(x['word'])==str else '')\n",
    "        full_representation.append(list(local.word) + ','.join([','.join(v) for v in hashtag_neighborhoods[k] if type(v)==str]).split(','))\n",
    "    else:\n",
    "        full_representation.append([])\n",
    "\n",
    "\"\"\"Building vector representations based on full representation\"\"\"\n",
    "vec_reps = []\n",
    "for k in tqdm(range(len(full_representation))):\n",
    "    loc = full_representation[k]\n",
    "    v = []\n",
    "    for w in loc:\n",
    "        t = w.replace('-',' ').replace('/',' ').replace('\\\\','')\n",
    "        try:\n",
    "            t = lem_words[t]\n",
    "            v.append(t)\n",
    "        except:\n",
    "            pass\n",
    "    vec_reps.append(deepcopy(v))\n",
    "    \n",
    "hashtag_sents = [[[lem_words[w.lower()] if w.lower() in lem_words.keys() else w.lower() for w in v] for v in hashtag_sents[k]] for k in tqdm(range(len(hashtag_sents)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Adding the hashtag neighborhoods to the first neighborhoods\"\"\"\n",
    "actual_words_5 = [actual_words_5[k] + hashtag_sents[k] for k in range(len(actual_words_5))]\n",
    "actual_words_7 = [actual_words_7[k] + hashtag_sents[k] for k in range(len(actual_words_7))]\n",
    "actual_words_9 = [actual_words_9[k] + hashtag_sents[k] for k in range(len(actual_words_9))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Saving the neighborhood representation as well as the full representation for further use\"\"\"\n",
    "pickle.dump((split_hashtags, tagged_hashtags), open('./hashtags_BA.b','wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(data, open('./processed_BA.b','wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump((actual_words_5, actual_words_7, actual_words_9), open('./neighborhoods_ba.b','wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(full_representation, open('./full_ba.b','wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(data['from_ba'], open('./from_ba.b','wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump((lem_verbs, lem_adj, lem_adv, lem_words, unique_verbs, unique_adverbs, unique_adjectives), open('./vocab_BA.b','wb'), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
