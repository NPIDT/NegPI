{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeighLoR notebook\n",
    "In this notebook, we evaluate the following models:\n",
    "1. NeighLoR\n",
    "2. Logistic regression\n",
    "3. NeighLoR + pretrained Word2Vec\n",
    "4. NeighLoR + custom Word2Vec\n",
    "5. Logistic regression + pretrained Word2Vec\n",
    "6. Logistic regression + custom Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "from scipy.special import logit, expit\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "from copy import deepcopy\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loading the representations and the data\"\"\"\n",
    "\n",
    "data = pickle.load(open('./processed_BA.b','rb'))\n",
    "actual_words_5, actual_words_7, actual_words_9 = pickle.load(open('./neighborhoods_ba.b','rb'))\n",
    "full_representation = pickle.load(open('./full_ba.b','rb'))\n",
    "lem_verbs, lem_adj, lem_adv, lem_words, unique_verbs, unique_adverbs, unique_adjectives = pickle.load(open('./vocab_BA.b','rb'))\n",
    "from_ba = pickle.load(open('./from_ba.b','rb'))\n",
    "data['from_ba'] = from_ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83572ea349b4b668ccb40019a1b2451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Feature extraction\"\"\"\n",
    "ys = list(data.pi)\n",
    "verb_cond = {}\n",
    "adverb_cond = {}\n",
    "adj_cond = {}\n",
    "verb_count = {}\n",
    "adverb_count = {}\n",
    "adj_count = {}\n",
    "\n",
    "unique_verb = list(lem_verbs.keys())\n",
    "unique_adv = list(set(list(lem_adv)).difference(unique_verbs))\n",
    "unique_adj = list(set(lem_adj).difference(unique_adverbs).difference(unique_verbs))\n",
    "\n",
    "for k in tqdm(range(data.shape[0])):\n",
    "    \"\"\"Loop over the tweets\"\"\"\n",
    "    y = ys[k]\n",
    "    local_pattern = full_representation[k]\n",
    "    visited = set()\n",
    "    for w in local_pattern:\n",
    "        if w.count('-')<=1 and w.count('/')<=1 and not(any(str(k) in w for k in range(10))) and not('#' in w):\n",
    "            w = w.replace('-',' ').replace('/',' ').replace('\\\\','')\n",
    "            if w in unique_verb:\n",
    "                try:\n",
    "                    verb_count[lem_verbs[w]] += 1\n",
    "                    verb_cond[lem_verbs[w]] += data.pi.iloc[k]\n",
    "                except:\n",
    "                    verb_count[lem_verbs[w]] = 1\n",
    "                    verb_cond[lem_verbs[w]] = data.pi.iloc[k]\n",
    "            elif w in unique_adv:\n",
    "                try:\n",
    "                    adverb_count[w] += 1\n",
    "                    adverb_cond[w] += data.pi.iloc[k]\n",
    "                except:\n",
    "                    adverb_count[w] = 1\n",
    "                    adverb_cond[w] = data.pi.iloc[k]\n",
    "            elif w in unique_adj:\n",
    "                try:\n",
    "                    adj_count[lem_adj[w]] += 1\n",
    "                    adj_cond[lem_adj[w]] += data.pi.iloc[k]\n",
    "                except:\n",
    "                    adj_count[lem_adj[w]] = 1\n",
    "                    adj_cond[lem_adj[w]] = data.pi.iloc[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Computing the probabilities\"\"\"\n",
    "verb_cond = pd.Series(verb_cond)\n",
    "adverb_cond = pd.Series(adverb_cond)\n",
    "adj_cond = pd.Series(adj_cond)\n",
    "verb_count = pd.Series(verb_count)\n",
    "adverb_count = pd.Series(adverb_count)\n",
    "adj_count = pd.Series(adj_count)\n",
    "\n",
    "verb_cond = verb_cond/verb_count\n",
    "adverb_cond = adverb_cond/adverb_count\n",
    "adj_cond = adj_cond/adj_count\n",
    "\n",
    "verb_cond.sort_values(ascending=False, inplace=True)\n",
    "adverb_cond.sort_values(ascending=False, inplace=True)\n",
    "adj_cond.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "verb_cond = pd.DataFrame([verb_cond, verb_count], index=['cond_proba','count']).T\n",
    "adverb_cond = pd.DataFrame([adverb_cond, adverb_count], index=['cond_proba','count']).T\n",
    "adj_cond = pd.DataFrame([adj_cond, adj_count], index=['cond_proba','count']).T\n",
    "\n",
    "verb_cond['lower_bound'] = (verb_cond['cond_proba'] - 1.96*np.sqrt(verb_cond['cond_proba']*(1-verb_cond['cond_proba'])/verb_cond['count'])).apply(lambda x: max(x,0))\n",
    "adverb_cond['lower_bound'] = (adverb_cond['cond_proba'] - 1.96*np.sqrt(adverb_cond['cond_proba']*(1-adverb_cond['cond_proba'])/adverb_cond['count'])).apply(lambda x: max(x,0))\n",
    "adj_cond['lower_bound'] = (adj_cond['cond_proba'] - 1.96*np.sqrt(adj_cond['cond_proba']*(1-adj_cond['cond_proba'])/adj_cond['count'])).apply(lambda x: max(x,0))\n",
    "\n",
    "verb_cond['cond_proba_no_pi'] = 1. - verb_cond['cond_proba']\n",
    "verb_cond['lower_bound_no_pi'] = verb_cond['cond_proba_no_pi'] - 1.96 * np.sqrt(verb_cond['cond_proba_no_pi'] * (1-verb_cond['cond_proba_no_pi'])/verb_cond['count'])\n",
    "\n",
    "adverb_cond['cond_proba_no_pi'] = 1. - adverb_cond['cond_proba']\n",
    "adverb_cond['lower_bound_no_pi'] = adverb_cond['cond_proba_no_pi'] - 1.96 * np.sqrt(adverb_cond['cond_proba_no_pi'] * (1-adverb_cond['cond_proba_no_pi'])/adverb_cond['count'])\n",
    "\n",
    "adj_cond['cond_proba_no_pi'] = 1. - adj_cond['cond_proba']\n",
    "adj_cond['lower_bound_no_pi'] = adj_cond['cond_proba_no_pi'] - 1.96 * np.sqrt(adj_cond['cond_proba_no_pi'] * (1-adj_cond['cond_proba_no_pi'])/adj_cond['count'])\n",
    "\n",
    "verb_cond = verb_cond[verb_cond['count']>=10]\n",
    "adverb_cond = adverb_cond[adverb_cond['count']>=10]\n",
    "adj_cond = adj_cond[adj_cond['count']>=10]\n",
    "\n",
    "verb_features_10 = list(verb_cond.sort_values('lower_bound', ascending=False).index[:verb_cond.shape[0]//10]) + list(verb_cond.sort_values('lower_bound_no_pi', ascending=False).index[:verb_cond.shape[0]//10])\n",
    "verb_features_20 = list(verb_cond.sort_values('lower_bound', ascending=False).index[:verb_cond.shape[0] * 2//10]) + list(verb_cond.sort_values('lower_bound_no_pi', ascending=False).index[:verb_cond.shape[0] * 2//10])\n",
    "verb_features_30 = list(verb_cond.sort_values('lower_bound', ascending=False).index[:verb_cond.shape[0] * 3//10]) + list(verb_cond.sort_values('lower_bound_no_pi', ascending=False).index[:verb_cond.shape[0] * 3//10])\n",
    "\n",
    "adverb_features_10 = list(adverb_cond.sort_values('lower_bound', ascending=False).index[:adverb_cond.shape[0]//10]) + list(adverb_cond.sort_values('lower_bound_no_pi', ascending=False).index[:adverb_cond.shape[0]//10])\n",
    "adverb_features_20 = list(adverb_cond.sort_values('lower_bound', ascending=False).index[:adverb_cond.shape[0] * 2//10]) + list(adverb_cond.sort_values('lower_bound_no_pi', ascending=False).index[:adverb_cond.shape[0] * 2//10])\n",
    "adverb_features_30 = list(adverb_cond.sort_values('lower_bound', ascending=False).index[:adverb_cond.shape[0] * 3//10]) + list(adverb_cond.sort_values('lower_bound_no_pi', ascending=False).index[:adverb_cond.shape[0] * 3//10])\n",
    "\n",
    "adj_features_10 = list(adj_cond.sort_values('lower_bound', ascending=False).index[:adj_cond.shape[0]//10]) + list(adj_cond.sort_values('lower_bound_no_pi', ascending=False).index[:adj_cond.shape[0]//10])\n",
    "adj_features_20 = list(adj_cond.sort_values('lower_bound', ascending=False).index[:adj_cond.shape[0] * 2//10]) + list(adj_cond.sort_values('lower_bound_no_pi', ascending=False).index[:adj_cond.shape[0] * 2//10])\n",
    "adj_features_30 = list(adj_cond.sort_values('lower_bound', ascending=False).index[:adj_cond.shape[0] * 3//10]) + list(adj_cond.sort_values('lower_bound_no_pi', ascending=False).index[:adj_cond.shape[0] * 3//10])\n",
    "\n",
    "\"\"\"Function to compute the one-hot encodings\"\"\"\n",
    "features_10 = verb_features_10 + adverb_features_10 + adj_features_10\n",
    "features_20 = verb_features_20 + adverb_features_20 + adj_features_20\n",
    "features_30 = verb_features_30 + adverb_features_30 + adj_features_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxLog:\n",
    "    \n",
    "    def __init__(self, d, fit_intercept=True, init_w=None, init_b=None, alpha=1e-3, epsilon=1e-2):\n",
    "        \"\"\"Defining the dimension and fit_intercept parameters\"\"\"\n",
    "        self.dim = d\n",
    "        self.grad = np.zeros(self.dim)\n",
    "        self.fit_intercept = fit_intercept\n",
    "        \n",
    "        \"\"\"Initializing the weights\"\"\"\n",
    "        self.w = 0.\n",
    "        if init_w is None:\n",
    "            self.w = np.random.normal(size=self.dim)\n",
    "        elif isinstance(init_w, np.ndarray) and init_w.shape[0]==self.dim:\n",
    "            self.w = init_w\n",
    "        else:\n",
    "            raise ValueError('Wrong dimension in intial parameters.')\n",
    "        \n",
    "        \"\"\"Initializing the bias\"\"\"\n",
    "        self.b = 0.\n",
    "        if fit_intercept:\n",
    "            if init_b is None:\n",
    "                self.b = np.random.normal()\n",
    "            elif isinstance(init_b, (float, np.float64)):\n",
    "                self.b = init_b\n",
    "            else:\n",
    "                raise ValueError('Bias initialization must be a float type.')\n",
    "                \n",
    "        \"\"\"Setting the learning rate\"\"\"\n",
    "        if isinstance(alpha, (float, np.float64)):\n",
    "            self.alpha = alpha\n",
    "        else:\n",
    "            raise ValueError('Learning rate must be a float type.')\n",
    "        \n",
    "        \"\"\"Setting the stopping criterion for the gradient descent\"\"\"\n",
    "        if isinstance(epsilon, (float, np.float64)):\n",
    "            self.epsilon = epsilon\n",
    "        else:\n",
    "            raise ValueError('Tolerance must be a float type.')\n",
    "        \n",
    "        \"\"\"Initializing error sequence\"\"\"\n",
    "        self.errors = []\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def check_dim(self, X):\n",
    "        \"\"\"Computing check on dimension of observations\"\"\"\n",
    "        dim_check = any(x.shape[1]!=self.dim for x in X)\n",
    "        if dim_check:\n",
    "            raise ValueError('Wrong dimension in observations')\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def check_attr(self, attr, message=''):\n",
    "        \"\"\"Function to check whether the instance has an attribute\"\"\"\n",
    "        if not hasattr(self, attr):\n",
    "            if message=='':\n",
    "                raise ValueError(\"Instance doesn't have attribute '{}'\".format(attr))\n",
    "            else:\n",
    "                raise ValueError(message)\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Computing check on dimension of observations\"\"\"\n",
    "        self.check_dim(X)\n",
    "            \n",
    "        \"\"\"Computing the predictions\"\"\"\n",
    "        pred = [np.max(expit(self.b + x.dot(self.w))) for x in X]\n",
    "        return np.array([int(p>=0.5) for p in pred])\n",
    "    \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Computing check on dimension of observations\"\"\"\n",
    "        self.check_dim(X)\n",
    "            \n",
    "        \"\"\"Computing the probability\"\"\"\n",
    "        pred = [np.max(expit(self.b + x.dot(self.w))) for x in X]\n",
    "        return np.array(pred)\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, parallel=False, n_iter=None):\n",
    "        \"\"\"Function to fit the model\n",
    "        Needs : compute the forward propagation, compute the backward propagation\"\"\"\n",
    "        \"\"\"Computing check on dimension of observations\"\"\"\n",
    "        self.check_dim(X)\n",
    "        \n",
    "        \"\"\"Initiating the error sequence\"\"\"\n",
    "        self.errors = []\n",
    "        self.grad = 10\n",
    "        \n",
    "        \"\"\"Performing the gradient descent\"\"\"\n",
    "        error = np.inf\n",
    "        if n_iter==None:\n",
    "            while error >= self.epsilon:\n",
    "                self.forward(X, y)\n",
    "                self.backward(X, y)\n",
    "                error = self.grad\n",
    "        else:\n",
    "            for k in tqdm(range(n_iter), desc='Fitting the model'):\n",
    "                self.forward(X, y)\n",
    "                self.backward(X, y)\n",
    "                if self.grad<=self.epsilon:\n",
    "                    break\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def forward(self, X, y):\n",
    "        \"\"\"Performing the forward pass of the model : registering the maximum indexes, the current proba scores, and the error\"\"\"\n",
    "        probas = [expit(self.b + x.dot(self.w)) for x in X]\n",
    "        self.max_ind = [np.argmax(v) for v in probas]\n",
    "        y_pred = np.array([np.max(v) for v in probas])\n",
    "        self.eta = y_pred\n",
    "        self.errors.append(- np.mean(np.array(y) * np.log(y_pred) + (1-np.array(y)) * np.log(1-y_pred)))\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        grad_w, grad_b = self.gradient(X, y)\n",
    "        self.w = self.w - self.alpha * grad_w\n",
    "        self.b = self.b - self.alpha * grad_b\n",
    "        self.grad = np.sqrt(np.sum(grad_w**2)+grad_b**2)\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def gradient(self, X, y):\n",
    "        \"\"\"Check on error and maximum index for gradient computation\"\"\"\n",
    "        self.check_attr('errors', message='Need to compute forward pass to have error')\n",
    "        self.check_attr('max_ind', message='Need to compute forward pass to have maximum index')\n",
    "        self.check_attr('eta', message='Need to compute forward pass to have current estimate probabilities')\n",
    "        \n",
    "        \"\"\"Computing the gradient for every observation\"\"\"\n",
    "        mi = self.max_ind\n",
    "        current_X = np.vstack([X[k][mi[k]].reshape((1,-1)) for k in range(len(X))])\n",
    "        grad_w = current_X.T.dot(self.eta - np.array(y))/current_X.shape[0]\n",
    "        grad_b = 0.\n",
    "        if self.fit_intercept:\n",
    "            grad_b = np.mean(self.eta - np.array(y))\n",
    "        return grad_w, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14cd0c81c6fa48d5a6ac96e686a5d009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc569a99efc45fa96b14251055ae4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ffb3a03258c4a7dba5bac6659af2c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528b026002224a24bc5449a88143de1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building representations\"\"\"\n",
    "d = len(features_20)\n",
    "\n",
    "def represent(x):\n",
    "    return [int(w in x) for w in features_20]\n",
    "\n",
    "X_basic = [[represent(pattern) for pattern in tweet] for tweet in tqdm(actual_words_7)]\n",
    "X_basic = [np.array(tweet) for tweet in tqdm(X_basic)]\n",
    "\n",
    "for k in tqdm(range(len(X_basic))):\n",
    "    if X_basic[k].shape[0]==0:\n",
    "        X_basic[k] = np.zeros((1,d))\n",
    "        \n",
    "X_basic = [np.hstack([X_basic[k], data.from_ba.iloc[k] * np.ones((X_basic[k].shape[0],1))]) for k in tqdm(range(data.shape[0]))]\n",
    "features_20 = features_20 + ['from_ba']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eba326fedb9480e8acacd385c1818b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=24), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-60:\n",
      "Process ForkPoolWorker-52:\n",
      "Process ForkPoolWorker-69:\n",
      "Process ForkPoolWorker-48:\n",
      "Process ForkPoolWorker-75:\n",
      "Process ForkPoolWorker-53:\n",
      "Process ForkPoolWorker-80:\n",
      "Process ForkPoolWorker-59:\n",
      "Process ForkPoolWorker-62:\n",
      "Process ForkPoolWorker-41:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-56:\n",
      "Process ForkPoolWorker-54:\n",
      "Process ForkPoolWorker-68:\n",
      "Process ForkPoolWorker-57:\n",
      "Process ForkPoolWorker-65:\n",
      "Process ForkPoolWorker-76:\n",
      "Process ForkPoolWorker-78:\n",
      "Process ForkPoolWorker-61:\n",
      "Process ForkPoolWorker-64:\n",
      "Process ForkPoolWorker-47:\n",
      "Process ForkPoolWorker-42:\n",
      "Process ForkPoolWorker-49:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-43:\n",
      "Process ForkPoolWorker-44:\n",
      "Process ForkPoolWorker-51:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-79:\n",
      "Process ForkPoolWorker-45:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkPoolWorker-55:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkPoolWorker-50:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-77:\n",
      "Process ForkPoolWorker-67:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-58:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-63:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-66:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-46:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Process ForkPoolWorker-72:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Process ForkPoolWorker-73:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Process ForkPoolWorker-70:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-74:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Process ForkPoolWorker-71:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/qrg-researchlab/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Collecting the negative bigrams\"\"\"\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def get_neg_pairs(text):\n",
    "    doc = nlp(text)\n",
    "    dep_df = pd.DataFrame([[token.text, token.pos_, token.dep_, token.head.text, token.head.pos_] for token in doc], columns=['token','token_pos','dep','head','head_pos'])\n",
    "    neg_dep = dep_df[(dep_df.dep=='neg') & (dep_df.head_pos=='VERB')]\n",
    "    \n",
    "    from nltk import WordNetLemmatizer\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    pairs = [[neg_dep.token.iloc[i], lmtzr.lemmatize(neg_dep['head'].iloc[i], 'v')] for i in range(neg_dep.shape[0])]\n",
    "    return pairs\n",
    "\n",
    "p = Pool(40)\n",
    "\n",
    "neg_pairs_data = []\n",
    "N = data.shape[0]\n",
    "\n",
    "for k in tqdm(range(N//500+1)):\n",
    "    neg_pairs_data += p.map(get_neg_pairs, list(data.text.iloc[k*500:min(N,(k+1)*500)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of verb bigrams : 110\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Recording the verb bigrams\"\"\"\n",
    "verb_bigrams = {}\n",
    "for k in range(len(neg_pairs_data)):\n",
    "    for i in range(len(neg_pairs_data[k])):\n",
    "        if neg_pairs_data[k][i][1] in verb_features_30:\n",
    "            try:\n",
    "                verb_bigrams[neg_pairs_data[k][i][1]] += 1\n",
    "            except:\n",
    "                verb_bigrams[neg_pairs_data[k][i][1]] = 1\n",
    "\n",
    "verb_bigrams = pd.Series(verb_bigrams)\n",
    "verb_bigrams = verb_bigrams[verb_bigrams>=5]\n",
    "verb_bigrams = list(verb_bigrams.index)\n",
    "\n",
    "print(\"Number of verb bigrams : {}\".format(len(verb_bigrams)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78292c67c84c406cb3e8a32e0eb286da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building the negative bigrams representations for neighborhoods\"\"\"\n",
    "neg_reps = []\n",
    "neg_presence = []\n",
    "for k in tqdm(range(data.shape[0])):\n",
    "    nr = []\n",
    "    npr = []\n",
    "    \n",
    "    for i in range(len(actual_words_7[k])):\n",
    "        local = []\n",
    "        neighborhood = set(actual_words_7[k][i])\n",
    "        \n",
    "        for neg_pair in neg_pairs_data[k]:\n",
    "            if len(neighborhood.intersection(neg_pair))==2:\n",
    "                if neg_pair[1] in verb_bigrams:\n",
    "                    local.append(neg_pair[1])\n",
    "                if neg_pair[0] in verb_bigrams:\n",
    "                    local.append(neg_pair[0])\n",
    "        \n",
    "        if len(local)>0:\n",
    "            npr.append(1)\n",
    "        else:\n",
    "            npr.append(0)\n",
    "        nr.append(deepcopy(local))\n",
    "    \n",
    "    neg_reps.append(deepcopy(nr))\n",
    "    neg_presence.append(np.array(npr).reshape((len(actual_words_7[k]),1)))\n",
    "    \n",
    "neg_reps = [np.array([[int(v in local) for v in verb_bigrams] for local in tweet]) for tweet in neg_reps]\n",
    "\n",
    "\"\"\"Correcting the defects of negation representations\"\"\"\n",
    "d_neg = len(verb_bigrams)\n",
    "\n",
    "for k in range(len(neg_reps)):\n",
    "    if neg_reps[k].shape[0]==0:\n",
    "        neg_reps[k] = np.zeros((1,d_neg))\n",
    "    if neg_presence[k].shape[0]==0:\n",
    "        neg_presence[k] = np.zeros((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Complementing the representations for both methods of handling negations\"\"\"\n",
    "X_neg = [np.hstack([X_basic[k], neg_reps[k]]) for k in range(len(X_basic))]\n",
    "d = len(features_20) + len(verb_bigrams)\n",
    "feat = features_20 + ['not_' + w for w in verb_bigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the NeighLoR approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6522a5cb3b744a78b893635c331d328b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "2-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2f7a66f6394a8db3714f07ca72a0d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "3-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e981a040ae884d4caf82944a0589c3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "4-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9afe266cd7964561a61ebd6d2e948813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "5-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f9f9cf0ed5460fa5838fcb0fe48f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "6-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7453206d5404c0b90ba4a850c1acab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "7-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fcdbf6a06e242b4ba452422198c517a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "8-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc45b52308048da823b76847d0572ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "9-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43720f1f396d4ea1a62264684b32a848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "10-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87f598dea0b489fa8e703fff6fae0b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "Precision : 0.864574833040054 +- 0.029080479987617792\n",
      "Recall : 0.7517989629038692 +- 0.04295139434114574\n",
      "F1 : 0.8033715953618777 +- 0.027473547512926404\n",
      "ROC AUC : 0.9654022805806847 +- 0.01200859868767601\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building indexes\"\"\"\n",
    "n = len(ys)\n",
    "d = X_neg[0].shape[1]\n",
    "\n",
    "\"\"\"Retrieving the positive and negative indexes\"\"\"\n",
    "neg_index = [k for k in range(len(ys)) if ys[k]==0]\n",
    "pos_index = [k for k in range(len(ys)) if ys[k]==1]\n",
    "\n",
    "\"\"\"Shuffling both the positive and negative indexes\"\"\"\n",
    "np.random.seed(seed=0)\n",
    "np.random.shuffle(neg_index)\n",
    "np.random.shuffle(pos_index)\n",
    "\n",
    "\"\"\"Computing train and test index sets for the chosen number of folds\"\"\"\n",
    "cv = 10\n",
    "\n",
    "s = len(neg_index)//cv\n",
    "neg_index_list = [neg_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "neg_index_list.append(neg_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in neg_index_list])==len(neg_index)) # Check on negative index completion\n",
    "\n",
    "s = len(pos_index)//cv\n",
    "pos_index_list = [pos_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "pos_index_list.append(pos_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in pos_index_list])==len(pos_index))# Check on positive index completion\n",
    "\n",
    "\"\"\"Initializing the list of outputs, predictions and probabilities to computed CV-ly\"\"\"\n",
    "Y = ys\n",
    "Y_Pred = []\n",
    "Y_Proba = []\n",
    "\n",
    "\"\"\"Intializing the list of cross-validated features\"\"\"\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "roc_auc_list = []\n",
    "f1_list = []\n",
    "\n",
    "\"\"\"Storing coefficients and biases for stability evaluation\"\"\"\n",
    "biases = []\n",
    "weights = []\n",
    "\n",
    "\"\"\"Performing the cross-validation of the model using the features of interest\n",
    "A caveat to be mentioned is that the feature selection was performed on the whole dataset, which may be a little biased towards choosing the right features.\n",
    "This effect will be neglected during this test.\"\"\"\n",
    "for k in range(cv):\n",
    "    \"\"\"Splitting the data into train and test\"\"\"\n",
    "    print('{}-th fold'.format(k+1))\n",
    "    print('Splitting the data')\n",
    "    neg_test = [X_neg[i] for i in neg_index_list[k]]\n",
    "    pos_test = [X_neg[i] for i in pos_index_list[k]]\n",
    "    neg_train = [X_neg[i] for i in set(neg_index).difference(neg_index_list[k])]\n",
    "    pos_train = [X_neg[i] for i in set(pos_index).difference(pos_index_list[k])]\n",
    "\n",
    "    X_train = pos_train + neg_train\n",
    "    X_test = pos_test + neg_test\n",
    "\n",
    "    y_train = [1] * len(pos_train) + [0] * len(neg_train)\n",
    "    y_test = [1] * len(pos_test) + [0] * len(neg_test)\n",
    "\n",
    "    \"\"\"Fitting the model\"\"\"\n",
    "    print('Fitting the model')\n",
    "    d = X_train[0].shape[1]\n",
    "    model = MaxLog(d=d, fit_intercept=True, alpha=10., epsilon=1e-4, init_b=logit(np.mean(y_train)), init_w=np.zeros(d))\n",
    "    model.fit(X_train, y_train, n_iter=3000, parallel=False)\n",
    "\n",
    "    print('Evaluation and storage of model parameters\\n')\n",
    "    \"\"\"Outputting the predictions and the probability scores\"\"\"\n",
    "    y_score = model.predict_proba(X_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    \"\"\"Computing the various metrics\"\"\"\n",
    "    f1_list.append(f1_score(y_test, y_pred))\n",
    "    precision_list.append(precision_score(y_test, y_pred))\n",
    "    recall_list.append(recall_score(y_test, y_pred))\n",
    "    roc_auc_list.append(roc_auc_score(y_test, y_score))\n",
    "\n",
    "    \"\"\"Adding predictions and scores to computed global cross-validated performance after the end of the process\"\"\"\n",
    "    Y_Pred.append(list(y_pred))\n",
    "    Y_Proba.append(list(y_score))\n",
    "\n",
    "    \"\"\"Storing the model's coefficients\"\"\"\n",
    "    biases.append(model.b)\n",
    "    weights.append(model.w)\n",
    "\n",
    "print(\"Cross-validation evaluation of the NeighLoR approach:\")\n",
    "print(\"Precision : {} +- {}\\nRecall : {} +- {}\\nF1 : {} +- {}\\nROC AUC : {} +- {}\".format(np.mean(precision_list), np.std(precision_list),\n",
    "                                                                                          np.mean(recall_list), np.std(recall_list),\n",
    "                                                                                          np.mean(f1_list), np.std(f1_list),\n",
    "                                                                                          np.mean(roc_auc_list), np.std(roc_auc_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d242a1840e3b4cbc9b58b27af31d2b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Training the model used to evaluate the out-of-sample data\"\"\"\n",
    "ys = list(data.pi)\n",
    "d = X_neg[0].shape[1]\n",
    "model = MaxLog(d=d, fit_intercept=True, alpha=10., epsilon=1e-4, init_b=logit(np.mean(ys)), init_w=np.zeros(d))\n",
    "model.fit(X_neg, ys, n_iter=5000)\n",
    "pickle.dump((model, feat), open(\"./BA-final-model.b\",\"wb\"), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, feat = pickle.load(open(\"./BA-final-model.b\",'rb'))\n",
    "coef_ = pd.Series({feat[k]: model.w[k] for k in range(len(feat))}).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "boycott          7.365126\n",
       "again            4.346433\n",
       "avoid            4.117051\n",
       "hell             4.073102\n",
       "never            3.841027\n",
       "virgin           3.688061\n",
       "rip              3.606456\n",
       "future           3.372269\n",
       "fly              3.185931\n",
       "racist           2.794640\n",
       "consider         2.760598\n",
       "not_choose       2.672799\n",
       "unacceptable     2.642037\n",
       "bother           2.586675\n",
       "trust            2.584697\n",
       "extremely        2.569666\n",
       "not_buy          2.520290\n",
       "terrible         2.485585\n",
       "recommend        2.417858\n",
       "short            2.400888\n",
       "total            2.358891\n",
       "use              2.350799\n",
       "refuse           2.322920\n",
       "glad             2.213056\n",
       "book             2.091789\n",
       "highly           2.068966\n",
       "horrible         1.988287\n",
       "not_recommend    1.957959\n",
       "wonder           1.910302\n",
       "disgusting       1.871725\n",
       "                   ...   \n",
       "now             -1.806789\n",
       "lose            -1.817357\n",
       "as              -1.827593\n",
       "website         -1.854154\n",
       "cancel          -1.973457\n",
       "there           -2.012327\n",
       "any             -2.060053\n",
       "ship            -2.134463\n",
       "start           -2.169379\n",
       "back            -2.236521\n",
       "not_take        -2.258492\n",
       "bag             -2.293437\n",
       "get             -2.339011\n",
       "go              -2.357097\n",
       "not_see         -2.358896\n",
       "really          -2.364511\n",
       "out             -2.442984\n",
       "plus            -2.547089\n",
       "not_happen      -2.573976\n",
       "due             -2.595287\n",
       "shame           -2.631614\n",
       "even            -2.742600\n",
       "good            -2.771711\n",
       "then            -2.813794\n",
       "not_treat       -2.966698\n",
       "service         -2.975398\n",
       "not_have        -2.992504\n",
       "just            -3.580407\n",
       "tomorrow        -3.832828\n",
       "no_ba           -5.134327\n",
       "Length: 593, dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87073c6c47c4e5daba42b0727066f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7079eabc88c6451a816f22ff72e593ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrg-researchlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building global one-hot encoding representation\"\"\"\n",
    "feat_reps_1d = np.array([[int(w in fr) for w in features_20] for fr in tqdm(full_representation)])\n",
    "neg_reps_1d = np.array([[int(v in nr) for v in verb_bigrams] for nr in tqdm(neg_reps)])\n",
    "X_neg_1d = np.hstack([feat_reps_1d, neg_reps_1d, np.array(from_ba).reshape(-1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "2-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "3-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "4-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "5-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "6-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "7-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "8-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "9-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "10-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "Cross-validation evaluation for the logistic regression:\n",
      "Precision : 0.7677593029044705 +- 0.03576740436415514\n",
      "Recall : 0.6055364978061427 +- 0.043846922906361864\n",
      "F1 : 0.6761518807287112 +- 0.03371618552305579\n",
      "ROC AUC : 0.934314635717926 +- 0.01187045541047397\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building indexes\"\"\"\n",
    "n = len(ys)\n",
    "\n",
    "\"\"\"Retrieving the positive and negative indexes\"\"\"\n",
    "neg_index = [k for k in range(len(ys)) if ys[k]==0]\n",
    "pos_index = [k for k in range(len(ys)) if ys[k]==1]\n",
    "\n",
    "\"\"\"Shuffling both the positive and negative indexes\"\"\"\n",
    "np.random.seed(seed=0)\n",
    "np.random.shuffle(neg_index)\n",
    "np.random.shuffle(pos_index)\n",
    "\n",
    "\"\"\"Computing train and test index sets for the chosen number of folds\"\"\"\n",
    "cv = 10\n",
    "\n",
    "s = len(neg_index)//cv\n",
    "neg_index_list = [neg_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "neg_index_list.append(neg_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in neg_index_list])==len(neg_index)) # Check on negative index completion\n",
    "\n",
    "s = len(pos_index)//cv\n",
    "pos_index_list = [pos_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "pos_index_list.append(pos_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in pos_index_list])==len(pos_index))# Check on positive index completion\n",
    "\n",
    "\"\"\"Initializing the list of outputs, predictions and probabilities to computed CV-ly\"\"\"\n",
    "Y = ys\n",
    "Y_Pred = []\n",
    "Y_Proba = []\n",
    "\n",
    "\"\"\"Intializing the list of cross-validated features\"\"\"\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "roc_auc_list = []\n",
    "f1_list = []\n",
    "\n",
    "\"\"\"Storing coefficients and biases for stability evaluation\"\"\"\n",
    "biases = []\n",
    "weights = []\n",
    "\n",
    "\"\"\"Performing the cross-validation of the model using the features of interest\n",
    "A caveat to be mentioned is that the feature selection was performed on the whole dataset, which may be a little biased towards choosing the right features.\n",
    "This effect will be neglected during this test.\"\"\"\n",
    "for k in range(cv):\n",
    "    \"\"\"Splitting the data into train and test\"\"\"\n",
    "    print('{}-th fold'.format(k+1))\n",
    "    print('Splitting the data')\n",
    "    neg_test = [X_neg_1d[i:i+1,:] for i in neg_index_list[k]]\n",
    "    pos_test = [X_neg_1d[i:i+1,:] for i in pos_index_list[k]]\n",
    "    neg_train = [X_neg_1d[i:i+1,:] for i in set(neg_index).difference(neg_index_list[k])]\n",
    "    pos_train = [X_neg_1d[i:i+1,:] for i in set(pos_index).difference(pos_index_list[k])]\n",
    "\n",
    "    X_train = np.vstack(pos_train + neg_train)\n",
    "    X_test = np.vstack(pos_test + neg_test)\n",
    "\n",
    "    y_train = [1] * len(pos_train) + [0] * len(neg_train)\n",
    "    y_test = [1] * len(pos_test) + [0] * len(neg_test)\n",
    "\n",
    "    \"\"\"Fitting the model\"\"\"\n",
    "    print('Fitting the model')\n",
    "    model = LogisticRegression(C=1e4)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print('Evaluation and storage of model parameters\\n')\n",
    "    \"\"\"Outputting the predictions and the probability scores\"\"\"\n",
    "    y_score = model.predict_proba(X_test)[:,1]\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    \"\"\"Computing the various metrics\"\"\"\n",
    "    f1_list.append(f1_score(y_test, y_pred))\n",
    "    precision_list.append(precision_score(y_test, y_pred))\n",
    "    recall_list.append(recall_score(y_test, y_pred))\n",
    "    roc_auc_list.append(roc_auc_score(y_test, y_score))\n",
    "\n",
    "    \"\"\"Adding predictions and scores to computed global cross-validated performance after the end of the process\"\"\"\n",
    "    Y_Pred.append(list(y_pred))\n",
    "    Y_Proba.append(list(y_score))\n",
    "\n",
    "print(\"Cross-validation evaluation for the logistic regression:\")\n",
    "print(\"Precision : {} +- {}\\nRecall : {} +- {}\\nF1 : {} +- {}\\nROC AUC : {} +- {}\".format(np.mean(precision_list), np.std(precision_list),\n",
    "                                                                                          np.mean(recall_list), np.std(recall_list),\n",
    "                                                                                          np.mean(f1_list), np.std(f1_list),\n",
    "                                                                                          np.mean(roc_auc_list), np.std(roc_auc_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Google Word2Vec + Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9e9cfd87d541c8a506223b2e30b3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Loading W2V matrix and building representation\"\"\"\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "google_w2v = KeyedVectors.load_word2vec_format('/home/qrg-researchlab/Downloads/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "def get_embed(x):\n",
    "    local = list(x[x.tag.isin(['V','A','R','N'])].word)\n",
    "    embed = [google_w2v[w].reshape((1,-1)) for w in local if w in google_w2v.vocab.keys()]\n",
    "    if len(embed)>0:\n",
    "        return np.vstack(embed).sum(axis=0)\n",
    "    else:\n",
    "        return np.zeros(300)\n",
    "\n",
    "X_pt_w2v = np.vstack([get_embed(data.tag_df.iloc[k]).reshape((1,-1)) for k in tqdm(range(data.shape[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "2-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "3-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "4-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "5-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "6-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "7-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "8-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "9-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "10-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "Cross-validation evaluation of logistic regression + pretrained Word2Vec:\n",
      "Precision : 0.7128213150543405 +- 0.024473492026425483\n",
      "Recall : 0.4790666134822497 +- 0.03905194890370076\n",
      "F1 : 0.5720173070150341 +- 0.03017373266672795\n",
      "ROC AUC : 0.8924396738483743 +- 0.01467660613448336\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import expit, logit\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\"\"\"Building indexes\"\"\"\n",
    "n = len(ys)\n",
    "d = X_pt_w2v.shape[1]\n",
    "\n",
    "\"\"\"Retrieving the positive and negative indexes\"\"\"\n",
    "neg_index = [k for k in range(len(ys)) if ys[k]==0]\n",
    "pos_index = [k for k in range(len(ys)) if ys[k]==1]\n",
    "\n",
    "\"\"\"Shuffling both the positive and negative indexes\"\"\"\n",
    "np.random.seed(seed=0)\n",
    "np.random.shuffle(neg_index)\n",
    "np.random.shuffle(pos_index)\n",
    "\n",
    "\"\"\"Computing train and test index sets for the chosen number of folds\"\"\"\n",
    "cv = 10\n",
    "\n",
    "s = len(neg_index)//cv\n",
    "neg_index_list = [neg_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "neg_index_list.append(neg_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in neg_index_list])==len(neg_index)) # Check on negative index completion\n",
    "\n",
    "s = len(pos_index)//cv\n",
    "pos_index_list = [pos_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "pos_index_list.append(pos_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in pos_index_list])==len(pos_index))# Check on positive index completion\n",
    "\n",
    "\"\"\"Initializing the list of outputs, predictions and probabilities to computed CV-ly\"\"\"\n",
    "Y = ys\n",
    "Y_Pred = []\n",
    "Y_Proba = []\n",
    "\n",
    "\"\"\"Intializing the list of cross-validated features\"\"\"\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "roc_auc_list = []\n",
    "f1_list = []\n",
    "\n",
    "\"\"\"Performing the cross-validation of the model using the features of interest\n",
    "\"\"\"\n",
    "for k in range(cv):\n",
    "    \"\"\"Splitting the data into train and test\"\"\"\n",
    "    print('{}-th fold'.format(k+1))\n",
    "    print('Splitting the data')\n",
    "    neg_test = np.vstack([X_pt_w2v[i:i+1] for i in neg_index_list[k]])\n",
    "    pos_test = np.vstack([X_pt_w2v[i:i+1] for i in pos_index_list[k]])\n",
    "    neg_train = np.vstack([X_pt_w2v[i:i+1] for i in set(neg_index).difference(neg_index_list[k])])\n",
    "    pos_train = np.vstack([X_pt_w2v[i:i+1] for i in set(pos_index).difference(pos_index_list[k])])\n",
    "\n",
    "    X_train = np.vstack([pos_train, neg_train])\n",
    "    X_test = np.vstack([pos_test, neg_test])\n",
    "\n",
    "    y_train = [1] * len(pos_train) + [0] * len(neg_train)\n",
    "    y_test = [1] * len(pos_test) + [0] * len(neg_test)\n",
    "\n",
    "    \"\"\"Fitting the model\"\"\"\n",
    "    print('Fitting the model')\n",
    "    d = X_train.shape[1]\n",
    "    model = LogisticRegression(fit_intercept=True, C=1e4)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print('Evaluation and storage of model parameters\\n')\n",
    "    \"\"\"Outputting the predictions and the probability scores\"\"\"\n",
    "    y_score = model.predict_proba(X_test)[:,1]\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    \"\"\"Computing the various metrics\"\"\"\n",
    "    f1_list.append(f1_score(y_test, y_pred))\n",
    "    precision_list.append(precision_score(y_test, y_pred))\n",
    "    recall_list.append(recall_score(y_test, y_pred))\n",
    "    roc_auc_list.append(roc_auc_score(y_test, y_score))\n",
    "\n",
    "    \"\"\"Adding predictions and scores to computed global cross-validated performance after the end of the process\"\"\"\n",
    "    Y_Pred.append(list(y_pred))\n",
    "    Y_Proba.append(list(y_score))\n",
    "\n",
    "print(\"Cross-validation evaluation of logistic regression + pretrained Word2Vec:\")\n",
    "print(\"Precision : {} +- {}\\nRecall : {} +- {}\\nF1 : {} +- {}\\nROC AUC : {} +- {}\".format(np.mean(precision_list), np.std(precision_list),\n",
    "                                                                                          np.mean(recall_list), np.std(recall_list),\n",
    "                                                                                          np.mean(f1_list), np.std(f1_list),\n",
    "                                                                                          np.mean(roc_auc_list), np.std(roc_auc_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Word2Vec + NeighLoR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716f4ad954eb4aeca66fde683d7dddd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54dab8aa2c294e7cb915ee8654e83a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb933a485a043058920442053b3850b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Building representations for word embeddings\"\"\"\n",
    "d = 300\n",
    "\n",
    "def represent(x):\n",
    "    rep = []\n",
    "    for w in x:\n",
    "        try:\n",
    "            rep.append(google_w2v[w])\n",
    "        except:\n",
    "            pass\n",
    "    if len(rep)>0:\n",
    "        return np.vstack(rep).sum(axis=0)\n",
    "    else:\n",
    "        return np.zeros(300)\n",
    "\n",
    "X_basic = [[represent(pattern).reshape((1,-1)) for pattern in tweet] if len(tweet)>0 else np.zeros((1,300)) for tweet in tqdm(actual_words_7)]\n",
    "X_basic = [np.vstack(tweet) for tweet in tqdm(X_basic)]\n",
    "\n",
    "for k in tqdm(range(len(X_basic))):\n",
    "    if X_basic[k].shape[0]==0:\n",
    "        X_basic[k] = np.zeros((1,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7e0b86c7284fc59fd3c0e5107ef877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrg-researchlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:114: RuntimeWarning: divide by zero encountered in log\n",
      "/home/qrg-researchlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:114: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "2-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d4c7d1451442988739268fd9060c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "3-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94383aadb4547d18fa3e18b7a422c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "4-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f15a0b8b64490ba4f0613ce343b142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "5-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c7cce883b246a1ae6e1325435403eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "6-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20952512758a4ab69788bc2d46119cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "7-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa846a8dce834c91bdad019ebbc8e068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "8-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213df12ada294cdcb5d94aaeeedba4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "9-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f516e4f00148d0b1b910e3bc4f6cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "10-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff68b586f864c9293964c78cbee4d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "Cross-validation evaluation of the pretrained Word2Vec + NeighLoR approach:\n",
      "Precision : 0.7711294427745828 +- 0.15406778609165464\n",
      "Recall : 0.6301874750698047 +- 0.30800864781456283\n",
      "F1 : 0.603518495723111 +- 0.22403213065299948\n",
      "ROC AUC : 0.9373578827964302 +- 0.038660013566189505\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building indexes\"\"\"\n",
    "n = len(ys)\n",
    "d = X_basic[0].shape[1]\n",
    "\n",
    "\"\"\"Retrieving the positive and negative indexes\"\"\"\n",
    "neg_index = [k for k in range(len(ys)) if ys[k]==0]\n",
    "pos_index = [k for k in range(len(ys)) if ys[k]==1]\n",
    "\n",
    "\"\"\"Shuffling both the positive and negative indexes\"\"\"\n",
    "np.random.seed(seed=0)\n",
    "np.random.shuffle(neg_index)\n",
    "np.random.shuffle(pos_index)\n",
    "\n",
    "\"\"\"Computing train and test index sets for the chosen number of folds\"\"\"\n",
    "cv = 10\n",
    "\n",
    "s = len(neg_index)//cv\n",
    "neg_index_list = [neg_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "neg_index_list.append(neg_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in neg_index_list])==len(neg_index)) # Check on negative index completion\n",
    "\n",
    "s = len(pos_index)//cv\n",
    "pos_index_list = [pos_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "pos_index_list.append(pos_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in pos_index_list])==len(pos_index))# Check on positive index completion\n",
    "\n",
    "\"\"\"Cross-validation on 10% features, without hashtags\"\"\"\n",
    "from scipy.special import logit\n",
    "\n",
    "\"\"\"Initializing the list of outputs, predictions and probabilities to computed CV-ly\"\"\"\n",
    "Y = ys\n",
    "Y_Pred = []\n",
    "Y_Proba = []\n",
    "\n",
    "\"\"\"Intializing the list of cross-validated features\"\"\"\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "roc_auc_list = []\n",
    "f1_list = []\n",
    "\n",
    "\"\"\"Storing coefficients and biases for stability evaluation\"\"\"\n",
    "biases = []\n",
    "weights = []\n",
    "\n",
    "\"\"\"Performing the cross-validation of the model using the features of interest\n",
    "A caveat to be mentioned is that the feature selection was performed on the whole dataset, which may be a little biased towards choosing the right features.\n",
    "This effect will be neglected during this test.\"\"\"\n",
    "for k in range(cv):\n",
    "    \"\"\"Splitting the data into train and test\"\"\"\n",
    "    print('{}-th fold'.format(k+1))\n",
    "    print('Splitting the data')\n",
    "    neg_test = [X_basic[i] for i in neg_index_list[k]]\n",
    "    pos_test = [X_basic[i] for i in pos_index_list[k]]\n",
    "    neg_train = [X_basic[i] for i in set(neg_index).difference(neg_index_list[k])]\n",
    "    pos_train = [X_basic[i] for i in set(pos_index).difference(pos_index_list[k])]\n",
    "\n",
    "    X_train = pos_train + neg_train\n",
    "    X_test = pos_test + neg_test\n",
    "\n",
    "    y_train = [1] * len(pos_train) + [0] * len(neg_train)\n",
    "    y_test = [1] * len(pos_test) + [0] * len(neg_test)\n",
    "\n",
    "    \"\"\"Fitting the model\"\"\"\n",
    "    print('Fitting the model')\n",
    "    d = X_train[0].shape[1]\n",
    "    model = MaxLog(d=d, fit_intercept=True, alpha=10., epsilon=1e-4, init_b=logit(np.mean(y_train)), init_w=np.zeros(d))\n",
    "    model.fit(X_train, y_train, n_iter=3000, parallel=False)\n",
    "\n",
    "    print('Evaluation and storage of model parameters\\n')\n",
    "    \"\"\"Outputting the predictions and the probability scores\"\"\"\n",
    "    y_score = model.predict_proba(X_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    \"\"\"Computing the various metrics\"\"\"\n",
    "    f1_list.append(f1_score(y_test, y_pred))\n",
    "    precision_list.append(precision_score(y_test, y_pred))\n",
    "    recall_list.append(recall_score(y_test, y_pred))\n",
    "    roc_auc_list.append(roc_auc_score(y_test, y_score))\n",
    "\n",
    "    \"\"\"Adding predictions and scores to computed global cross-validated performance after the end of the process\"\"\"\n",
    "    Y_Pred.append(list(y_pred))\n",
    "    Y_Proba.append(list(y_score))\n",
    "\n",
    "    \"\"\"Storing the model's coefficients\"\"\"\n",
    "    biases.append(model.b)\n",
    "    weights.append(model.w)\n",
    "\n",
    "print(\"Cross-validation evaluation of the pretrained Word2Vec + NeighLoR approach:\")\n",
    "print(\"Precision : {} +- {}\\nRecall : {} +- {}\\nF1 : {} +- {}\\nROC AUC : {} +- {}\".format(np.mean(precision_list), np.std(precision_list),\n",
    "                                                                                          np.mean(recall_list), np.std(recall_list),\n",
    "                                                                                          np.mean(f1_list), np.std(f1_list),\n",
    "                                                                                          np.mean(roc_auc_list), np.std(roc_auc_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training custom Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1894a20b1ff24ec2be8f8c926a2e84fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=221), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05cae4ef67314e25a22ac972898c5a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=343480), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16dfb66a3b1b4fbaa7f241b111b96e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=343480), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrg-researchlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'word'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-57e382191e90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mlocal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mlocal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlem_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlem_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'word'"
     ]
    }
   ],
   "source": [
    "\"\"\"Loading large BA data to train our own specialized embedding\"\"\"\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "\"\"\"Loading British Airways data\"\"\"\n",
    "big_data = []\n",
    "\n",
    "for file in tqdm(os.listdir('../processed_data/british%20airways/')):\n",
    "    if file[-2:]=='.b':\n",
    "        big_data.append(pickle.load(open(os.path.join('../processed_data/british%20airways/', file), 'rb')))\n",
    "\n",
    "data_df = pd.concat(big_data, axis=0, sort=False)\n",
    "data_df.index = list(range(data_df.shape[0]))\n",
    "\n",
    "\"\"\"Collecting verbs, adverbs, and adjectives\"\"\"\n",
    "lmtzr = WordNetLemmatizer()\n",
    "unique_verbs = set()\n",
    "unique_adverbs = set()\n",
    "unique_adjectives = set()\n",
    "\n",
    "for k in tqdm(range(data_df.shape[0])):\n",
    "    local = data_df.tag_df.iloc[k]\n",
    "    try:\n",
    "        local = local[local.word.apply(lambda x: x.count('-')<=1 and x.count('/')<=1 and not(any(str(k) in x for k in range(10))) and not('#' in x))]\n",
    "        unique_verbs.update(list(local[local.tag=='V'].word.apply(lambda x: lmtzr.lemmatize(x.replace('-','').replace('/','').replace('\\\\',''),'v'))))\n",
    "        unique_adverbs.update(list(local[local.tag=='R'].word.apply(lambda x: x.replace('-','').replace('/','').replace('\\\\',''))))\n",
    "        unique_adjectives.update(list(local[local.tag=='A'].word.apply(lambda x: x.replace('-','').replace('/','').replace('\\\\',''))))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\"\"\"Lemmatizing all verbs\"\"\"\n",
    "lem_verbs = {verb: lmtzr.lemmatize(verb, 'v') for verb in unique_verbs}\n",
    "lem_adj = {adj: lmtzr.lemmatize(adj, 'a') for adj in unique_adjectives}\n",
    "lem_adv = {adv: adv for adv in unique_adverbs}\n",
    "lem_words = dict()\n",
    "lem_words.update(lem_adv)\n",
    "lem_words.update(lem_adj)\n",
    "lem_words.update(lem_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8ff574a7754e7693ba558181d1ffba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=343480), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "081322b98f244b1ea5061c50bb965c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=341435), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(107018275, 129195600)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Applying lemmatization to the text\"\"\"\n",
    "to_remove = []\n",
    "for k in tqdm(range(data_df.shape[0])):\n",
    "    local = data_df.tag_df.iloc[k]\n",
    "    try:\n",
    "        local['word'] = local.word.apply(lambda x: lem_words[x] if x in lem_words.keys() else x)\n",
    "        data_df.tag_df['word'] = deepcopy(local.word)\n",
    "    except:\n",
    "        to_remove.append(k)\n",
    "\n",
    "\"\"\"Training the embedding\"\"\"\n",
    "sentences = [list(data_df.tag_df.iloc[k][data_df.tag_df.iloc[k].tag.isin(['V','R','A','N'])].word) for k in tqdm(set(list(range(data_df.shape[0]))).difference(to_remove))]\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=50,\n",
    "                     alpha=0.05,\n",
    "                     min_alpha=0.001,\n",
    "                     negative=5,\n",
    "                     workers=40)\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Word2Vec + logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf4b94e14ef428cbcf08afa394f3bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrg-researchlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building the representation\"\"\"\n",
    "def get_embed(x):\n",
    "    local = list(x[x.tag.isin(['V','A','R','N'])].word)\n",
    "    embed = [w2v_model[w].reshape((1,-1)) for w in local if w in w2v_model.wv.vocab.keys()]\n",
    "    if len(embed)>0:\n",
    "        return np.vstack(embed).sum(axis=0)\n",
    "    else:\n",
    "        return np.zeros(50)\n",
    "\n",
    "X_pt_w2v = np.vstack([get_embed(data.tag_df.iloc[k]).reshape((1,-1)) for k in tqdm(range(data.shape[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "2-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "3-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "4-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "5-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "6-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "7-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "8-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "9-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "10-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "Cross-validation evaluation of custom Word2Vec + logistic regression:\n",
      "Precision : 0.7188268144720181 +- 0.046236661107451796\n",
      "Recall : 0.40711607499002794 +- 0.031795179993905645\n",
      "F1 : 0.5188489108535073 +- 0.030894057171317324\n",
      "ROC AUC : 0.8904383012661213 +- 0.0172568891411263\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building indexes\"\"\"\n",
    "n = len(ys)\n",
    "d = X_pt_w2v.shape[1]\n",
    "\n",
    "\"\"\"Retrieving the positive and negative indexes\"\"\"\n",
    "neg_index = [k for k in range(len(ys)) if ys[k]==0]\n",
    "pos_index = [k for k in range(len(ys)) if ys[k]==1]\n",
    "\n",
    "\"\"\"Shuffling both the positive and negative indexes\"\"\"\n",
    "np.random.seed(seed=0)\n",
    "np.random.shuffle(neg_index)\n",
    "np.random.shuffle(pos_index)\n",
    "\n",
    "\"\"\"Computing train and test index sets for the chosen number of folds\"\"\"\n",
    "cv = 10\n",
    "\n",
    "s = len(neg_index)//cv\n",
    "neg_index_list = [neg_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "neg_index_list.append(neg_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in neg_index_list])==len(neg_index)) # Check on negative index completion\n",
    "\n",
    "s = len(pos_index)//cv\n",
    "pos_index_list = [pos_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "pos_index_list.append(pos_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in pos_index_list])==len(pos_index))# Check on positive index completion\n",
    "\n",
    "\"\"\"Cross-validation on 10% features, without hashtags\"\"\"\n",
    "from scipy.special import logit\n",
    "\n",
    "\"\"\"Initializing the list of outputs, predictions and probabilities to computed CV-ly\"\"\"\n",
    "Y = ys\n",
    "Y_Pred = []\n",
    "Y_Proba = []\n",
    "\n",
    "\"\"\"Intializing the list of cross-validated features\"\"\"\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "roc_auc_list = []\n",
    "f1_list = []\n",
    "\n",
    "\"\"\"Performing the cross-validation of the model using the features of interest\n",
    "A caveat to be mentioned is that the feature selection was performed on the whole dataset, which may be a little biased towards choosing the right features.\n",
    "This effect will be neglected during this test.\"\"\"\n",
    "for k in range(cv):\n",
    "    \"\"\"Splitting the data into train and test\"\"\"\n",
    "    print('{}-th fold'.format(k+1))\n",
    "    print('Splitting the data')\n",
    "    neg_test = np.vstack([X_pt_w2v[i:i+1] for i in neg_index_list[k]])\n",
    "    pos_test = np.vstack([X_pt_w2v[i:i+1] for i in pos_index_list[k]])\n",
    "    neg_train = np.vstack([X_pt_w2v[i:i+1] for i in set(neg_index).difference(neg_index_list[k])])\n",
    "    pos_train = np.vstack([X_pt_w2v[i:i+1] for i in set(pos_index).difference(pos_index_list[k])])\n",
    "\n",
    "    X_train = np.vstack([pos_train, neg_train])\n",
    "    X_test = np.vstack([pos_test, neg_test])\n",
    "\n",
    "    y_train = [1] * len(pos_train) + [0] * len(neg_train)\n",
    "    y_test = [1] * len(pos_test) + [0] * len(neg_test)\n",
    "\n",
    "    \"\"\"Fitting the model\"\"\"\n",
    "    print('Fitting the model')\n",
    "    d = X_train.shape[1]\n",
    "    model = LogisticRegression(fit_intercept=True, C=1e4)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print('Evaluation and storage of model parameters\\n')\n",
    "    \"\"\"Outputting the predictions and the probability scores\"\"\"\n",
    "    y_score = model.predict_proba(X_test)[:,1]\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    \"\"\"Computing the various metrics\"\"\"\n",
    "    f1_list.append(f1_score(y_test, y_pred))\n",
    "    precision_list.append(precision_score(y_test, y_pred))\n",
    "    recall_list.append(recall_score(y_test, y_pred))\n",
    "    roc_auc_list.append(roc_auc_score(y_test, y_score))\n",
    "\n",
    "    \"\"\"Adding predictions and scores to computed global cross-validated performance after the end of the process\"\"\"\n",
    "    Y_Pred.append(list(y_pred))\n",
    "    Y_Proba.append(list(y_score))\n",
    "\n",
    "print(\"Cross-validation evaluation of custom Word2Vec + logistic regression:\")\n",
    "print(\"Precision : {} +- {}\\nRecall : {} +- {}\\nF1 : {} +- {}\\nROC AUC : {} +- {}\".format(np.mean(precision_list), np.std(precision_list),\n",
    "                                                                                          np.mean(recall_list), np.std(recall_list),\n",
    "                                                                                          np.mean(f1_list), np.std(f1_list),\n",
    "                                                                                          np.mean(roc_auc_list), np.std(roc_auc_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Word2Vec + NeighLoR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e90a89b2f6c4bf99cdcd84f5a5bc3b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrg-researchlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db581ba91e848e2be79c3fb6fb87e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4e115bafe544fc95fc5c1531016e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11684), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Building representations for word embeddings\"\"\"\n",
    "d = 50\n",
    "\n",
    "def represent(x):\n",
    "    rep = []\n",
    "    for w in x:\n",
    "        try:\n",
    "            rep.append(w2v_model[w])\n",
    "        except:\n",
    "            pass\n",
    "    if len(rep)>0:\n",
    "        return np.vstack(rep).sum(axis=0)\n",
    "    else:\n",
    "        return np.zeros(d)\n",
    "\n",
    "X_basic = [[represent(pattern).reshape((1,-1)) for pattern in tweet] if len(tweet)>0 else np.zeros((1,d)) for tweet in tqdm(actual_words_7)]\n",
    "X_basic = [np.vstack(X_basic[k]) for k in tqdm(range(len(X_basic)))]\n",
    "\n",
    "\n",
    "for k in tqdm(range(len(X_basic))):\n",
    "    if X_basic[k].shape[0]==0:\n",
    "        X_basic[k] = np.zeros((1,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36fa177e2d59444eabaa911738054190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrg-researchlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:114: RuntimeWarning: divide by zero encountered in log\n",
      "/home/qrg-researchlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:114: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "2-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8941b34598a4fbbaa53521ae5780240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "3-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8f1eb950fa4b8ab105d12cbfb8ab47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "4-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e169793f2f34213ac0ef8bb834b2f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "5-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51b533d70ac4524992a6d96211a68cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "6-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ce8fc9da624e26b60d08226a3fe110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "7-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d34418bb6674ffb9491873f0f6fe36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "8-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde99f385f8c4605ac8e19858d58504b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "9-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2838afedb09b4c80941348246034c485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "10-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada39a878bb84253ab099818d16ed19c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "Precision : 0.5005134923924911 +- 0.17389033353749067\n",
      "Recall : 0.3455684084563223 +- 0.26311095458875655\n",
      "F1 : 0.323154679297601 +- 0.1735324810461968\n",
      "ROC AUC : 0.7807655438904357 +- 0.06943205690523833\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import expit, logit\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\"\"\"Building indexes\"\"\"\n",
    "n = len(ys)\n",
    "d = X_basic[0].shape[1]\n",
    "\n",
    "\"\"\"Retrieving the positive and negative indexes\"\"\"\n",
    "neg_index = [k for k in range(len(ys)) if ys[k]==0]\n",
    "pos_index = [k for k in range(len(ys)) if ys[k]==1]\n",
    "\n",
    "\"\"\"Shuffling both the positive and negative indexes\"\"\"\n",
    "np.random.seed(seed=0)\n",
    "np.random.shuffle(neg_index)\n",
    "np.random.shuffle(pos_index)\n",
    "\n",
    "\"\"\"Computing train and test index sets for the chosen number of folds\"\"\"\n",
    "cv = 10\n",
    "\n",
    "s = len(neg_index)//cv\n",
    "neg_index_list = [neg_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "neg_index_list.append(neg_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in neg_index_list])==len(neg_index)) # Check on negative index completion\n",
    "\n",
    "s = len(pos_index)//cv\n",
    "pos_index_list = [pos_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "pos_index_list.append(pos_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in pos_index_list])==len(pos_index))# Check on positive index completion\n",
    "\n",
    "\"\"\"Cross-validation on 10% features, without hashtags\"\"\"\n",
    "from scipy.special import logit\n",
    "\n",
    "\"\"\"Initializing the list of outputs, predictions and probabilities to computed CV-ly\"\"\"\n",
    "Y = ys\n",
    "Y_Pred = []\n",
    "Y_Proba = []\n",
    "\n",
    "\"\"\"Intializing the list of cross-validated features\"\"\"\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "roc_auc_list = []\n",
    "f1_list = []\n",
    "\n",
    "\"\"\"Storing coefficients and biases for stability evaluation\"\"\"\n",
    "biases = []\n",
    "weights = []\n",
    "\n",
    "\"\"\"Performing the cross-validation of the model using the features of interest\n",
    "A caveat to be mentioned is that the feature selection was performed on the whole dataset, which may be a little biased towards choosing the right features.\n",
    "This effect will be neglected during this test.\"\"\"\n",
    "for k in range(cv):\n",
    "    \"\"\"Splitting the data into train and test\"\"\"\n",
    "    print('{}-th fold'.format(k+1))\n",
    "    print('Splitting the data')\n",
    "    neg_test = [X_basic[i] for i in neg_index_list[k]]\n",
    "    pos_test = [X_basic[i] for i in pos_index_list[k]]\n",
    "    neg_train = [X_basic[i] for i in set(neg_index).difference(neg_index_list[k])]\n",
    "    pos_train = [X_basic[i] for i in set(pos_index).difference(pos_index_list[k])]\n",
    "\n",
    "    X_train = pos_train + neg_train\n",
    "    X_test = pos_test + neg_test\n",
    "\n",
    "    y_train = [1] * len(pos_train) + [0] * len(neg_train)\n",
    "    y_test = [1] * len(pos_test) + [0] * len(neg_test)\n",
    "\n",
    "    \"\"\"Fitting the model\"\"\"\n",
    "    print('Fitting the model')\n",
    "    d = X_train[0].shape[1]\n",
    "    model = MaxLog(d=d, fit_intercept=True, alpha=10., epsilon=1e-4, init_b=logit(np.mean(y_train)), init_w=np.zeros(d))\n",
    "    model.fit(X_train, y_train, n_iter=3000, parallel=False)\n",
    "\n",
    "    print('Evaluation and storage of model parameters\\n')\n",
    "    \"\"\"Outputting the predictions and the probability scores\"\"\"\n",
    "    y_score = model.predict_proba(X_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    \"\"\"Computing the various metrics\"\"\"\n",
    "    f1_list.append(f1_score(y_test, y_pred))\n",
    "    precision_list.append(precision_score(y_test, y_pred))\n",
    "    recall_list.append(recall_score(y_test, y_pred))\n",
    "    roc_auc_list.append(roc_auc_score(y_test, y_score))\n",
    "\n",
    "    \"\"\"Adding predictions and scores to computed global cross-validated performance after the end of the process\"\"\"\n",
    "    Y_Pred.append(list(y_pred))\n",
    "    Y_Proba.append(list(y_score))\n",
    "\n",
    "    \"\"\"Storing the model's coefficients\"\"\"\n",
    "    biases.append(model.b)\n",
    "    weights.append(model.w)\n",
    "    \n",
    "print(\"Precision : {} +- {}\\nRecall : {} +- {}\\nF1 : {} +- {}\\nROC AUC : {} +- {}\".format(np.mean(precision_list), np.std(precision_list),\n",
    "                                                                                          np.mean(recall_list), np.std(recall_list),\n",
    "                                                                                          np.mean(f1_list), np.std(f1_list),\n",
    "                                                                                          np.mean(roc_auc_list), np.std(roc_auc_list)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
