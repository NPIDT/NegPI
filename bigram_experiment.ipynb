{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrg-researchlab/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/qrg-researchlab/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/qrg-researchlab/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/qrg-researchlab/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/qrg-researchlab/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxLog:\n",
    "    \n",
    "    def __init__(self, d, fit_intercept=True, init_w=None, init_b=None, alpha=1e-3, epsilon=1e-2):\n",
    "        \"\"\"Defining the dimension and fit_intercept parameters\"\"\"\n",
    "        self.dim = d\n",
    "        self.grad = np.zeros(self.dim)\n",
    "        self.fit_intercept = fit_intercept\n",
    "        \n",
    "        \"\"\"Initializing the weights\"\"\"\n",
    "        self.w = 0.\n",
    "        if init_w is None:\n",
    "            self.w = np.random.normal(size=self.dim)\n",
    "        elif isinstance(init_w, np.ndarray) and init_w.shape[0]==self.dim:\n",
    "            self.w = init_w\n",
    "        else:\n",
    "            raise ValueError('Wrong dimension in intial parameters.')\n",
    "        \n",
    "        \"\"\"Initializing the bias\"\"\"\n",
    "        self.b = 0.\n",
    "        if fit_intercept:\n",
    "            if init_b is None:\n",
    "                self.b = np.random.normal()\n",
    "            elif isinstance(init_b, (float, np.float64)):\n",
    "                self.b = init_b\n",
    "            else:\n",
    "                raise ValueError('Bias initialization must be a float type.')\n",
    "                \n",
    "        \"\"\"Setting the learning rate\"\"\"\n",
    "        if isinstance(alpha, (float, np.float64)):\n",
    "            self.alpha = alpha\n",
    "        else:\n",
    "            raise ValueError('Learning rate must be a float type.')\n",
    "        \n",
    "        \"\"\"Setting the stopping criterion for the gradient descent\"\"\"\n",
    "        if isinstance(epsilon, (float, np.float64)):\n",
    "            self.epsilon = epsilon\n",
    "        else:\n",
    "            raise ValueError('Tolerance must be a float type.')\n",
    "        \n",
    "        \"\"\"Initializing error sequence\"\"\"\n",
    "        self.errors = []\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def check_dim(self, X):\n",
    "        \"\"\"Computing check on dimension of observations\"\"\"\n",
    "        dim_check = any(x.shape[1]!=self.dim for x in X)\n",
    "        if dim_check:\n",
    "            raise ValueError('Wrong dimension in observations')\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def check_attr(self, attr, message=''):\n",
    "        \"\"\"Function to check whether the instance has an attribute\"\"\"\n",
    "        if not hasattr(self, attr):\n",
    "            if message=='':\n",
    "                raise ValueError(\"Instance doesn't have attribute '{}'\".format(attr))\n",
    "            else:\n",
    "                raise ValueError(message)\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Computing check on dimension of observations\"\"\"\n",
    "        self.check_dim(X)\n",
    "            \n",
    "        \"\"\"Computing the predictions\"\"\"\n",
    "        pred = [np.max(expit(self.b + x.dot(self.w))) for x in X]\n",
    "        return np.array([int(p>=0.5) for p in pred])\n",
    "    \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Computing check on dimension of observations\"\"\"\n",
    "        self.check_dim(X)\n",
    "            \n",
    "        \"\"\"Computing the probability\"\"\"\n",
    "        pred = [np.max(expit(self.b + x.dot(self.w))) for x in X]\n",
    "        return np.array(pred)\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, parallel=False, n_iter=None):\n",
    "        \"\"\"Function to fit the model\n",
    "        Needs : compute the forward propagation, compute the backward propagation\"\"\"\n",
    "        \"\"\"Computing check on dimension of observations\"\"\"\n",
    "        self.check_dim(X)\n",
    "        \n",
    "        \"\"\"Initiating the error sequence\"\"\"\n",
    "        self.errors = []\n",
    "        self.grad = 10\n",
    "        \n",
    "        \"\"\"Performing the gradient descent\"\"\"\n",
    "        error = np.inf\n",
    "        if n_iter==None:\n",
    "            while error >= self.epsilon:\n",
    "                self.forward(X, y)\n",
    "                self.backward(X, y)\n",
    "                error = self.grad\n",
    "        else:\n",
    "            for k in tqdm(range(n_iter), desc='Fitting the model'):\n",
    "                self.forward(X, y)\n",
    "                self.backward(X, y)\n",
    "                if self.grad<=self.epsilon:\n",
    "                    break\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def forward(self, X, y):\n",
    "        \"\"\"Performing the forward pass of the model : registering the maximum indexes, the current proba scores, and the error\"\"\"\n",
    "        probas = [expit(self.b + x.dot(self.w)) for x in X]\n",
    "        self.max_ind = [np.argmax(v) for v in probas]\n",
    "        y_pred = np.array([np.max(v) for v in probas])\n",
    "        self.eta = y_pred\n",
    "        self.errors.append(- np.mean(np.array(y) * np.log(y_pred) + (1-np.array(y)) * np.log(1-y_pred)))\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        grad_w, grad_b = self.gradient(X, y)\n",
    "        self.w = self.w - self.alpha * grad_w\n",
    "        self.b = self.b - self.alpha * grad_b\n",
    "        self.grad = np.sqrt(np.sum(grad_w**2)+grad_b**2)\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def gradient(self, X, y):\n",
    "        \"\"\"Check on error and maximum index for gradient computation\"\"\"\n",
    "        self.check_attr('errors', message='Need to compute forward pass to have error')\n",
    "        self.check_attr('max_ind', message='Need to compute forward pass to have maximum index')\n",
    "        self.check_attr('eta', message='Need to compute forward pass to have current estimate probabilities')\n",
    "        \n",
    "        \"\"\"Computing the gradient for every observation\"\"\"\n",
    "        mi = self.max_ind\n",
    "        current_X = np.vstack([X[k][mi[k]].reshape((1,-1)) for k in range(len(X))])\n",
    "        grad_w = current_X.T.dot(self.eta - np.array(y))/current_X.shape[0]\n",
    "        grad_b = 0.\n",
    "        if self.fit_intercept:\n",
    "            grad_b = np.mean(self.eta - np.array(y))\n",
    "        return grad_w, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e26fc8f4ef7446c963c93a8a3ef3bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11540), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Examining bigrams that are highly correlated with the negative PI class\"\"\"\n",
    "data = pickle.load(open('./label_tag_data.p','rb'))\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "bigram_count = {}\n",
    "bigram_class = {}\n",
    "\n",
    "for k in tqdm(range(data.shape[0])):\n",
    "    local = data.tag_df.iloc[k]\n",
    "    for i in range(local.shape[0]-1):\n",
    "        if local.tag.iloc[i] in ['P','V','R','A','N','@','D'] and local.tag.iloc[i+1] in ['P','V','R','A','N','@','D']: #or local.word.iloc[i] in ['not','ever','never']:\n",
    "            try:\n",
    "                bigram_count['_'.join(list(local.word.iloc[i:i+2].apply(lambda x: lmtzr.lemmatize(x.replace(\"'\",'').replace('-',''), 'v'))))] += 1\n",
    "                bigram_class['_'.join(list(local.word.iloc[i:i+2].apply(lambda x: lmtzr.lemmatize(x.replace(\"'\",'').replace('-',''), 'v'))))] += data.pi.iloc[k]\n",
    "            except KeyError:\n",
    "                bigram_count['_'.join(list(local.word.iloc[i:i+2].apply(lambda x: lmtzr.lemmatize(x.replace(\"'\",'').replace('-',''), 'v'))))] = 1\n",
    "                bigram_class['_'.join(list(local.word.iloc[i:i+2].apply(lambda x: lmtzr.lemmatize(x.replace(\"'\",'').replace('-',''), 'v'))))] = data.pi.iloc[k]\n",
    "                \n",
    "\"\"\"Computing conditional probabilities\"\"\"\n",
    "bigram_count = pd.Series(bigram_count)\n",
    "bigram_class = pd.Series(bigram_class)\n",
    "cond_proba = bigram_class/bigram_count\n",
    "proba_df = pd.DataFrame([bigram_count, cond_proba], index=['count','proba']).T\n",
    "proba_df['score_1'] = cond_proba - np.sqrt(cond_proba*(1-cond_proba)/bigram_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>proba</th>\n",
       "      <th>score_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>be_pst</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>daysno_resolution</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resolution_from</th>\n",
       "      <td>14.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seek_further</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>further_legal</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in_vacation</th>\n",
       "      <td>17.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baggage_in</th>\n",
       "      <td>16.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@alex_cruz_tweet</th>\n",
       "      <td>17.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>department_of</th>\n",
       "      <td>16.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>week_ruin</th>\n",
       "      <td>14.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vacation_week</th>\n",
       "      <td>14.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entire_vacation</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vacation_since</th>\n",
       "      <td>14.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@ba_again</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.963135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ba_again</th>\n",
       "      <td>60.0</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.901130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>never_again</th>\n",
       "      <td>139.0</td>\n",
       "      <td>0.920863</td>\n",
       "      <td>0.897966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of_transportation</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.876985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ever_fly</th>\n",
       "      <td>32.0</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.854723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>never_fly</th>\n",
       "      <td>316.0</td>\n",
       "      <td>0.844937</td>\n",
       "      <td>0.824575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>will_seek</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.822412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>again_will</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.822412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>legal_advice</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.822412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fly_this</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.822412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use_@ba</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.792320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_baggage</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.792320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ever_again</th>\n",
       "      <td>34.0</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.792202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airline_again</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.778896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>never_travel</th>\n",
       "      <td>36.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.771220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@ba_@alex_cruz</th>\n",
       "      <td>34.0</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.758151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>complaint_with</th>\n",
       "      <td>23.0</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.747053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_drink</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.341886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be_far</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.341886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to_charge</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.337854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be_awful</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.337854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be_fly</th>\n",
       "      <td>154.0</td>\n",
       "      <td>0.370130</td>\n",
       "      <td>0.331222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_complaint</th>\n",
       "      <td>62.0</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.325237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>crew_member</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.323274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be_appal</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.323274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leave_at</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.323274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_family</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.307784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be_rude</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.304414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your_company</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.304414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>after_my</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.304414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be_treat</th>\n",
       "      <td>31.0</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.299614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>@ba_never</th>\n",
       "      <td>40.0</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.298453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ruin_our</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.296311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_bag</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.296311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>an_infant</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.296311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be_use</th>\n",
       "      <td>56.0</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.293113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airline_be</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.292400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>their_customer</th>\n",
       "      <td>23.0</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.289541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to_avoid</th>\n",
       "      <td>32.0</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.289418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_company</th>\n",
       "      <td>26.0</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.289204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel_with</th>\n",
       "      <td>99.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.285955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with_@ba</th>\n",
       "      <td>211.0</td>\n",
       "      <td>0.317536</td>\n",
       "      <td>0.285488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with_an</th>\n",
       "      <td>27.0</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.277435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>another_airline</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.274981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fly_business</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.274348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_refund</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.274348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flight_due</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.273509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   count     proba   score_1\n",
       "be_pst              15.0  1.000000  1.000000\n",
       "daysno_resolution   13.0  1.000000  1.000000\n",
       "resolution_from     14.0  1.000000  1.000000\n",
       "seek_further        10.0  1.000000  1.000000\n",
       "further_legal       10.0  1.000000  1.000000\n",
       "in_vacation         17.0  1.000000  1.000000\n",
       "baggage_in          16.0  1.000000  1.000000\n",
       "@alex_cruz_tweet    17.0  1.000000  1.000000\n",
       "department_of       16.0  1.000000  1.000000\n",
       "week_ruin           14.0  1.000000  1.000000\n",
       "vacation_week       14.0  1.000000  1.000000\n",
       "entire_vacation     13.0  1.000000  1.000000\n",
       "vacation_since      14.0  1.000000  1.000000\n",
       "@ba_again           54.0  0.981481  0.963135\n",
       "ba_again            60.0  0.933333  0.901130\n",
       "never_again        139.0  0.920863  0.897966\n",
       "of_transportation   16.0  0.937500  0.876985\n",
       "ever_fly            32.0  0.906250  0.854723\n",
       "never_fly          316.0  0.844937  0.824575\n",
       "will_seek           11.0  0.909091  0.822412\n",
       "again_will          11.0  0.909091  0.822412\n",
       "legal_advice        11.0  0.909091  0.822412\n",
       "fly_this            11.0  0.909091  0.822412\n",
       "use_@ba             16.0  0.875000  0.792320\n",
       "no_baggage          16.0  0.875000  0.792320\n",
       "ever_again          34.0  0.852941  0.792202\n",
       "airline_again       15.0  0.866667  0.778896\n",
       "never_travel        36.0  0.833333  0.771220\n",
       "@ba_@alex_cruz      34.0  0.823529  0.758151\n",
       "complaint_with      23.0  0.826087  0.747053\n",
       "...                  ...       ...       ...\n",
       "a_drink             10.0  0.500000  0.341886\n",
       "be_far              10.0  0.500000  0.341886\n",
       "to_charge           15.0  0.466667  0.337854\n",
       "be_awful            15.0  0.466667  0.337854\n",
       "be_fly             154.0  0.370130  0.331222\n",
       "a_complaint         62.0  0.387097  0.325237\n",
       "crew_member         13.0  0.461538  0.323274\n",
       "be_appal            13.0  0.461538  0.323274\n",
       "leave_at            13.0  0.461538  0.323274\n",
       "a_family            19.0  0.421053  0.307784\n",
       "be_rude             11.0  0.454545  0.304414\n",
       "your_company        11.0  0.454545  0.304414\n",
       "after_my            11.0  0.454545  0.304414\n",
       "be_treat            31.0  0.387097  0.299614\n",
       "@ba_never           40.0  0.375000  0.298453\n",
       "ruin_our            14.0  0.428571  0.296311\n",
       "no_bag              14.0  0.428571  0.296311\n",
       "an_infant           14.0  0.428571  0.296311\n",
       "be_use              56.0  0.357143  0.293113\n",
       "airline_be          17.0  0.411765  0.292400\n",
       "their_customer      23.0  0.391304  0.289541\n",
       "to_avoid            32.0  0.375000  0.289418\n",
       "a_company           26.0  0.384615  0.289204\n",
       "travel_with         99.0  0.333333  0.285955\n",
       "with_@ba           211.0  0.317536  0.285488\n",
       "with_an             27.0  0.370370  0.277435\n",
       "another_airline     21.0  0.380952  0.274981\n",
       "fly_business        12.0  0.416667  0.274348\n",
       "no_refund           12.0  0.416667  0.274348\n",
       "flight_due          15.0  0.400000  0.273509\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba_df[proba_df['count']>=10].sort_values('score_1', ascending=False).iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loading the relevant features\"\"\"\n",
    "top_verbs_10 = pickle.load(open('./features_10/top_verbs_10.p','rb'))\n",
    "bot_verbs_10 = pickle.load(open('./features_10/bot_verbs_10.p','rb'))\n",
    "top_adverbs_10 = pickle.load(open('./features_10/top_adverbs_10.p','rb'))\n",
    "bot_adverbs_10 = pickle.load(open('./features_10/bot_adverbs_10.p','rb'))\n",
    "hashtag_words = pickle.load(open('./hashtags/hashtag_words.p','rb'))\n",
    "\n",
    "actual_words_5 = pickle.load(open('./neighborhoods/actual_words_hashtag_free_5.p','rb'))\n",
    "ys = list(data.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e69f652f4924db5a4849693e88afa9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11540), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building bigrams neighborhoods\"\"\"\n",
    "bigram_neighborhoods = []\n",
    "for k in tqdm(range(len(actual_words_5))):\n",
    "    bigram_neighborhoods.append([['_'.join(V[i:i+2]) for i in range(len(V)-1)] for V in actual_words_5[k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e08f39fe1ab485b923eb2d913806ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11540), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building one-hot encodings for bigram neighborhoods\"\"\"\n",
    "bigram_feat = list(proba_df[proba_df['count']>=10].sort_values('score_1', ascending=False).index[:100])\n",
    "ohe_bn = [np.array([[int(w in V) for w in bigram_feat] for V in N]) for N in tqdm(bigram_neighborhoods)]\n",
    "for k in range(len(ohe_bn)):\n",
    "    if ohe_bn[k].shape[0]==0:\n",
    "        ohe_bn[k] = np.zeros((1,len(bigram_feat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ccf981f9644e099df5eb2c599904d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11540), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building one-hot encodings for normal neighborhoods\"\"\"\n",
    "features = top_verbs_10 + bot_verbs_10 + top_adverbs_10 + bot_adverbs_10\n",
    "ohe_norm = [np.array([[int(w in V) for w in features] for V in N]) for N in tqdm(actual_words_5)]\n",
    "for k in range(len(ohe_norm)):\n",
    "    if ohe_norm[k].shape[0]==0:\n",
    "        ohe_norm[k] = np.zeros((1,len(features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704c8a850b74489888fda3010a4c5b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11540), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building one-hot encodings for hashtag neighborhoods\"\"\"\n",
    "features = top_verbs_10 + bot_verbs_10 + top_adverbs_10 + bot_adverbs_10\n",
    "ohe_hash = [np.array([[int(w in V) for w in features] for V in N]) for N in tqdm(hashtag_words)]\n",
    "for k in range(len(ohe_hash)):\n",
    "    if ohe_hash[k].shape[0]==0:\n",
    "        ohe_hash[k] = np.zeros((1,len(features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598f6bfdd92840dfa8511955270f856f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11540), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building bigrams' hashtags' neighborhoods\"\"\"\n",
    "bigram_hash = []\n",
    "for k in tqdm(range(len(actual_words_5))):\n",
    "    bigram_hash.append([['_'.join(V[i:i+2]) for i in range(len(V)-1)] for V in hashtag_words[k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877f5bd554ce4797a1b7a1ef5cbf7b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11540), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building one-hot encodings for bigram hashtags\"\"\"\n",
    "bigram_feat = list(proba_df[proba_df['count']>=10].sort_values('score_1', ascending=False).index[:100])\n",
    "ohe_bn_hash = [np.array([[int(w in V) for w in bigram_feat] for V in N]) for N in tqdm(bigram_hash)]\n",
    "for k in range(len(ohe_bn)):\n",
    "    if ohe_bn_hash[k].shape[0]==0:\n",
    "        ohe_bn_hash[k] = np.zeros((1,len(bigram_feat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86abe60fb8dd4aa08757615e2397a551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11540), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building full representation using both neighborhoods and bigram neighborhoods\"\"\"\n",
    "X_full = [np.vstack([np.hstack([ohe_norm[k], ohe_bn[k]]),\n",
    "                     np.hstack([ohe_hash[k], ohe_bn_hash[k]])]) for k in tqdm(range(data.shape[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Elementary check on dimensions\"\"\"\n",
    "assert(all(x.shape[1]==X_full[0].shape[1]) for x in X_full)\n",
    "print(X_full[0].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Building indexes\"\"\"\n",
    "\n",
    "n = len(ys)\n",
    "d = X_full[0].shape[1]\n",
    "\n",
    "\"\"\"Retrieving the positive and negative indexes\"\"\"\n",
    "neg_index = [k for k in range(len(ys)) if ys[k]==0]\n",
    "pos_index = [k for k in range(len(ys)) if ys[k]==1]\n",
    "\n",
    "\"\"\"Shuffling both the positive and negative indexes\"\"\"\n",
    "np.random.seed(seed=0)\n",
    "np.random.shuffle(neg_index)\n",
    "np.random.shuffle(pos_index)\n",
    "\n",
    "\"\"\"Computing train and test index sets for the chosen number of folds\"\"\"\n",
    "cv = 10\n",
    "\n",
    "s = len(neg_index)//cv\n",
    "neg_index_list = [neg_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "neg_index_list.append(neg_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in neg_index_list])==len(neg_index)) # Check on negative index completion\n",
    "\n",
    "s = len(pos_index)//cv\n",
    "pos_index_list = [pos_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "pos_index_list.append(pos_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in pos_index_list])==len(pos_index))# Check on positive index completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97dbfb46a90746039ed992b0663b6a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "2-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59970be7690b43898f90549df9e94688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "3-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708d2849479946d18133a6e9fe076acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "4-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b83af16c1644dc89c76f55b8b9add6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "5-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367a62dc0f2f4a1b82dc93ffb9d9b2ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "6-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21f93b1574243a98c28fddf3dc88674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "7-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29d3ddefe9545bb89d696b90425c427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "8-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf287c7551434088ab77737dca2d3f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "9-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37ad8df5f614cd69c312a0f7870185c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "10-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc220b58452c423495692b71c68f09d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "Average precision : 0.878211600819666\n",
      "Average recall : 0.7841934173100366\n",
      "Average F1 score : 0.8278022027707685\n",
      "Average ROC score : 0.9697050539953643\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Cross-validation on 10% features, with hashtags\"\"\"\n",
    "from scipy.special import logit, expit\n",
    "\n",
    "\"\"\"Initializing the list of outputs, predictions and probabilities to computed CV-ly\"\"\"\n",
    "Y = ys\n",
    "Y_Pred = []\n",
    "Y_Proba = []\n",
    "alpha = 10\n",
    "\n",
    "\"\"\"Intializing the list of cross-validated features\"\"\"\n",
    "precision_list_full = []\n",
    "recall_list_full = []\n",
    "roc_auc_list_full = []\n",
    "f1_list_full = []\n",
    "\n",
    "\"\"\"Storing coefficients and biases for stability evaluation\"\"\"\n",
    "biases_full = []\n",
    "weights_full = []\n",
    "\n",
    "\"\"\"Performing the cross-validation of the model using the features of interest\n",
    "A caveat to be mentioned is that the feature selection was performed on the whole dataset, which may be a little biased towards choosing the right features.\n",
    "This effect will be neglected during this test.\"\"\"\n",
    "for k in range(cv):\n",
    "    \"\"\"Splitting the data into train and test\"\"\"\n",
    "    print('{}-th fold'.format(k+1))\n",
    "    print('Splitting the data')\n",
    "    neg_test = [X_full[i] for i in neg_index_list[k]]\n",
    "    pos_test = [X_full[i] for i in pos_index_list[k]]\n",
    "    neg_train = [X_full[i] for i in set(neg_index).difference(neg_index_list[k])]\n",
    "    pos_train = [X_full[i] for i in set(pos_index).difference(pos_index_list[k])]\n",
    "\n",
    "    X_train = pos_train + neg_train\n",
    "    X_test = pos_test + neg_test\n",
    "\n",
    "    y_train = [1] * len(pos_train) + [0] * len(neg_train)\n",
    "    y_test = [1] * len(pos_test) + [0] * len(neg_test)\n",
    "\n",
    "    \"\"\"Fitting the model\"\"\"\n",
    "    print('Fitting the model')\n",
    "    d = X_train[0].shape[1]\n",
    "    model = MaxLog(d=d, fit_intercept=True, alpha=10., epsilon=1e-4, init_b=logit(np.mean(y_train)), init_w=np.zeros(d))\n",
    "    for k in tqdm(range(2000)):\n",
    "        model.forward(X_train, y_train)\n",
    "        grad_w, grad_b = model.gradient(X_train, y_train)\n",
    "        model.w -= alpha * grad_w\n",
    "        model.b -= alpha * grad_b\n",
    "        grad = np.sqrt(np.sum(grad_w**2)+grad_b**2)\n",
    "        if grad<=1e-4:\n",
    "            break\n",
    "\n",
    "\n",
    "    print('Evaluation and storage of model parameters\\n')\n",
    "    \"\"\"Outputting the predictions and the probability scores\"\"\"\n",
    "    y_score = model.predict_proba(X_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    \"\"\"Computing the various metrics\"\"\"\n",
    "    f1_list_full.append(f1_score(y_test, y_pred))\n",
    "    precision_list_full.append(precision_score(y_test, y_pred))\n",
    "    recall_list_full.append(recall_score(y_test, y_pred))\n",
    "    roc_auc_list_full.append(roc_auc_score(y_test, y_score))\n",
    "\n",
    "    \"\"\"Adding predictions and scores to computed global cross-validated performance after the end of the process\"\"\"\n",
    "    Y_Pred.append(list(y_pred))\n",
    "    Y_Proba.append(list(y_score))\n",
    "\n",
    "    \"\"\"Storing the model's coefficients\"\"\"\n",
    "    biases_full.append(model.b)\n",
    "    weights_full.append(model.w)\n",
    "\n",
    "print('Average precision : {}'.format(np.mean(precision_list_full)))\n",
    "print('Average recall : {}'.format(np.mean(recall_list_full)))\n",
    "print('Average F1 score : {}'.format(np.mean(f1_list_full)))\n",
    "print('Average ROC score : {}'.format(np.mean(roc_auc_list_full)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
