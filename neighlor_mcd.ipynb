{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeighLoR notebook\n",
    "In this notebook, we evaluate the following models on McDonald's data set:\n",
    "1. NeighLoR\n",
    "2. Logistic regression\n",
    "3. NeighLoR + pretrained Word2Vec\n",
    "4. NeighLoR + custom Word2Vec\n",
    "5. Logistic regression + pretrained Word2Vec\n",
    "6. Logistic regression + custom Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ark_tweet_nlp import CMUTweetTagger\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import multiprocessing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "import nltk.data\n",
    "from multiprocessing import Pool\n",
    "import enchant\n",
    "from spellchecker import SpellChecker\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "data = pickle.load(open('./mcd_data.b', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxLog:\n",
    "    \n",
    "    def __init__(self, d, fit_intercept=True, init_w=None, init_b=None, alpha=1e-3, epsilon=1e-2):\n",
    "        \"\"\"Defining the dimension and fit_intercept parameters\"\"\"\n",
    "        self.dim = d\n",
    "        self.grad = np.zeros(self.dim)\n",
    "        self.fit_intercept = fit_intercept\n",
    "        \n",
    "        \"\"\"Initializing the weights\"\"\"\n",
    "        self.w = 0.\n",
    "        if init_w is None:\n",
    "            self.w = np.random.normal(size=self.dim)\n",
    "        elif isinstance(init_w, np.ndarray) and init_w.shape[0]==self.dim:\n",
    "            self.w = init_w\n",
    "        else:\n",
    "            raise ValueError('Wrong dimension in intial parameters.')\n",
    "        \n",
    "        \"\"\"Initializing the bias\"\"\"\n",
    "        self.b = 0.\n",
    "        if fit_intercept:\n",
    "            if init_b is None:\n",
    "                self.b = np.random.normal()\n",
    "            elif isinstance(init_b, (float, np.float64)):\n",
    "                self.b = init_b\n",
    "            else:\n",
    "                raise ValueError('Bias initialization must be a float type.')\n",
    "                \n",
    "        \"\"\"Setting the learning rate\"\"\"\n",
    "        if isinstance(alpha, (float, np.float64)):\n",
    "            self.alpha = alpha\n",
    "        else:\n",
    "            raise ValueError('Learning rate must be a float type.')\n",
    "        \n",
    "        \"\"\"Setting the stopping criterion for the gradient descent\"\"\"\n",
    "        if isinstance(epsilon, (float, np.float64)):\n",
    "            self.epsilon = epsilon\n",
    "        else:\n",
    "            raise ValueError('Tolerance must be a float type.')\n",
    "        \n",
    "        \"\"\"Initializing error sequence\"\"\"\n",
    "        self.errors = []\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def check_dim(self, X):\n",
    "        \"\"\"Computing check on dimension of observations\"\"\"\n",
    "        dim_check = any(x.shape[1]!=self.dim for x in X)\n",
    "        if dim_check:\n",
    "            raise ValueError('Wrong dimension in observations')\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def check_attr(self, attr, message=''):\n",
    "        \"\"\"Function to check whether the instance has an attribute\"\"\"\n",
    "        if not hasattr(self, attr):\n",
    "            if message=='':\n",
    "                raise ValueError(\"Instance doesn't have attribute '{}'\".format(attr))\n",
    "            else:\n",
    "                raise ValueError(message)\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Computing check on dimension of observations\"\"\"\n",
    "        self.check_dim(X)\n",
    "            \n",
    "        \"\"\"Computing the predictions\"\"\"\n",
    "        pred = [np.max(expit(self.b + x.dot(self.w))) for x in X]\n",
    "        return np.array([int(p>=0.5) for p in pred])\n",
    "    \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Computing check on dimension of observations\"\"\"\n",
    "        self.check_dim(X)\n",
    "            \n",
    "        \"\"\"Computing the probability\"\"\"\n",
    "        pred = [np.max(expit(self.b + x.dot(self.w))) for x in X]\n",
    "        return np.array(pred)\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, parallel=False, n_iter=None):\n",
    "        \"\"\"Function to fit the model\n",
    "        Needs : compute the forward propagation, compute the backward propagation\"\"\"\n",
    "        \"\"\"Computing check on dimension of observations\"\"\"\n",
    "        self.check_dim(X)\n",
    "        \n",
    "        \"\"\"Initiating the error sequence\"\"\"\n",
    "        self.errors = []\n",
    "        self.grad = 10\n",
    "        \n",
    "        \"\"\"Performing the gradient descent\"\"\"\n",
    "        error = np.inf\n",
    "        if n_iter==None:\n",
    "            while error >= self.epsilon:\n",
    "                self.forward(X, y)\n",
    "                self.backward(X, y)\n",
    "                error = self.grad\n",
    "        else:\n",
    "            for k in tqdm(range(n_iter), desc='Fitting the model'):\n",
    "                self.forward(X, y)\n",
    "                self.backward(X, y)\n",
    "                if self.grad<=self.epsilon:\n",
    "                    break\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def forward(self, X, y):\n",
    "        \"\"\"Performing the forward pass of the model : registering the maximum indexes, the current proba scores, and the error\"\"\"\n",
    "        probas = [expit(self.b + x.dot(self.w)) for x in X]\n",
    "        self.max_ind = [np.argmax(v) for v in probas]\n",
    "        y_pred = np.array([np.max(v) for v in probas])\n",
    "        self.eta = y_pred\n",
    "        self.errors.append(- np.mean(np.array(y) * np.log(y_pred) + (1-np.array(y)) * np.log(1-y_pred)))\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        grad_w, grad_b = self.gradient(X, y)\n",
    "        self.w = self.w - self.alpha * grad_w\n",
    "        self.b = self.b - self.alpha * grad_b\n",
    "        self.grad = np.sqrt(np.sum(grad_w**2)+grad_b**2)\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def gradient(self, X, y):\n",
    "        \"\"\"Check on error and maximum index for gradient computation\"\"\"\n",
    "        self.check_attr('errors', message='Need to compute forward pass to have error')\n",
    "        self.check_attr('max_ind', message='Need to compute forward pass to have maximum index')\n",
    "        self.check_attr('eta', message='Need to compute forward pass to have current estimate probabilities')\n",
    "        \n",
    "        \"\"\"Computing the gradient for every observation\"\"\"\n",
    "        mi = self.max_ind\n",
    "        current_X = np.vstack([X[k][mi[k]].reshape((1,-1)) for k in range(len(X))])\n",
    "        grad_w = current_X.T.dot(self.eta - np.array(y))/current_X.shape[0]\n",
    "        grad_b = 0.\n",
    "        if self.fit_intercept:\n",
    "            grad_b = np.mean(self.eta - np.array(y))\n",
    "        return grad_w, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" McDonald 's is trash, in this case I mean literally. They are not worth the money. UberEats definitely needs to refund you.\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4b8e399b1b43ccae1c0088250ee12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23102), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data.rename(columns={'tagged_data':'tag_df'}, inplace=True)\n",
    "\n",
    "for k in tqdm(range(data.shape[0])):\n",
    "    local = data.tag_df.iloc[k]\n",
    "    local['word'] = local['word'].apply(lambda x: x.replace('-','').replace('/','').replace('\\\\','').replace(\"'\",'').replace(' ','').lower())\n",
    "    data['tag_df'].at[k] = deepcopy(local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f7d45dbcdd46828fa4094bff2f51ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23102), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Collecting verbs, adverbs, and adjectives\"\"\"\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "unique_verbs = set()\n",
    "unique_adverbs = set()\n",
    "unique_adjectives = set()\n",
    "\n",
    "for k in tqdm(range(data.shape[0])):\n",
    "    local = data.tag_df.iloc[k]\n",
    "    local = local[local.word.apply(lambda x: x.count('-')<=1 and x.count('/')<=1 and not(any(str(k) in x for k in range(10))) and not('#' in x))]\n",
    "    unique_verbs.update(list(local[local.tag=='V'].word.apply(lambda x: lmtzr.lemmatize(x.replace('-','').replace('/','').replace('\\\\',''),'v'))))\n",
    "    unique_adverbs.update(list(local[local.tag=='R'].word.apply(lambda x: x.replace('-','').replace('/','').replace('\\\\',''))))\n",
    "    unique_adjectives.update(list(local[local.tag=='A'].word.apply(lambda x: x.replace('-','').replace('/','').replace('\\\\',''))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Lemmatizing all verbs\"\"\"\n",
    "lem_verbs = {verb: lmtzr.lemmatize(verb, 'v') for verb in unique_verbs}\n",
    "lem_adj = {adj: lmtzr.lemmatize(adj, 'a') for adj in unique_adjectives}\n",
    "lem_adv = {adv: adv for adv in unique_adverbs}\n",
    "lem_words = dict()\n",
    "lem_words.update(lem_adv)\n",
    "lem_words.update(lem_adj)\n",
    "lem_words.update(lem_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886635e787dd4b73b4ff9664a43a1015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23102), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building full representation\"\"\"\n",
    "full_representation = []\n",
    "for k in tqdm(range(data.shape[0])):\n",
    "    local = data.tag_df.iloc[k]\n",
    "    if local.shape[0]>0:\n",
    "        local['word'] = local.word.apply(lambda x: x.replace('-',' ').replace('/',' ').replace('\\\\','').replace(\"'\",''))\n",
    "        local['word'] = local.T.apply(lambda x: (lmtzr.lemmatize(x['word'].replace(\"'\",''), x['tag'].lower()) if x['tag'] in ['V','R','A','N'] else x['word'].lower()) if type(x['word'])==str else '')\n",
    "        full_representation.append(list(local.word))\n",
    "    else:\n",
    "        full_representation.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c49feb8facc4f81a5e2a96dd47b7f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23102), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building vector representations based on full representation\"\"\"\n",
    "vec_reps = []\n",
    "for k in tqdm(range(len(full_representation))):\n",
    "    loc = full_representation[k]\n",
    "    v = []\n",
    "    for w in loc:\n",
    "        t = w.replace('-',' ').replace('/',' ').replace('\\\\','')\n",
    "        try:\n",
    "            t = lem_words[t]\n",
    "            v.append(t)\n",
    "        except:\n",
    "            pass\n",
    "    vec_reps.append(deepcopy(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5d2427763346e3b28c352c00135eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23102), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Feature extraction\"\"\"\n",
    "data['pi'] = data['neg_pi']\n",
    "ys = list(data.pi)\n",
    "verb_cond = {}\n",
    "adverb_cond = {}\n",
    "adj_cond = {}\n",
    "verb_count = {}\n",
    "adverb_count = {}\n",
    "adj_count = {}\n",
    "\n",
    "unique_verb = list(lem_verbs.keys())\n",
    "unique_adv = list(set(list(lem_adv)).difference(unique_verbs))\n",
    "unique_adj = list(set(lem_adj).difference(unique_adverbs).difference(unique_verbs))\n",
    "\n",
    "for k in tqdm(range(data.shape[0])):\n",
    "    \"\"\"Loop over the tweets\"\"\"\n",
    "    y = ys[k]\n",
    "    local_pattern = full_representation[k]\n",
    "    visited = set()\n",
    "    for w in local_pattern:\n",
    "        if w.count('-')<=1 and w.count('/')<=1 and not(any(str(k) in w for k in range(10))) and not('#' in w):\n",
    "            w = w.replace('-',' ').replace('/',' ').replace('\\\\','')\n",
    "            if w in unique_verb:\n",
    "                try:\n",
    "                    verb_count[lem_verbs[w]] += 1\n",
    "                    verb_cond[lem_verbs[w]] += data.pi.iloc[k]\n",
    "                except:\n",
    "                    verb_count[lem_verbs[w]] = 1\n",
    "                    verb_cond[lem_verbs[w]] = data.pi.iloc[k]\n",
    "            elif w in unique_adv:\n",
    "                try:\n",
    "                    adverb_count[w] += 1\n",
    "                    adverb_cond[w] += data.pi.iloc[k]\n",
    "                except:\n",
    "                    adverb_count[w] = 1\n",
    "                    adverb_cond[w] = data.pi.iloc[k]\n",
    "            elif w in unique_adj:\n",
    "                try:\n",
    "                    adj_count[lem_adj[w]] += 1\n",
    "                    adj_cond[lem_adj[w]] += data.pi.iloc[k]\n",
    "                except:\n",
    "                    adj_count[lem_adj[w]] = 1\n",
    "                    adj_cond[lem_adj[w]] = data.pi.iloc[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Computing the probabilities\"\"\"\n",
    "verb_cond = pd.Series(verb_cond)\n",
    "adverb_cond = pd.Series(adverb_cond)\n",
    "adj_cond = pd.Series(adj_cond)\n",
    "verb_count = pd.Series(verb_count)\n",
    "adverb_count = pd.Series(adverb_count)\n",
    "adj_count = pd.Series(adj_count)\n",
    "\n",
    "verb_cond = verb_cond/verb_count\n",
    "adverb_cond = adverb_cond/adverb_count\n",
    "adj_cond = adj_cond/adj_count\n",
    "\n",
    "verb_cond.sort_values(ascending=False, inplace=True)\n",
    "adverb_cond.sort_values(ascending=False, inplace=True)\n",
    "adj_cond.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "verb_cond = pd.DataFrame([verb_cond, verb_count], index=['cond_proba','count']).T\n",
    "adverb_cond = pd.DataFrame([adverb_cond, adverb_count], index=['cond_proba','count']).T\n",
    "adj_cond = pd.DataFrame([adj_cond, adj_count], index=['cond_proba','count']).T\n",
    "\n",
    "verb_cond['lower_bound'] = (verb_cond['cond_proba'] - 1.96*np.sqrt(verb_cond['cond_proba']*(1-verb_cond['cond_proba'])/verb_cond['count'])).apply(lambda x: max(x,0))\n",
    "adverb_cond['lower_bound'] = (adverb_cond['cond_proba'] - 1.96*np.sqrt(adverb_cond['cond_proba']*(1-adverb_cond['cond_proba'])/adverb_cond['count'])).apply(lambda x: max(x,0))\n",
    "adj_cond['lower_bound'] = (adj_cond['cond_proba'] - 1.96*np.sqrt(adj_cond['cond_proba']*(1-adj_cond['cond_proba'])/adj_cond['count'])).apply(lambda x: max(x,0))\n",
    "\n",
    "verb_cond['cond_proba_no_pi'] = 1. - verb_cond['cond_proba']\n",
    "verb_cond['lower_bound_no_pi'] = verb_cond['cond_proba_no_pi'] - 1.96 * np.sqrt(verb_cond['cond_proba_no_pi'] * (1-verb_cond['cond_proba_no_pi'])/verb_cond['count'])\n",
    "\n",
    "adverb_cond['cond_proba_no_pi'] = 1. - adverb_cond['cond_proba']\n",
    "adverb_cond['lower_bound_no_pi'] = adverb_cond['cond_proba_no_pi'] - 1.96 * np.sqrt(adverb_cond['cond_proba_no_pi'] * (1-adverb_cond['cond_proba_no_pi'])/adverb_cond['count'])\n",
    "\n",
    "adj_cond['cond_proba_no_pi'] = 1. - adj_cond['cond_proba']\n",
    "adj_cond['lower_bound_no_pi'] = adj_cond['cond_proba_no_pi'] - 1.96 * np.sqrt(adj_cond['cond_proba_no_pi'] * (1-adj_cond['cond_proba_no_pi'])/adj_cond['count'])\n",
    "\n",
    "verb_cond = verb_cond[verb_cond['count']>=10]\n",
    "adverb_cond = adverb_cond[adverb_cond['count']>=10]\n",
    "adj_cond = adj_cond[adj_cond['count']>=10]\n",
    "\n",
    "verb_features_10 = list(verb_cond.sort_values('lower_bound', ascending=False).index[:verb_cond.shape[0]//10]) + list(verb_cond.sort_values('lower_bound_no_pi', ascending=False).index[:verb_cond.shape[0]//10])\n",
    "verb_features_20 = list(verb_cond.sort_values('lower_bound', ascending=False).index[:verb_cond.shape[0] * 2//10]) + list(verb_cond.sort_values('lower_bound_no_pi', ascending=False).index[:verb_cond.shape[0] * 2//10])\n",
    "verb_features_30 = list(verb_cond.sort_values('lower_bound', ascending=False).index[:verb_cond.shape[0] * 3//10]) + list(verb_cond.sort_values('lower_bound_no_pi', ascending=False).index[:verb_cond.shape[0] * 3//10])\n",
    "\n",
    "adverb_features_10 = list(adverb_cond.sort_values('lower_bound', ascending=False).index[:adverb_cond.shape[0]//10]) + list(adverb_cond.sort_values('lower_bound_no_pi', ascending=False).index[:adverb_cond.shape[0]//10])\n",
    "adverb_features_20 = list(adverb_cond.sort_values('lower_bound', ascending=False).index[:adverb_cond.shape[0] * 2//10]) + list(adverb_cond.sort_values('lower_bound_no_pi', ascending=False).index[:adverb_cond.shape[0] * 2//10])\n",
    "adverb_features_30 = list(adverb_cond.sort_values('lower_bound', ascending=False).index[:adverb_cond.shape[0] * 3//10]) + list(adverb_cond.sort_values('lower_bound_no_pi', ascending=False).index[:adverb_cond.shape[0] * 3//10])\n",
    "\n",
    "adj_features_10 = list(adj_cond.sort_values('lower_bound', ascending=False).index[:adj_cond.shape[0]//10]) + list(adj_cond.sort_values('lower_bound_no_pi', ascending=False).index[:adj_cond.shape[0]//10])\n",
    "adj_features_20 = list(adj_cond.sort_values('lower_bound', ascending=False).index[:adj_cond.shape[0] * 2//10]) + list(adj_cond.sort_values('lower_bound_no_pi', ascending=False).index[:adj_cond.shape[0] * 2//10])\n",
    "adj_features_30 = list(adj_cond.sort_values('lower_bound', ascending=False).index[:adj_cond.shape[0] * 3//10]) + list(adj_cond.sort_values('lower_bound_no_pi', ascending=False).index[:adj_cond.shape[0] * 3//10])\n",
    "\n",
    "\"\"\"Function to compute the one-hot encodings\"\"\"\n",
    "features_10 = verb_features_10 + adverb_features_10 + adj_features_10\n",
    "features_20 = verb_features_20 + adverb_features_20 + adj_features_20\n",
    "features_30 = verb_features_30 + adverb_features_30 + adj_features_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac345668a3a4578a59bdb4788624aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23102), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Getting feature representation for logistic regression\"\"\"\n",
    "feat_reps = np.array([[int(w in fr) for w in features_20] for fr in tqdm(full_representation)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b668b64dc91472d98443b31f620dd4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=47), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Collecting the negative bigrams\"\"\"\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def get_neg_pairs(text):\n",
    "    doc = nlp(text)\n",
    "    dep_df = pd.DataFrame([[token.text, token.pos_, token.dep_, token.head.text, token.head.pos_] for token in doc], columns=['token','token_pos','dep','head','head_pos'])\n",
    "    neg_dep = dep_df[(dep_df.dep=='neg') & (dep_df.head_pos=='VERB')]\n",
    "    \n",
    "    from nltk import WordNetLemmatizer\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    pairs = [[neg_dep.token.iloc[i], lmtzr.lemmatize(neg_dep['head'].iloc[i], 'v')] for i in range(neg_dep.shape[0])]\n",
    "    return pairs\n",
    "\n",
    "p = Pool(40)\n",
    "\n",
    "neg_pairs_data = []\n",
    "N = data.shape[0]\n",
    "\n",
    "for k in tqdm(range(N//500+1)):\n",
    "    neg_pairs_data += p.map(get_neg_pairs, list(data.text.iloc[k*500:min(N,(k+1)*500)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Recording the verb bigrams\"\"\"\n",
    "verb_bigrams = {}\n",
    "for k in range(len(neg_pairs_data)):\n",
    "    for i in range(len(neg_pairs_data[k])):\n",
    "        if neg_pairs_data[k][i][1] in verb_features_30:\n",
    "            try:\n",
    "                verb_bigrams[neg_pairs_data[k][i][1]] += 1\n",
    "            except:\n",
    "                verb_bigrams[neg_pairs_data[k][i][1]] = 1\n",
    "\n",
    "verb_bigrams = pd.Series(verb_bigrams)\n",
    "verb_bigrams = verb_bigrams[verb_bigrams>=5]\n",
    "verb_bigrams = list(verb_bigrams.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8500b3bd88a4d69ac96de7c9ef6fb70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23102), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49693c173bc42d19355e742b5207e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23102), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building the negative bigrams representations for neighborhoods\"\"\"\n",
    "neg_reps = []\n",
    "neg_presence = []\n",
    "for k in tqdm(range(data.shape[0])):\n",
    "    nr = []\n",
    "        \n",
    "    for neg_pair in neg_pairs_data[k]:\n",
    "        if len(set(full_representation[k]).intersection(neg_pair))==2:\n",
    "            if neg_pair[1] in verb_bigrams:\n",
    "                nr.append(neg_pair[1])\n",
    "            if neg_pair[0] in verb_bigrams:\n",
    "                nr.append(neg_pair[0])    \n",
    "    neg_reps.append(deepcopy(nr))\n",
    "    \n",
    "neg_reps = np.array([[int(v in nr) for v in verb_bigrams] for nr in tqdm(neg_reps)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23102, 610)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_reps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23102, 86)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_reps.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "2-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "3-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "4-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "5-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "6-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "7-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "8-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "9-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "10-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "Precision : 0.622349282185821 +- 0.028318278755104835\n",
      "Recall : 0.422443621965915 +- 0.031426230484572555\n",
      "F1 : 0.5028022139561593 +- 0.02858124285567076\n",
      "ROC AUC : 0.9175996593103275 +- 0.007618572326005925\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import expit, logit\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\"\"\"Building indexes\"\"\"\n",
    "n = len(ys)\n",
    "X_neg_1 = np.hstack([feat_reps, neg_reps])\n",
    "d = X_neg_1.shape[1]\n",
    "\n",
    "\"\"\"Retrieving the positive and negative indexes\"\"\"\n",
    "neg_index = [k for k in range(len(ys)) if ys[k]==0]\n",
    "pos_index = [k for k in range(len(ys)) if ys[k]==1]\n",
    "\n",
    "\"\"\"Shuffling both the positive and negative indexes\"\"\"\n",
    "np.random.seed(seed=0)\n",
    "np.random.shuffle(neg_index)\n",
    "np.random.shuffle(pos_index)\n",
    "\n",
    "\"\"\"Computing train and test index sets for the chosen number of folds\"\"\"\n",
    "cv = 10\n",
    "\n",
    "s = len(neg_index)//cv\n",
    "neg_index_list = [neg_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "neg_index_list.append(neg_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in neg_index_list])==len(neg_index)) # Check on negative index completion\n",
    "\n",
    "s = len(pos_index)//cv\n",
    "pos_index_list = [pos_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "pos_index_list.append(pos_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in pos_index_list])==len(pos_index))# Check on positive index completion\n",
    "\n",
    "\"\"\"Cross-validation on 10% features, without hashtags\"\"\"\n",
    "from scipy.special import logit\n",
    "\n",
    "\"\"\"Initializing the list of outputs, predictions and probabilities to computed CV-ly\"\"\"\n",
    "Y = ys\n",
    "Y_Pred = []\n",
    "Y_Proba = []\n",
    "\n",
    "\"\"\"Intializing the list of cross-validated features\"\"\"\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "roc_auc_list = []\n",
    "f1_list = []\n",
    "\n",
    "\"\"\"Performing the cross-validation of the model using the features of interest\n",
    "A caveat to be mentioned is that the feature selection was performed on the whole dataset, which may be a little biased towards choosing the right features.\n",
    "This effect will be neglected during this test.\"\"\"\n",
    "for k in range(cv):\n",
    "    \"\"\"Splitting the data into train and test\"\"\"\n",
    "    print('{}-th fold'.format(k+1))\n",
    "    print('Splitting the data')\n",
    "    neg_test = np.vstack([X_neg_1[i:i+1] for i in neg_index_list[k]])\n",
    "    pos_test = np.vstack([X_neg_1[i:i+1] for i in pos_index_list[k]])\n",
    "    neg_train = np.vstack([X_neg_1[i:i+1] for i in set(neg_index).difference(neg_index_list[k])])\n",
    "    pos_train = np.vstack([X_neg_1[i:i+1] for i in set(pos_index).difference(pos_index_list[k])])\n",
    "\n",
    "    X_train = np.vstack([pos_train, neg_train])\n",
    "    X_test = np.vstack([pos_test, neg_test])\n",
    "\n",
    "    y_train = [1] * len(pos_train) + [0] * len(neg_train)\n",
    "    y_test = [1] * len(pos_test) + [0] * len(neg_test)\n",
    "\n",
    "    \"\"\"Fitting the model\"\"\"\n",
    "    print('Fitting the model')\n",
    "    d = X_train.shape[1]\n",
    "    model = LogisticRegression(fit_intercept=True, C=1e6)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print('Evaluation and storage of model parameters\\n')\n",
    "    \"\"\"Outputting the predictions and the probability scores\"\"\"\n",
    "    y_score = model.predict_proba(X_test)[:,1]\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    \"\"\"Computing the various metrics\"\"\"\n",
    "    f1_list.append(f1_score(y_test, y_pred))\n",
    "    precision_list.append(precision_score(y_test, y_pred))\n",
    "    recall_list.append(recall_score(y_test, y_pred))\n",
    "    roc_auc_list.append(roc_auc_score(y_test, y_score))\n",
    "\n",
    "    \"\"\"Adding predictions and scores to computed global cross-validated performance after the end of the process\"\"\"\n",
    "    Y_Pred.append(list(y_pred))\n",
    "    Y_Proba.append(list(y_score))\n",
    "\n",
    "print(\"Precision : {} +- {}\\nRecall : {} +- {}\\nF1 : {} +- {}\\nROC AUC : {} +- {}\".format(np.mean(precision_list), np.std(precision_list),\n",
    "                                                                                          np.mean(recall_list), np.std(recall_list),\n",
    "                                                                                          np.mean(f1_list), np.std(f1_list),\n",
    "                                                                                          np.mean(roc_auc_list), np.std(roc_auc_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeighLoR approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff71ec6773e487897123e0e9e93c0a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=47), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building the neighborhoods for the NeighLoR model\"\"\"\n",
    "unique_corpus = unique_verbs.union(unique_adverbs).union(unique_adjectives)\n",
    "\n",
    "def build_neighborhood(local):\n",
    "    #Instantiating lemmatizer\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    \n",
    "    try:\n",
    "        #Removing non interesting tags, tweet specific tags (emojis, existentials, numbers, URLs, the &, punctuation, unknown, @ mentions, determinants)\n",
    "        local = local[local.word.apply(lambda x: x.count('-')<=1 and x.count('/')<=1 and not(any(str(k) in x for k in range(10))) and not('#' in x))]\n",
    "        local = local[local.tag.isin(['U','&',',','$','!','^','#']).apply(lambda x: not x)]\n",
    "        if local.shape[0]==0:\n",
    "            return([],[],[])\n",
    "        else: \n",
    "            #Lemmatizing the words to remove the verb and adverb tokens to be considered\n",
    "            local['word'] = local.word.apply(lambda x: x.replace('-',' ').replace('/',' ').replace('\\\\','').replace(\"'\",''))\n",
    "            #Extracting the verb and adverb patterns\n",
    "            local_words_5 = []\n",
    "            local_words_7 = []\n",
    "            local_words_9 = []\n",
    "            for i in range(local.shape[0]):\n",
    "                w = local.word.iloc[i]\n",
    "                if w in unique_corpus or local.tag.iloc[i] in ['V','R','A']:#,'N']:\n",
    "                    neighborhood_5 = local.iloc[max(0,i-2):min(local.shape[0],i+3)]\n",
    "                    neighborhood_7 = local.iloc[max(0,i-3):min(local.shape[0],i+4)]\n",
    "                    neighborhood_9 = local.iloc[max(0,i-4):min(local.shape[0],i+5)]\n",
    "                    local_words_5.append(list(neighborhood_5.word))\n",
    "                    local_words_7.append(list(neighborhood_7.word))\n",
    "                    local_words_9.append(list(neighborhood_9.word))\n",
    "                #If the word is a verb, add it to the bank of verbs\n",
    "                if (local.tag.iloc[i]=='V') and ('#' not in w) and ('&' not in w) and not(any(str(k) in w for k in range(10))) and ('-' not in w) and ('/' not in w):\n",
    "                    if type(w)==list:\n",
    "                        unique_verbs.update(set(w))\n",
    "                    else:\n",
    "                        unique_verbs.add(w)\n",
    "            return(local_words_5, local_words_7, local_words_9)\n",
    "    except:\n",
    "        return ([],[],[])\n",
    "\n",
    "neighborhoods = []\n",
    "p = Pool(40)\n",
    "for k in tqdm(range(N//500+1)):\n",
    "    neighborhoods += p.map(build_neighborhood, list(data.tag_df.iloc[500*k:min(500*(k+1),N)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab985afadcf44e5fb4f4372ec28955d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23102), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "actual_words_7 = [[[lem_words[w.lower()] if w.lower() in lem_words.keys() else w.lower() for w in v] for v in neighborhoods[k][1]] for k in tqdm(range(len(neighborhoods)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " McDonald 's is trash, in this case I mean literally. They are not worth the money. UberEats definitely needs to refund you.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['s', 'is', 'trash', 'in'],\n",
       " ['s', 'is', 'trash', 'in', 'this'],\n",
       " ['s', 'is', 'trash', 'in', 'this', 'case'],\n",
       " ['trash', 'in', 'this', 'case', 'i', 'mean', 'literally'],\n",
       " ['this', 'case', 'i', 'mean', 'literally', 'they', 'are'],\n",
       " ['case', 'i', 'mean', 'literally', 'they', 'are', 'not'],\n",
       " ['mean', 'literally', 'they', 'are', 'not', 'worth', 'the'],\n",
       " ['literally', 'they', 'are', 'not', 'worth', 'the', 'money'],\n",
       " ['they', 'are', 'not', 'worth', 'the', 'money', 'definitely'],\n",
       " ['worth', 'the', 'money', 'definitely', 'needs', 'to', 'refund'],\n",
       " ['the', 'money', 'definitely', 'needs', 'to', 'refund', 'you'],\n",
       " ['money', 'definitely', 'needs', 'to', 'refund', 'you'],\n",
       " ['definitely', 'needs', 'to', 'refund', 'you']]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.text.iloc[0])\n",
    "actual_words_7[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be3f05e3b0b428ebfed412b5395e00f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23102), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb629730b644ec0ba32c2cd6a32d7f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23102), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8852a012d6fa4d5c8360375232252cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23102), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building representations\"\"\"\n",
    "d = len(features_20)\n",
    "\n",
    "def represent(x):\n",
    "    return [int(w in x) for w in features_20]\n",
    "\n",
    "X_basic = [[represent(pattern) for pattern in tweet] for tweet in tqdm(actual_words_7)]\n",
    "X_basic = [np.array(tweet) for tweet in tqdm(X_basic)]\n",
    "\n",
    "for k in tqdm(range(len(X_basic))):\n",
    "    if X_basic[k].shape[0]==0:\n",
    "        X_basic[k] = np.zeros((1,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4bc44e99f634417b71c90786c46f2f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23102), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building the negative bigrams representations for neighborhoods\"\"\"\n",
    "neg_reps = []\n",
    "neg_presence = []\n",
    "for k in tqdm(range(data.shape[0])):\n",
    "    nr = []\n",
    "    npr = []\n",
    "    \n",
    "    for i in range(len(actual_words_7[k])):\n",
    "        local = []\n",
    "        neighborhood = set(actual_words_7[k][i])\n",
    "        \n",
    "        for neg_pair in neg_pairs_data[k]:\n",
    "            if len(neighborhood.intersection(neg_pair))==2:\n",
    "                if neg_pair[1] in verb_bigrams:\n",
    "                    local.append(neg_pair[1])\n",
    "                if neg_pair[0] in verb_bigrams:\n",
    "                    local.append(neg_pair[0])\n",
    "        \n",
    "        if len(local)>0:\n",
    "            npr.append(1)\n",
    "        else:\n",
    "            npr.append(0)\n",
    "        nr.append(deepcopy(local))\n",
    "    \n",
    "    neg_reps.append(deepcopy(nr))\n",
    "    neg_presence.append(np.array(npr).reshape((len(actual_words_7[k]),1)))\n",
    "    \n",
    "neg_reps = [np.array([[int(v in local) for v in verb_bigrams] for local in tweet]) for tweet in neg_reps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Correcting the defects of negation representations\"\"\"\n",
    "d_neg = len(verb_bigrams)\n",
    "\n",
    "for k in range(len(neg_reps)):\n",
    "    if neg_reps[k].shape[0]==0:\n",
    "        neg_reps[k] = np.zeros((1,d_neg))\n",
    "    if neg_presence[k].shape[0]==0:\n",
    "        neg_presence[k] = np.zeros((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Complementing the representations for both methods of handling negations\"\"\"\n",
    "X_neg_1 = [np.hstack([X_basic[k], neg_reps[k]]) for k in range(len(X_basic))]\n",
    "d1 = len(features_20) + len(verb_bigrams)\n",
    "feat_1 = features_20 + ['not_' + w for w in verb_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c0f6b0d433419eac599ddb4c765c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "2-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f016e9d1f7bb4f11b61f2c6cb344b725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "3-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b10d71eeac04c0ca8a932366cc3fb37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "4-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05525c7b8b44216a49195fa99c91be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "5-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d54738fa6a54e15b3d1afb3e8cf0fd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "6-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69a5183a95944d8ab7a62119dac63a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "7-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a661b37a53849a3aa6aa76843d16b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "8-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e9b3c3976e4823b77775340d90ff63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "9-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0e9bc3d3874d418ae553fe1923a681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "10-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6762d6adce548cfa4bd69145190a416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=5000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation and storage of model parameters\n",
      "\n",
      "Precision : 0.6790144704884488 +- 0.03880330055449566\n",
      "Recall : 0.5267085556894474 +- 0.02634014347925252\n",
      "F1 : 0.5928325669676058 +- 0.02749710994785614\n",
      "ROC AUC : 0.9485173815476425 +- 0.007414074217384614\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import expit, logit\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\"\"\"Building indexes\"\"\"\n",
    "n = len(ys)\n",
    "d = X_neg_1[0].shape[1]\n",
    "\n",
    "\"\"\"Retrieving the positive and negative indexes\"\"\"\n",
    "neg_index = [k for k in range(len(ys)) if ys[k]==0]\n",
    "pos_index = [k for k in range(len(ys)) if ys[k]==1]\n",
    "\n",
    "\"\"\"Shuffling both the positive and negative indexes\"\"\"\n",
    "np.random.seed(seed=0)\n",
    "np.random.shuffle(neg_index)\n",
    "np.random.shuffle(pos_index)\n",
    "\n",
    "\"\"\"Computing train and test index sets for the chosen number of folds\"\"\"\n",
    "cv = 10\n",
    "\n",
    "s = len(neg_index)//cv\n",
    "neg_index_list = [neg_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "neg_index_list.append(neg_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in neg_index_list])==len(neg_index)) # Check on negative index completion\n",
    "\n",
    "s = len(pos_index)//cv\n",
    "pos_index_list = [pos_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "pos_index_list.append(pos_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in pos_index_list])==len(pos_index))# Check on positive index completion\n",
    "\n",
    "\"\"\"Cross-validation on 10% features, without hashtags\"\"\"\n",
    "from scipy.special import logit\n",
    "\n",
    "\"\"\"Initializing the list of outputs, predictions and probabilities to computed CV-ly\"\"\"\n",
    "Y = ys\n",
    "Y_Pred = []\n",
    "Y_Proba = []\n",
    "\n",
    "\"\"\"Intializing the list of cross-validated features\"\"\"\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "roc_auc_list = []\n",
    "f1_list = []\n",
    "\n",
    "\"\"\"Storing coefficients and biases for stability evaluation\"\"\"\n",
    "biases = []\n",
    "weights = []\n",
    "\n",
    "\"\"\"Performing the cross-validation of the model using the features of interest\n",
    "A caveat to be mentioned is that the feature selection was performed on the whole dataset, which may be a little biased towards choosing the right features.\n",
    "This effect will be neglected during this test.\"\"\"\n",
    "for k in range(cv):\n",
    "    \"\"\"Splitting the data into train and test\"\"\"\n",
    "    print('{}-th fold'.format(k+1))\n",
    "    print('Splitting the data')\n",
    "    neg_test = [X_neg_1[i] for i in neg_index_list[k]]\n",
    "    pos_test = [X_neg_1[i] for i in pos_index_list[k]]\n",
    "    neg_train = [X_neg_1[i] for i in set(neg_index).difference(neg_index_list[k])]\n",
    "    pos_train = [X_neg_1[i] for i in set(pos_index).difference(pos_index_list[k])]\n",
    "\n",
    "    X_train = pos_train + neg_train\n",
    "    X_test = pos_test + neg_test\n",
    "\n",
    "    y_train = [1] * len(pos_train) + [0] * len(neg_train)\n",
    "    y_test = [1] * len(pos_test) + [0] * len(neg_test)\n",
    "\n",
    "    \"\"\"Fitting the model\"\"\"\n",
    "    print('Fitting the model')\n",
    "    d = X_train[0].shape[1]\n",
    "    model = MaxLog(d=d, fit_intercept=True, alpha=10., epsilon=1e-4, init_b=logit(np.mean(y_train)), init_w=np.zeros(d))\n",
    "    model.fit(X_train, y_train, n_iter=5000, parallel=False)\n",
    "\n",
    "    print('Evaluation and storage of model parameters\\n')\n",
    "    \"\"\"Outputting the predictions and the probability scores\"\"\"\n",
    "    y_score = model.predict_proba(X_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    \"\"\"Computing the various metrics\"\"\"\n",
    "    f1_list.append(f1_score(y_test, y_pred))\n",
    "    precision_list.append(precision_score(y_test, y_pred))\n",
    "    recall_list.append(recall_score(y_test, y_pred))\n",
    "    roc_auc_list.append(roc_auc_score(y_test, y_score))\n",
    "\n",
    "    \"\"\"Adding predictions and scores to computed global cross-validated performance after the end of the process\"\"\"\n",
    "    Y_Pred.append(list(y_pred))\n",
    "    Y_Proba.append(list(y_score))\n",
    "\n",
    "    \"\"\"Storing the model's coefficients\"\"\"\n",
    "    biases.append(model.b)\n",
    "    weights.append(model.w)\n",
    "    \n",
    "print(\"Precision : {} +- {}\\nRecall : {} +- {}\\nF1 : {} +- {}\\nROC AUC : {} +- {}\".format(np.mean(precision_list), np.std(precision_list),\n",
    "                                                                                          np.mean(recall_list), np.std(recall_list),\n",
    "                                                                                          np.mean(f1_list), np.std(f1_list),\n",
    "                                                                                          np.mean(roc_auc_list), np.std(roc_auc_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Word2Vec + Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd0a42b2bb04162bd5b950d21c4a43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23102), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Loading W2V matrix and building representation\"\"\"\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "google_w2v = KeyedVectors.load_word2vec_format('/home/qrg-researchlab/Downloads/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "def get_embed(x):\n",
    "    local = list(x[x.tag.isin(['V','A','R','N'])].word)\n",
    "    embed = [google_w2v[w].reshape((1,-1)) for w in local if w in google_w2v.vocab.keys()]\n",
    "    if len(embed)>0:\n",
    "        return np.vstack(embed).sum(axis=0)\n",
    "    else:\n",
    "        return np.zeros(300)\n",
    "\n",
    "X_pt_w2v = np.vstack([get_embed(data.tag_df.iloc[k]).reshape((1,-1)) for k in tqdm(range(data.shape[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "2-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "3-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "4-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "5-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "6-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "7-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "8-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "9-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "10-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "Cross-validation evaluation of logistic regression + pretrained Word2Vec:\n",
      "Precision : 0.6165052279949155 +- 0.0411909574836597\n",
      "Recall : 0.41898777758650374 +- 0.03312995628723719\n",
      "F1 : 0.4985445735819159 +- 0.034553903824631395\n",
      "ROC AUC : 0.9231831642753946 +- 0.009770245971234891\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import expit, logit\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\"\"\"Building indexes\"\"\"\n",
    "n = len(ys)\n",
    "d = X_pt_w2v.shape[1]\n",
    "\n",
    "\"\"\"Retrieving the positive and negative indexes\"\"\"\n",
    "neg_index = [k for k in range(len(ys)) if ys[k]==0]\n",
    "pos_index = [k for k in range(len(ys)) if ys[k]==1]\n",
    "\n",
    "\"\"\"Shuffling both the positive and negative indexes\"\"\"\n",
    "np.random.seed(seed=0)\n",
    "np.random.shuffle(neg_index)\n",
    "np.random.shuffle(pos_index)\n",
    "\n",
    "\"\"\"Computing train and test index sets for the chosen number of folds\"\"\"\n",
    "cv = 10\n",
    "\n",
    "s = len(neg_index)//cv\n",
    "neg_index_list = [neg_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "neg_index_list.append(neg_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in neg_index_list])==len(neg_index)) # Check on negative index completion\n",
    "\n",
    "s = len(pos_index)//cv\n",
    "pos_index_list = [pos_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "pos_index_list.append(pos_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in pos_index_list])==len(pos_index))# Check on positive index completion\n",
    "\n",
    "\"\"\"Initializing the list of outputs, predictions and probabilities to computed CV-ly\"\"\"\n",
    "Y = ys\n",
    "Y_Pred = []\n",
    "Y_Proba = []\n",
    "\n",
    "\"\"\"Intializing the list of cross-validated features\"\"\"\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "roc_auc_list = []\n",
    "f1_list = []\n",
    "\n",
    "\"\"\"Performing the cross-validation of the model using the features of interest\n",
    "\"\"\"\n",
    "for k in range(cv):\n",
    "    \"\"\"Splitting the data into train and test\"\"\"\n",
    "    print('{}-th fold'.format(k+1))\n",
    "    print('Splitting the data')\n",
    "    neg_test = np.vstack([X_pt_w2v[i:i+1] for i in neg_index_list[k]])\n",
    "    pos_test = np.vstack([X_pt_w2v[i:i+1] for i in pos_index_list[k]])\n",
    "    neg_train = np.vstack([X_pt_w2v[i:i+1] for i in set(neg_index).difference(neg_index_list[k])])\n",
    "    pos_train = np.vstack([X_pt_w2v[i:i+1] for i in set(pos_index).difference(pos_index_list[k])])\n",
    "\n",
    "    X_train = np.vstack([pos_train, neg_train])\n",
    "    X_test = np.vstack([pos_test, neg_test])\n",
    "\n",
    "    y_train = [1] * len(pos_train) + [0] * len(neg_train)\n",
    "    y_test = [1] * len(pos_test) + [0] * len(neg_test)\n",
    "\n",
    "    \"\"\"Fitting the model\"\"\"\n",
    "    print('Fitting the model')\n",
    "    d = X_train.shape[1]\n",
    "    model = LogisticRegression(fit_intercept=True, C=1e4)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print('Evaluation and storage of model parameters\\n')\n",
    "    \"\"\"Outputting the predictions and the probability scores\"\"\"\n",
    "    y_score = model.predict_proba(X_test)[:,1]\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    \"\"\"Computing the various metrics\"\"\"\n",
    "    f1_list.append(f1_score(y_test, y_pred))\n",
    "    precision_list.append(precision_score(y_test, y_pred))\n",
    "    recall_list.append(recall_score(y_test, y_pred))\n",
    "    roc_auc_list.append(roc_auc_score(y_test, y_score))\n",
    "\n",
    "    \"\"\"Adding predictions and scores to computed global cross-validated performance after the end of the process\"\"\"\n",
    "    Y_Pred.append(list(y_pred))\n",
    "    Y_Proba.append(list(y_score))\n",
    "\n",
    "print(\"Cross-validation evaluation of logistic regression + pretrained Word2Vec:\")\n",
    "print(\"Precision : {} +- {}\\nRecall : {} +- {}\\nF1 : {} +- {}\\nROC AUC : {} +- {}\".format(np.mean(precision_list), np.std(precision_list),\n",
    "                                                                                          np.mean(recall_list), np.std(recall_list),\n",
    "                                                                                          np.mean(f1_list), np.std(f1_list),\n",
    "                                                                                          np.mean(roc_auc_list), np.std(roc_auc_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Word2Vec + NeighLoR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de199485731447fe80286ed62a8a012f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23102), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab31447b5b143a79f3666b8886942ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23102), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5960dbf38472401d8ba8d53bab265b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23102), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building representations for word embeddings\"\"\"\n",
    "d = 300\n",
    "\n",
    "def represent(x):\n",
    "    rep = []\n",
    "    for w in x:\n",
    "        try:\n",
    "            rep.append(google_w2v[w])\n",
    "        except:\n",
    "            pass\n",
    "    if len(rep)>0:\n",
    "        return np.vstack(rep).sum(axis=0)\n",
    "    else:\n",
    "        return np.zeros(300)\n",
    "\n",
    "X_basic = [[represent(pattern).reshape((1,-1)) for pattern in tweet] if len(tweet)>0 else np.zeros((1,300)) for tweet in tqdm(actual_words_7)]\n",
    "X_basic = [np.vstack(tweet) for tweet in tqdm(X_basic)]\n",
    "\n",
    "for k in tqdm(range(len(X_basic))):\n",
    "    if X_basic[k].shape[0]==0:\n",
    "        X_basic[k] = np.zeros((1,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b9b3dad7c546da97b3642ef02f4fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "2-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524eb9c5781a4df3aabc7a320ee0ac0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "3-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e9ad0da69e4e2595583bb16812f923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "4-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0231498de3d479294086fae93cc0f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "5-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533ba181976445c4beaa3baba042fbee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "6-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c57f21584a614d10b1bb7661e269e732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "7-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e380b9b562841bf9b3e516157f58ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "8-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423da130503548b7b6f553b393f88b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "9-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8c2a62031f443bb7702c9f0a7da9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "10-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b22bd6707914e87bdd3a832bf9f03e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "Cross-validation evaluation of the pretrained Word2Vec + NeighLoR approach:\n",
      "Precision : 0.47646366130020257 +- 0.20170974089467342\n",
      "Recall : 0.5338268204510243 +- 0.3686147533978055\n",
      "F1 : 0.40309420669281104 +- 0.2370649924487037\n",
      "ROC AUC : 0.9160995513875927 +- 0.05478500654908393\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building indexes\"\"\"\n",
    "n = len(ys)\n",
    "d = X_basic[0].shape[1]\n",
    "\n",
    "\"\"\"Retrieving the positive and negative indexes\"\"\"\n",
    "neg_index = [k for k in range(len(ys)) if ys[k]==0]\n",
    "pos_index = [k for k in range(len(ys)) if ys[k]==1]\n",
    "\n",
    "\"\"\"Shuffling both the positive and negative indexes\"\"\"\n",
    "np.random.seed(seed=0)\n",
    "np.random.shuffle(neg_index)\n",
    "np.random.shuffle(pos_index)\n",
    "\n",
    "\"\"\"Computing train and test index sets for the chosen number of folds\"\"\"\n",
    "cv = 10\n",
    "\n",
    "s = len(neg_index)//cv\n",
    "neg_index_list = [neg_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "neg_index_list.append(neg_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in neg_index_list])==len(neg_index)) # Check on negative index completion\n",
    "\n",
    "s = len(pos_index)//cv\n",
    "pos_index_list = [pos_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "pos_index_list.append(pos_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in pos_index_list])==len(pos_index))# Check on positive index completion\n",
    "\n",
    "\"\"\"Cross-validation on 10% features, without hashtags\"\"\"\n",
    "from scipy.special import logit\n",
    "\n",
    "\"\"\"Initializing the list of outputs, predictions and probabilities to computed CV-ly\"\"\"\n",
    "Y = ys\n",
    "Y_Pred = []\n",
    "Y_Proba = []\n",
    "\n",
    "\"\"\"Intializing the list of cross-validated features\"\"\"\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "roc_auc_list = []\n",
    "f1_list = []\n",
    "\n",
    "\"\"\"Storing coefficients and biases for stability evaluation\"\"\"\n",
    "biases = []\n",
    "weights = []\n",
    "\n",
    "\"\"\"Performing the cross-validation of the model using the features of interest\n",
    "A caveat to be mentioned is that the feature selection was performed on the whole dataset, which may be a little biased towards choosing the right features.\n",
    "This effect will be neglected during this test.\"\"\"\n",
    "for k in range(cv):\n",
    "    \"\"\"Splitting the data into train and test\"\"\"\n",
    "    print('{}-th fold'.format(k+1))\n",
    "    print('Splitting the data')\n",
    "    neg_test = [X_basic[i] for i in neg_index_list[k]]\n",
    "    pos_test = [X_basic[i] for i in pos_index_list[k]]\n",
    "    neg_train = [X_basic[i] for i in set(neg_index).difference(neg_index_list[k])]\n",
    "    pos_train = [X_basic[i] for i in set(pos_index).difference(pos_index_list[k])]\n",
    "\n",
    "    X_train = pos_train + neg_train\n",
    "    X_test = pos_test + neg_test\n",
    "\n",
    "    y_train = [1] * len(pos_train) + [0] * len(neg_train)\n",
    "    y_test = [1] * len(pos_test) + [0] * len(neg_test)\n",
    "\n",
    "    \"\"\"Fitting the model\"\"\"\n",
    "    print('Fitting the model')\n",
    "    d = X_train[0].shape[1]\n",
    "    model = MaxLog(d=d, fit_intercept=True, alpha=10., epsilon=1e-4, init_b=logit(np.mean(y_train)), init_w=np.zeros(d))\n",
    "    model.fit(X_train, y_train, n_iter=3000, parallel=False)\n",
    "\n",
    "    print('Evaluation and storage of model parameters\\n')\n",
    "    \"\"\"Outputting the predictions and the probability scores\"\"\"\n",
    "    y_score = model.predict_proba(X_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    \"\"\"Computing the various metrics\"\"\"\n",
    "    f1_list.append(f1_score(y_test, y_pred))\n",
    "    precision_list.append(precision_score(y_test, y_pred))\n",
    "    recall_list.append(recall_score(y_test, y_pred))\n",
    "    roc_auc_list.append(roc_auc_score(y_test, y_score))\n",
    "\n",
    "    \"\"\"Adding predictions and scores to computed global cross-validated performance after the end of the process\"\"\"\n",
    "    Y_Pred.append(list(y_pred))\n",
    "    Y_Proba.append(list(y_score))\n",
    "\n",
    "    \"\"\"Storing the model's coefficients\"\"\"\n",
    "    biases.append(model.b)\n",
    "    weights.append(model.w)\n",
    "\n",
    "print(\"Cross-validation evaluation of the pretrained Word2Vec + NeighLoR approach:\")\n",
    "print(\"Precision : {} +- {}\\nRecall : {} +- {}\\nF1 : {} +- {}\\nROC AUC : {} +- {}\".format(np.mean(precision_list), np.std(precision_list),\n",
    "                                                                                          np.mean(recall_list), np.std(recall_list),\n",
    "                                                                                          np.mean(f1_list), np.std(f1_list),\n",
    "                                                                                          np.mean(roc_auc_list), np.std(roc_auc_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training custom Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee22d251ec249c784e1daa3993bb377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=221), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73faa060f7234896915068853c50297f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=329236), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Loading large BA data to train our own specialized embedding\"\"\"\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "\"\"\"Loading British Airways data\"\"\"\n",
    "big_data = []\n",
    "\n",
    "for file in tqdm(os.listdir('./processed_data/mcdonalds/')):\n",
    "    if file[-2:]=='.b':\n",
    "        big_data.append(pickle.load(open(os.path.join('./processed_data/mcdonalds/', file), 'rb')))\n",
    "\n",
    "data_df = pd.concat(big_data, axis=0, sort=False)\n",
    "data_df.index = list(range(data_df.shape[0]))\n",
    "\n",
    "\"\"\"Collecting verbs, adverbs, and adjectives\"\"\"\n",
    "lmtzr = WordNetLemmatizer()\n",
    "unique_verbs = set()\n",
    "unique_adverbs = set()\n",
    "unique_adjectives = set()\n",
    "\n",
    "N = data_df.shape[0]//3\n",
    "\n",
    "for k in tqdm(range(N)):\n",
    "    local = data_df.tag_df.iloc[k]\n",
    "    local = local[local.word.apply(lambda x: x.count('-')<=1 and x.count('/')<=1 and not(any(str(k) in x for k in range(10))) and not('#' in x))]\n",
    "    unique_verbs.update(list(local[local.tag=='V'].word.apply(lambda x: lmtzr.lemmatize(x.replace('-','').replace('/','').replace('\\\\',''),'v'))))\n",
    "    unique_adverbs.update(list(local[local.tag=='R'].word.apply(lambda x: x.replace('-','').replace('/','').replace('\\\\',''))))\n",
    "    unique_adjectives.update(list(local[local.tag=='A'].word.apply(lambda x: x.replace('-','').replace('/','').replace('\\\\',''))))\n",
    "    \n",
    "\"\"\"Lemmatizing all verbs\"\"\"\n",
    "lem_verbs = {verb: lmtzr.lemmatize(verb, 'v') for verb in unique_verbs}\n",
    "lem_adj = {adj: lmtzr.lemmatize(adj, 'a') for adj in unique_adjectives}\n",
    "lem_adv = {adv: adv for adv in unique_adverbs}\n",
    "lem_words = dict()\n",
    "lem_words.update(lem_adv)\n",
    "lem_words.update(lem_adj)\n",
    "lem_words.update(lem_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198b992317dd43a894f6d9274851bd8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=329236), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2118dbce694c779539cf6f57d4007f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=329236), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(75682287, 90847260)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Applying lemmatization to the text\"\"\"\n",
    "for k in tqdm(range(N)):\n",
    "    local = data_df.tag_df.iloc[k]\n",
    "    local['word'] = local.word.apply(lambda x: lem_words[x] if x in lem_words.keys() else x)\n",
    "    data_df.tag_df['word'] = deepcopy(local.word)\n",
    "\n",
    "\"\"\"Training the embedding\"\"\"\n",
    "sentences = [list(data_df.tag_df.iloc[k][data_df.tag_df.iloc[k].tag.isin(['V','R','A','N'])].word) for k in tqdm(range(N))]\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=50,\n",
    "                     alpha=0.05,\n",
    "                     min_alpha=0.001,\n",
    "                     negative=5,\n",
    "                     workers=40)\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Word2Vec + logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c215b8e2b2f4dbcb9d9aae98d200c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23102), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrg-researchlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building the representation\"\"\"\n",
    "def get_embed(x):\n",
    "    local = list(x[x.tag.isin(['V','A','R','N'])].word)\n",
    "    embed = [w2v_model[w].reshape((1,-1)) for w in local if w in w2v_model.wv.vocab.keys()]\n",
    "    if len(embed)>0:\n",
    "        return np.vstack(embed).sum(axis=0)\n",
    "    else:\n",
    "        return np.zeros(50)\n",
    "\n",
    "X_pt_w2v = np.vstack([get_embed(data.tag_df.iloc[k]).reshape((1,-1)) for k in tqdm(range(data.shape[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "2-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "3-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "4-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "5-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "6-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "7-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "8-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "9-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "10-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "Cross-validation evaluation of custom Word2Vec + logistic regression:\n",
      "Precision : 0.5989990660719601 +- 0.04832958033403801\n",
      "Recall : 0.2518290583577208 +- 0.025260428570059716\n",
      "F1 : 0.3536801791402726 +- 0.027809466571613022\n",
      "ROC AUC : 0.9030782785633165 +- 0.012457165071335801\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building indexes\"\"\"\n",
    "n = len(ys)\n",
    "d = X_pt_w2v.shape[1]\n",
    "\n",
    "\"\"\"Retrieving the positive and negative indexes\"\"\"\n",
    "neg_index = [k for k in range(len(ys)) if ys[k]==0]\n",
    "pos_index = [k for k in range(len(ys)) if ys[k]==1]\n",
    "\n",
    "\"\"\"Shuffling both the positive and negative indexes\"\"\"\n",
    "np.random.seed(seed=0)\n",
    "np.random.shuffle(neg_index)\n",
    "np.random.shuffle(pos_index)\n",
    "\n",
    "\"\"\"Computing train and test index sets for the chosen number of folds\"\"\"\n",
    "cv = 10\n",
    "\n",
    "s = len(neg_index)//cv\n",
    "neg_index_list = [neg_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "neg_index_list.append(neg_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in neg_index_list])==len(neg_index)) # Check on negative index completion\n",
    "\n",
    "s = len(pos_index)//cv\n",
    "pos_index_list = [pos_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "pos_index_list.append(pos_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in pos_index_list])==len(pos_index))# Check on positive index completion\n",
    "\n",
    "\"\"\"Cross-validation on 10% features, without hashtags\"\"\"\n",
    "from scipy.special import logit\n",
    "\n",
    "\"\"\"Initializing the list of outputs, predictions and probabilities to computed CV-ly\"\"\"\n",
    "Y = ys\n",
    "Y_Pred = []\n",
    "Y_Proba = []\n",
    "\n",
    "\"\"\"Intializing the list of cross-validated features\"\"\"\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "roc_auc_list = []\n",
    "f1_list = []\n",
    "\n",
    "\"\"\"Performing the cross-validation of the model using the features of interest\n",
    "A caveat to be mentioned is that the feature selection was performed on the whole dataset, which may be a little biased towards choosing the right features.\n",
    "This effect will be neglected during this test.\"\"\"\n",
    "for k in range(cv):\n",
    "    \"\"\"Splitting the data into train and test\"\"\"\n",
    "    print('{}-th fold'.format(k+1))\n",
    "    print('Splitting the data')\n",
    "    neg_test = np.vstack([X_pt_w2v[i:i+1] for i in neg_index_list[k]])\n",
    "    pos_test = np.vstack([X_pt_w2v[i:i+1] for i in pos_index_list[k]])\n",
    "    neg_train = np.vstack([X_pt_w2v[i:i+1] for i in set(neg_index).difference(neg_index_list[k])])\n",
    "    pos_train = np.vstack([X_pt_w2v[i:i+1] for i in set(pos_index).difference(pos_index_list[k])])\n",
    "\n",
    "    X_train = np.vstack([pos_train, neg_train])\n",
    "    X_test = np.vstack([pos_test, neg_test])\n",
    "\n",
    "    y_train = [1] * len(pos_train) + [0] * len(neg_train)\n",
    "    y_test = [1] * len(pos_test) + [0] * len(neg_test)\n",
    "\n",
    "    \"\"\"Fitting the model\"\"\"\n",
    "    print('Fitting the model')\n",
    "    d = X_train.shape[1]\n",
    "    model = LogisticRegression(fit_intercept=True, C=1e4)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print('Evaluation and storage of model parameters\\n')\n",
    "    \"\"\"Outputting the predictions and the probability scores\"\"\"\n",
    "    y_score = model.predict_proba(X_test)[:,1]\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    \"\"\"Computing the various metrics\"\"\"\n",
    "    f1_list.append(f1_score(y_test, y_pred))\n",
    "    precision_list.append(precision_score(y_test, y_pred))\n",
    "    recall_list.append(recall_score(y_test, y_pred))\n",
    "    roc_auc_list.append(roc_auc_score(y_test, y_score))\n",
    "\n",
    "    \"\"\"Adding predictions and scores to computed global cross-validated performance after the end of the process\"\"\"\n",
    "    Y_Pred.append(list(y_pred))\n",
    "    Y_Proba.append(list(y_score))\n",
    "\n",
    "print(\"Cross-validation evaluation of custom Word2Vec + logistic regression:\")\n",
    "print(\"Precision : {} +- {}\\nRecall : {} +- {}\\nF1 : {} +- {}\\nROC AUC : {} +- {}\".format(np.mean(precision_list), np.std(precision_list),\n",
    "                                                                                          np.mean(recall_list), np.std(recall_list),\n",
    "                                                                                          np.mean(f1_list), np.std(f1_list),\n",
    "                                                                                          np.mean(roc_auc_list), np.std(roc_auc_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Word2Vec + NeighLoR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19703ae9450c44dd883de6117ec0e183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23102), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrg-researchlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a19c8e1b304b1698daa8644c8b34fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23102), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e492c9fa56d246b0b711b9d3425147b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=23102), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building representations for word embeddings\"\"\"\n",
    "d = 50\n",
    "\n",
    "def represent(x):\n",
    "    rep = []\n",
    "    for w in x:\n",
    "        try:\n",
    "            rep.append(w2v_model[w])\n",
    "        except:\n",
    "            pass\n",
    "    if len(rep)>0:\n",
    "        return np.vstack(rep).sum(axis=0)\n",
    "    else:\n",
    "        return np.zeros(d)\n",
    "\n",
    "X_basic = [[represent(pattern).reshape((1,-1)) for pattern in tweet] if len(tweet)>0 else np.zeros((1,d)) for tweet in tqdm(actual_words_7)]\n",
    "X_basic = [np.vstack(X_basic[k]) for k in tqdm(range(len(X_basic)))]\n",
    "\n",
    "\n",
    "for k in tqdm(range(len(X_basic))):\n",
    "    if X_basic[k].shape[0]==0:\n",
    "        X_basic[k] = np.zeros((1,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d743c88eb8fd45e181e9768008d82e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrg-researchlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:114: RuntimeWarning: divide by zero encountered in log\n",
      "/home/qrg-researchlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:114: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "2-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482cab1c91334e33847182e40299ce7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "3-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbfb389d552b4392ac44d13e02922f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "4-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc62434d16ae440796923a7e703ea1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "5-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb9308d50e94498acd82ed76bc8b571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "6-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a00629697742788b3b4813481adc9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "7-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eff07152deb4f4e81907197c0b768f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "8-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877c34dc6f174fb6943992a2506f627f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "9-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "275c51ec8fd74508a30f64e8765d2bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "10-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481c310ff25e4459826b117ab3b1e248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=3000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "Precision : 0.23245150909030662 +- 0.21292063669519432\n",
      "Recall : 0.21756756756756754 +- 0.2671919285116916\n",
      "F1 : 0.1985917900177823 +- 0.21676004093296022\n",
      "ROC AUC : 0.6127302845459981 +- 0.2395909525406043\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import expit, logit\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "\"\"\"Building indexes\"\"\"\n",
    "n = len(ys)\n",
    "d = X_basic[0].shape[1]\n",
    "\n",
    "\"\"\"Retrieving the positive and negative indexes\"\"\"\n",
    "neg_index = [k for k in range(len(ys)) if ys[k]==0]\n",
    "pos_index = [k for k in range(len(ys)) if ys[k]==1]\n",
    "\n",
    "\"\"\"Shuffling both the positive and negative indexes\"\"\"\n",
    "np.random.seed(seed=0)\n",
    "np.random.shuffle(neg_index)\n",
    "np.random.shuffle(pos_index)\n",
    "\n",
    "\"\"\"Computing train and test index sets for the chosen number of folds\"\"\"\n",
    "cv = 10\n",
    "\n",
    "s = len(neg_index)//cv\n",
    "neg_index_list = [neg_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "neg_index_list.append(neg_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in neg_index_list])==len(neg_index)) # Check on negative index completion\n",
    "\n",
    "s = len(pos_index)//cv\n",
    "pos_index_list = [pos_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "pos_index_list.append(pos_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in pos_index_list])==len(pos_index))# Check on positive index completion\n",
    "\n",
    "\"\"\"Cross-validation on 10% features, without hashtags\"\"\"\n",
    "from scipy.special import logit\n",
    "\n",
    "\"\"\"Initializing the list of outputs, predictions and probabilities to computed CV-ly\"\"\"\n",
    "Y = ys\n",
    "Y_Pred = []\n",
    "Y_Proba = []\n",
    "\n",
    "\"\"\"Intializing the list of cross-validated features\"\"\"\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "roc_auc_list = []\n",
    "f1_list = []\n",
    "\n",
    "\"\"\"Storing coefficients and biases for stability evaluation\"\"\"\n",
    "biases = []\n",
    "weights = []\n",
    "\n",
    "\"\"\"Performing the cross-validation of the model using the features of interest\n",
    "A caveat to be mentioned is that the feature selection was performed on the whole dataset, which may be a little biased towards choosing the right features.\n",
    "This effect will be neglected during this test.\"\"\"\n",
    "for k in range(cv):\n",
    "    \"\"\"Splitting the data into train and test\"\"\"\n",
    "    print('{}-th fold'.format(k+1))\n",
    "    print('Splitting the data')\n",
    "    neg_test = [X_basic[i] for i in neg_index_list[k]]\n",
    "    pos_test = [X_basic[i] for i in pos_index_list[k]]\n",
    "    neg_train = [X_basic[i] for i in set(neg_index).difference(neg_index_list[k])]\n",
    "    pos_train = [X_basic[i] for i in set(pos_index).difference(pos_index_list[k])]\n",
    "\n",
    "    X_train = pos_train + neg_train\n",
    "    X_test = pos_test + neg_test\n",
    "\n",
    "    y_train = [1] * len(pos_train) + [0] * len(neg_train)\n",
    "    y_test = [1] * len(pos_test) + [0] * len(neg_test)\n",
    "\n",
    "    \"\"\"Fitting the model\"\"\"\n",
    "    print('Fitting the model')\n",
    "    d = X_train[0].shape[1]\n",
    "    model = MaxLog(d=d, fit_intercept=True, alpha=10., epsilon=1e-4, init_b=logit(np.mean(y_train)), init_w=np.zeros(d))\n",
    "    model.fit(X_train, y_train, n_iter=3000, parallel=False)\n",
    "\n",
    "    print('Evaluation and storage of model parameters\\n')\n",
    "    \"\"\"Outputting the predictions and the probability scores\"\"\"\n",
    "    y_score = model.predict_proba(X_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    \"\"\"Computing the various metrics\"\"\"\n",
    "    f1_list.append(f1_score(y_test, y_pred))\n",
    "    precision_list.append(precision_score(y_test, y_pred))\n",
    "    recall_list.append(recall_score(y_test, y_pred))\n",
    "    roc_auc_list.append(roc_auc_score(y_test, y_score))\n",
    "\n",
    "    \"\"\"Adding predictions and scores to computed global cross-validated performance after the end of the process\"\"\"\n",
    "    Y_Pred.append(list(y_pred))\n",
    "    Y_Proba.append(list(y_score))\n",
    "\n",
    "    \"\"\"Storing the model's coefficients\"\"\"\n",
    "    biases.append(model.b)\n",
    "    weights.append(model.w)\n",
    "    \n",
    "print(\"Precision : {} +- {}\\nRecall : {} +- {}\\nF1 : {} +- {}\\nROC AUC : {} +- {}\".format(np.mean(precision_list), np.std(precision_list),\n",
    "                                                                                          np.mean(recall_list), np.std(recall_list),\n",
    "                                                                                          np.mean(f1_list), np.std(f1_list),\n",
    "                                                                                          np.mean(roc_auc_list), np.std(roc_auc_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
