{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrg-researchlab/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/qrg-researchlab/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/qrg-researchlab/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/qrg-researchlab/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/qrg-researchlab/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "from scipy.special import logit, expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxLog:\n",
    "    \n",
    "    def __init__(self, d, fit_intercept=True, init_w=None, init_b=None, alpha=1e-3, epsilon=1e-2):\n",
    "        \"\"\"Defining the dimension and fit_intercept parameters\"\"\"\n",
    "        self.dim = d\n",
    "        self.grad = np.zeros(self.dim)\n",
    "        self.fit_intercept = fit_intercept\n",
    "        \n",
    "        \"\"\"Initializing the weights\"\"\"\n",
    "        self.w = 0.\n",
    "        if init_w is None:\n",
    "            self.w = np.random.normal(size=self.dim)\n",
    "        elif isinstance(init_w, np.ndarray) and init_w.shape[0]==self.dim:\n",
    "            self.w = init_w\n",
    "        else:\n",
    "            raise ValueError('Wrong dimension in intial parameters.')\n",
    "        \n",
    "        \"\"\"Initializing the bias\"\"\"\n",
    "        self.b = 0.\n",
    "        if fit_intercept:\n",
    "            if init_b is None:\n",
    "                self.b = np.random.normal()\n",
    "            elif isinstance(init_b, (float, np.float64)):\n",
    "                self.b = init_b\n",
    "            else:\n",
    "                raise ValueError('Bias initialization must be a float type.')\n",
    "                \n",
    "        \"\"\"Setting the learning rate\"\"\"\n",
    "        if isinstance(alpha, (float, np.float64)):\n",
    "            self.alpha = alpha\n",
    "        else:\n",
    "            raise ValueError('Learning rate must be a float type.')\n",
    "        \n",
    "        \"\"\"Setting the stopping criterion for the gradient descent\"\"\"\n",
    "        if isinstance(epsilon, (float, np.float64)):\n",
    "            self.epsilon = epsilon\n",
    "        else:\n",
    "            raise ValueError('Tolerance must be a float type.')\n",
    "        \n",
    "        \"\"\"Initializing error sequence\"\"\"\n",
    "        self.errors = []\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def check_dim(self, X):\n",
    "        \"\"\"Computing check on dimension of observations\"\"\"\n",
    "        dim_check = any(x.shape[1]!=self.dim for x in X)\n",
    "        if dim_check:\n",
    "            raise ValueError('Wrong dimension in observations')\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def check_attr(self, attr, message=''):\n",
    "        \"\"\"Function to check whether the instance has an attribute\"\"\"\n",
    "        if not hasattr(self, attr):\n",
    "            if message=='':\n",
    "                raise ValueError(\"Instance doesn't have attribute '{}'\".format(attr))\n",
    "            else:\n",
    "                raise ValueError(message)\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Computing check on dimension of observations\"\"\"\n",
    "        self.check_dim(X)\n",
    "            \n",
    "        \"\"\"Computing the predictions\"\"\"\n",
    "        pred = [np.max(expit(self.b + x.dot(self.w))) for x in X]\n",
    "        return np.array([int(p>=0.5) for p in pred])\n",
    "    \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Computing check on dimension of observations\"\"\"\n",
    "        self.check_dim(X)\n",
    "            \n",
    "        \"\"\"Computing the probability\"\"\"\n",
    "        pred = [np.max(expit(self.b + x.dot(self.w))) for x in X]\n",
    "        return np.array(pred)\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, parallel=False, n_iter=None):\n",
    "        \"\"\"Function to fit the model\n",
    "        Needs : compute the forward propagation, compute the backward propagation\"\"\"\n",
    "        \"\"\"Computing check on dimension of observations\"\"\"\n",
    "        self.check_dim(X)\n",
    "        \n",
    "        \"\"\"Initiating the error sequence\"\"\"\n",
    "        self.errors = []\n",
    "        self.grad = 10\n",
    "        \n",
    "        \"\"\"Performing the gradient descent\"\"\"\n",
    "        error = np.inf\n",
    "        if n_iter==None:\n",
    "            while error >= self.epsilon:\n",
    "                self.forward(X, y)\n",
    "                self.backward(X, y)\n",
    "                error = self.grad\n",
    "        else:\n",
    "            for k in tqdm(range(n_iter), desc='Fitting the model'):\n",
    "                self.forward(X, y)\n",
    "                self.backward(X, y)\n",
    "                if self.grad<=self.epsilon:\n",
    "                    break\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def forward(self, X, y):\n",
    "        \"\"\"Performing the forward pass of the model : registering the maximum indexes, the current proba scores, and the error\"\"\"\n",
    "        probas = [expit(self.b + x.dot(self.w)) for x in X]\n",
    "        self.max_ind = [np.argmax(v) for v in probas]\n",
    "        y_pred = np.array([np.max(v) for v in probas])\n",
    "        self.eta = y_pred\n",
    "        self.errors.append(- np.mean(np.array(y) * np.log(y_pred) + (1-np.array(y)) * np.log(1-y_pred)))\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        grad_w, grad_b = self.gradient(X, y)\n",
    "        self.w = self.w - self.alpha * grad_w\n",
    "        self.b = self.b - self.alpha * grad_b\n",
    "        self.grad = np.sqrt(np.sum(grad_w**2)+grad_b**2)\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def gradient(self, X, y):\n",
    "        \"\"\"Check on error and maximum index for gradient computation\"\"\"\n",
    "        self.check_attr('errors', message='Need to compute forward pass to have error')\n",
    "        self.check_attr('max_ind', message='Need to compute forward pass to have maximum index')\n",
    "        self.check_attr('eta', message='Need to compute forward pass to have current estimate probabilities')\n",
    "        \n",
    "        \"\"\"Computing the gradient for every observation\"\"\"\n",
    "        mi = self.max_ind\n",
    "        current_X = np.vstack([X[k][mi[k]].reshape((1,-1)) for k in range(len(X))])\n",
    "        grad_w = current_X.T.dot(self.eta - np.array(y))/current_X.shape[0]\n",
    "        grad_b = 0.\n",
    "        if self.fit_intercept:\n",
    "            grad_b = np.mean(self.eta - np.array(y))\n",
    "        return grad_w, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loading the features\"\"\"\n",
    "top_verbs_30 = pickle.load(open('../features_30/top_verbs_30.p', 'rb'))\n",
    "bot_verbs_30 = pickle.load(open('../features_30/bot_verbs_30.p', 'rb'))\n",
    "top_adverbs_30 = pickle.load(open('../features_30/top_adverbs_30.p', 'rb'))\n",
    "bot_adverbs_30 = pickle.load(open('../features_30/bot_adverbs_30.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loading the neighborhoods and the target\"\"\"\n",
    "actual_words_9 = pickle.load(open('../neighborhoods/actual_words_hashtag_free_9.p','rb'))\n",
    "hashtag_words = pickle.load(open('../neighborhoods/hashtag_words.p','rb'))\n",
    "data = pd.read_csv('../basic_data.csv', sep='\\t')\n",
    "ys = list(data.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def represent(pattern):\n",
    "    tv = [int(v in pattern) for v in top_verbs_30]\n",
    "    bv = [int(v in pattern) for v in bot_verbs_30]\n",
    "    ta = [int(v in pattern) for v in top_adverbs_30]\n",
    "    ba = [int(v in pattern) for v in bot_adverbs_30]\n",
    "    return tv+bv+ta+ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e543b36d244511bd4d94a2fd55a481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11540), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456ee8a1b4874595a710a61a3a61e447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11540), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb58ef4a69df4e4ebee4c3fb5145c170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11540), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45fec7d95ad2439590d412c356e61bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11540), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca07a6096d741ecb048298ddfb8c9cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11540), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4c72c1228f467584d1aaa639f540b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11540), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_basic = [[represent(pattern) for pattern in tweet] for tweet in tqdm(actual_words_9)]\n",
    "X_hashtag = [[represent(pattern) for pattern in tweet] for tweet in tqdm(hashtag_words)]\n",
    "\n",
    "X_full = [X_basic[k] + X_hashtag[k] for k in tqdm(range(len(ys)))]\n",
    "\n",
    "X_basic = [np.array(tweet) for tweet in tqdm(X_basic)]\n",
    "X_full = [np.array(tweet) for tweet in tqdm(X_full)]\n",
    "\n",
    "d = len(top_adverbs_30) + len(top_verbs_30) + len(bot_adverbs_30) + len(bot_verbs_30)\n",
    "for k in tqdm(range(len(X_basic))):\n",
    "    if X_basic[k].shape[0]==0:\n",
    "        X_basic[k] = np.zeros((1,d))\n",
    "    if X_full[k].shape[0]==0:\n",
    "        X_full[k] = np.zeros((1,d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Building indexes\"\"\"\n",
    "\n",
    "n = len(ys)\n",
    "d = X_basic[0].shape[0]\n",
    "\n",
    "\"\"\"Retrieving the positive and negative indexes\"\"\"\n",
    "neg_index = [k for k in range(len(ys)) if ys[k]==0]\n",
    "pos_index = [k for k in range(len(ys)) if ys[k]==1]\n",
    "\n",
    "\"\"\"Shuffling both the positive and negative indexes\"\"\"\n",
    "np.random.seed(seed=0)\n",
    "np.random.shuffle(neg_index)\n",
    "np.random.shuffle(pos_index)\n",
    "\n",
    "\"\"\"Computing train and test index sets for the chosen number of folds\"\"\"\n",
    "cv = 10\n",
    "\n",
    "s = len(neg_index)//cv\n",
    "neg_index_list = [neg_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "neg_index_list.append(neg_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in neg_index_list])==len(neg_index)) # Check on negative index completion\n",
    "\n",
    "s = len(pos_index)//cv\n",
    "pos_index_list = [pos_index[k*s:(k+1)*s] for k in range(cv-1)]\n",
    "pos_index_list.append(pos_index[(cv-1)*s:])\n",
    "assert(np.sum([len(e) for e in pos_index_list])==len(pos_index))# Check on positive index completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f19faf232644918692b37430809fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "2-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4dcdf3ad23146838661049ae2e2f879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "3-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c454f18d9ee44184b6c32f36dfcb4eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "4-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e4e454f4134642a7a522112b9ff981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "5-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2965100d58434d8f509a0909f3092d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "6-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf8e0329d8a4c86aa9918b8c271754d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "7-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874152fb9f5642b68992b5dc52159648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "8-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972651253f7244ed82c8b6deae719172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "9-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba472e818e974014b390f435a681cdf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "10-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702c82bffec54e52a1d23a6d81cb2af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Cross-validation on 30% features, without hashtags\"\"\"\n",
    "from scipy.special import logit, expit\n",
    "\n",
    "\"\"\"Initializing the list of outputs, predictions and probabilities to computed CV-ly\"\"\"\n",
    "Y = ys\n",
    "Y_Pred = []\n",
    "Y_Proba = []\n",
    "\n",
    "\"\"\"Intializing the list of cross-validated features\"\"\"\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "roc_auc_list = []\n",
    "f1_list = []\n",
    "\n",
    "\"\"\"Storing coefficients and biases for stability evaluation\"\"\"\n",
    "biases = []\n",
    "weights = []\n",
    "\n",
    "\"\"\"Performing the cross-validation of the model using the features of interest\n",
    "A caveat to be mentioned is that the feature selection was performed on the whole dataset, which may be a little biased towards choosing the right features.\n",
    "This effect will be neglected during this test.\"\"\"\n",
    "for k in range(cv):\n",
    "    \"\"\"Splitting the data into train and test\"\"\"\n",
    "    print('{}-th fold'.format(k+1))\n",
    "    print('Splitting the data')\n",
    "    neg_test = [X_basic[i] for i in neg_index_list[k]]\n",
    "    pos_test = [X_basic[i] for i in pos_index_list[k]]\n",
    "    neg_train = [X_basic[i] for i in set(neg_index).difference(neg_index_list[k])]\n",
    "    pos_train = [X_basic[i] for i in set(pos_index).difference(pos_index_list[k])]\n",
    "\n",
    "    X_train = pos_train + neg_train\n",
    "    X_test = pos_test + neg_test\n",
    "\n",
    "    y_train = [1] * len(pos_train) + [0] * len(neg_train)\n",
    "    y_test = [1] * len(pos_test) + [0] * len(neg_test)\n",
    "\n",
    "    \"\"\"Fitting the model\"\"\"\n",
    "    print('Fitting the model')\n",
    "    d = X_train[0].shape[1]\n",
    "    model = MaxLog(d=d, fit_intercept=True, alpha=10., epsilon=1e-4, init_b=logit(np.mean(y_train)), init_w=np.zeros(d))\n",
    "    model.fit(X_train, y_train, n_iter=4000, parallel=False)\n",
    "\n",
    "    print('Evaluation and storage of model parameters\\n')\n",
    "    \"\"\"Outputting the predictions and the probability scores\"\"\"\n",
    "    y_score = model.predict_proba(X_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    \"\"\"Computing the various metrics\"\"\"\n",
    "    f1_list.append(f1_score(y_test, y_pred))\n",
    "    precision_list.append(precision_score(y_test, y_pred))\n",
    "    recall_list.append(recall_score(y_test, y_pred))\n",
    "    roc_auc_list.append(roc_auc_score(y_test, y_score))\n",
    "\n",
    "    \"\"\"Adding predictions and scores to computed global cross-validated performance after the end of the process\"\"\"\n",
    "    Y_Pred.append(list(y_pred))\n",
    "    Y_Proba.append(list(y_score))\n",
    "\n",
    "    \"\"\"Storing the model's coefficients\"\"\"\n",
    "    biases.append(model.b)\n",
    "    weights.append(model.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision : 0.8037526794498756\n",
      "Average recall : 0.582169849654612\n",
      "Average F1 score : 0.6741729374428147\n",
      "Average ROC score : 0.9031240105709198\n"
     ]
    }
   ],
   "source": [
    "print('Average precision : {}'.format(np.mean(precision_list)))\n",
    "print('Average recall : {}'.format(np.mean(recall_list)))\n",
    "print('Average F1 score : {}'.format(np.mean(f1_list)))\n",
    "print('Average ROC score : {}'.format(np.mean(roc_auc_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9f19d651144eb4a42fdb23804f0945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "2-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f65e94dbb842f7815726ffe35e92b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "3-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fef4e7152d748b69f6303a68cf8b5c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "4-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ed21e497614da1babf1e065a9d95fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "5-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c13bd579bd848cf87743b4be0d0d666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "6-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4711dc0f8ea470aa61d0cf19e1a5f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "7-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3725bc38d53f4a6aa83296eb98e6acff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "8-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d2a389a644486c972306f1f2f739b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "9-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b43535f1fba349969e22c5ade05ff71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "10-th fold\n",
      "Splitting the data\n",
      "Fitting the model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59643282b56746659bfdacca02e4b3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Fitting the model', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation and storage of model parameters\n",
      "\n",
      "Average precision : 0.8381609361799812\n",
      "Average recall : 0.741202763104429\n",
      "Average F1 score : 0.7860758257277707\n",
      "Average ROC score : 0.9649338558588403\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Cross-validation on 5% features, with hashtags\"\"\"\n",
    "from scipy.special import logit\n",
    "\n",
    "\"\"\"Initializing the list of outputs, predictions and probabilities to computed CV-ly\"\"\"\n",
    "Y = ys\n",
    "Y_Pred = []\n",
    "Y_Proba = []\n",
    "\n",
    "\"\"\"Intializing the list of cross-validated features\"\"\"\n",
    "precision_list_full = []\n",
    "recall_list_full = []\n",
    "roc_auc_list_full = []\n",
    "f1_list_full = []\n",
    "\n",
    "\"\"\"Storing coefficients and biases for stability evaluation\"\"\"\n",
    "biases_full = []\n",
    "weights_full = []\n",
    "\n",
    "\"\"\"Performing the cross-validation of the model using the features of interest\n",
    "A caveat to be mentioned is that the feature selection was performed on the whole dataset, which may be a little biased towards choosing the right features.\n",
    "This effect will be neglected during this test.\"\"\"\n",
    "for k in range(cv):\n",
    "    \"\"\"Splitting the data into train and test\"\"\"\n",
    "    print('{}-th fold'.format(k+1))\n",
    "    print('Splitting the data')\n",
    "    neg_test = [X_full[i] for i in neg_index_list[k]]\n",
    "    pos_test = [X_full[i] for i in pos_index_list[k]]\n",
    "    neg_train = [X_full[i] for i in set(neg_index).difference(neg_index_list[k])]\n",
    "    pos_train = [X_full[i] for i in set(pos_index).difference(pos_index_list[k])]\n",
    "\n",
    "    X_train = pos_train + neg_train\n",
    "    X_test = pos_test + neg_test\n",
    "\n",
    "    y_train = [1] * len(pos_train) + [0] * len(neg_train)\n",
    "    y_test = [1] * len(pos_test) + [0] * len(neg_test)\n",
    "\n",
    "    \"\"\"Fitting the model\"\"\"\n",
    "    print('Fitting the model')\n",
    "    d = X_train[0].shape[1]\n",
    "    model = MaxLog(d=d, fit_intercept=True, alpha=10., epsilon=1e-4, init_b=logit(np.mean(y_train)), init_w=np.zeros(d))\n",
    "    model.fit(X_train, y_train, n_iter=4000, parallel=False)\n",
    "\n",
    "    print('Evaluation and storage of model parameters\\n')\n",
    "    \"\"\"Outputting the predictions and the probability scores\"\"\"\n",
    "    y_score = model.predict_proba(X_test)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    \"\"\"Computing the various metrics\"\"\"\n",
    "    f1_list_full.append(f1_score(y_test, y_pred))\n",
    "    precision_list_full.append(precision_score(y_test, y_pred))\n",
    "    recall_list_full.append(recall_score(y_test, y_pred))\n",
    "    roc_auc_list_full.append(roc_auc_score(y_test, y_score))\n",
    "\n",
    "    \"\"\"Adding predictions and scores to computed global cross-validated performance after the end of the process\"\"\"\n",
    "    Y_Pred.append(list(y_pred))\n",
    "    Y_Proba.append(list(y_score))\n",
    "\n",
    "    \"\"\"Storing the model's coefficients\"\"\"\n",
    "    biases_full.append(model.b)\n",
    "    weights_full.append(model.w)\n",
    "    \n",
    "print('Average precision : {}'.format(np.mean(precision_list_full)))\n",
    "print('Average recall : {}'.format(np.mean(recall_list_full)))\n",
    "print('Average F1 score : {}'.format(np.mean(f1_list_full)))\n",
    "print('Average ROC score : {}'.format(np.mean(roc_auc_list_full)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th fold\n",
      "Splitting the data\n",
      "2-th fold\n",
      "Splitting the data\n",
      "3-th fold\n",
      "Splitting the data\n",
      "4-th fold\n",
      "Splitting the data\n",
      "5-th fold\n",
      "Splitting the data\n",
      "6-th fold\n",
      "Splitting the data\n",
      "7-th fold\n",
      "Splitting the data\n",
      "8-th fold\n",
      "Splitting the data\n",
      "9-th fold\n",
      "Splitting the data\n",
      "10-th fold\n",
      "Splitting the data\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Benchmarking against plain logistic regression and Naive Bayes\"\"\"\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\"\"\"Initializing the list of outputs, predictions and probabilities to computed CV-ly\"\"\"\n",
    "Y = ys\n",
    "Y_Pred = []\n",
    "Y_Proba = []\n",
    "\n",
    "\"\"\"Intializing the list of cross-validated metrics\"\"\"\n",
    "precision_log = []\n",
    "recall_log = []\n",
    "roc_log = []\n",
    "f1_log = []\n",
    "\n",
    "precision_mnb = []\n",
    "recall_mnb = []\n",
    "roc_mnb = []\n",
    "f1_mnb = []\n",
    "\n",
    "\"\"\"Storing coefficients and biases for stability evaluation\"\"\"\n",
    "biases_full = []\n",
    "weights_full = []\n",
    "\n",
    "\"\"\"Performing the cross-validation of the model using the features of interest\n",
    "A caveat to be mentioned is that the feature selection was performed on the whole dataset, which may be a little biased towards choosing the right features.\n",
    "This effect will be neglected during this test.\"\"\"\n",
    "for k in range(cv):\n",
    "    \"\"\"Splitting the data into train and test\"\"\"\n",
    "    print('{}-th fold'.format(k+1))\n",
    "    print('Splitting the data')\n",
    "    neg_test = [np.minimum(np.sum(X_full[i] ,axis=0), np.ones(d)) for i in neg_index_list[k]]\n",
    "    pos_test = [np.minimum(np.sum(X_full[i] ,axis=0), np.ones(d)) for i in pos_index_list[k]]\n",
    "    neg_train = [np.minimum(np.sum(X_full[i] ,axis=0), np.ones(d)) for i in set(neg_index).difference(neg_index_list[k])]\n",
    "    pos_train = [np.minimum(np.sum(X_full[i] ,axis=0), np.ones(d)) for i in set(pos_index).difference(pos_index_list[k])]\n",
    "\n",
    "    X_train = pos_train + neg_train\n",
    "    X_test = pos_test + neg_test\n",
    "\n",
    "    y_train = [1] * len(pos_train) + [0] * len(neg_train)\n",
    "    y_test = [1] * len(pos_test) + [0] * len(neg_test)\n",
    "    \n",
    "    logistic = LogisticRegression(C=1e5, fit_intercept=True)\n",
    "    logistic.fit(X_train, y_train)\n",
    "    y_pred_log = logistic.predict(X_test)\n",
    "    y_score_log = logistic.predict_proba(X_test)[:,1]\n",
    "    precision_log.append(precision_score(y_test, y_pred_log))\n",
    "    recall_log.append(recall_score(y_test, y_pred_log))\n",
    "    f1_log.append(f1_score(y_test, y_pred_log))\n",
    "    roc_log.append(roc_auc_score(y_test, y_score_log))\n",
    "    \n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(X_train, y_train)\n",
    "    y_pred_mnb = mnb.predict(X_test)\n",
    "    y_score_mnb = mnb.predict_proba(X_test)[:,1]\n",
    "    precision_mnb.append(precision_score(y_test, y_pred_mnb))\n",
    "    recall_mnb.append(recall_score(y_test, y_pred_mnb))\n",
    "    f1_mnb.append(f1_score(y_test, y_pred_mnb))\n",
    "    roc_mnb.append(roc_auc_score(y_test, y_score_mnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision : 0.7847993425476396\n",
      "Average recall : 0.6820398212108898\n",
      "Average F1 score : 0.7291243885622529\n",
      "Average ROC AUC : 0.9486631304805566\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Results for plain logisitic regression\"\"\"\n",
    "print('Average precision : {}'.format(np.mean(precision_log)))\n",
    "print('Average recall : {}'.format(np.mean(recall_log)))\n",
    "print('Average F1 score : {}'.format(np.mean(f1_log)))\n",
    "print('Average ROC AUC : {}'.format(np.mean(roc_log)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision : 0.760264684849509\n",
      "Average recall : 0.602056074766355\n",
      "Average F1 score : 0.6715919268739444\n",
      "Average ROC AUC : 0.9164547088196343\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Results for plain Naive Bayes\"\"\"\n",
    "print('Average precision : {}'.format(np.mean(precision_mnb)))\n",
    "print('Average recall : {}'.format(np.mean(recall_mnb)))\n",
    "print('Average F1 score : {}'.format(np.mean(f1_mnb)))\n",
    "print('Average ROC AUC : {}'.format(np.mean(roc_mnb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th fold\n",
      "Splitting the data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrg-researchlab/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-th fold\n",
      "Splitting the data\n",
      "3-th fold\n",
      "Splitting the data\n",
      "4-th fold\n",
      "Splitting the data\n",
      "5-th fold\n",
      "Splitting the data\n",
      "6-th fold\n",
      "Splitting the data\n",
      "7-th fold\n",
      "Splitting the data\n",
      "8-th fold\n",
      "Splitting the data\n",
      "9-th fold\n",
      "Splitting the data\n",
      "10-th fold\n",
      "Splitting the data\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Benchmarking against L1 logistic regression\"\"\"\n",
    "import copy\n",
    "\n",
    "\"\"\"Initializing the list of outputs, predictions and probabilities to computed CV-ly\"\"\"\n",
    "Y = ys\n",
    "Y_Pred = []\n",
    "Y_Proba = []\n",
    "\n",
    "\"\"\"Intializing the list of cross-validated metrics\"\"\"\n",
    "precision_l1 = []\n",
    "recall_l1 = []\n",
    "roc_l1 = []\n",
    "f1_l1 = []\n",
    "\n",
    "lamdas = np.arange(-3,3.5,0.5)\n",
    "\n",
    "\"\"\"Performing the cross-validation of the model using the features of interest\n",
    "A caveat to be mentioned is that the feature selection was performed on the whole dataset, which may be a little biased towards choosing the right features.\n",
    "This effect will be neglected during this test.\"\"\"\n",
    "for k in range(cv):\n",
    "    \"\"\"Splitting the data into train and test\"\"\"\n",
    "    print('{}-th fold'.format(k+1))\n",
    "    print('Splitting the data')\n",
    "    neg_test = [np.minimum(np.sum(X_full[i] ,axis=0), np.ones(d)) for i in neg_index_list[k]]\n",
    "    pos_test = [np.minimum(np.sum(X_full[i] ,axis=0), np.ones(d)) for i in pos_index_list[k]]\n",
    "    neg_train = [np.minimum(np.sum(X_full[i] ,axis=0), np.ones(d)) for i in set(neg_index).difference(neg_index_list[k])]\n",
    "    pos_train = [np.minimum(np.sum(X_full[i] ,axis=0), np.ones(d)) for i in set(pos_index).difference(pos_index_list[k])]\n",
    "\n",
    "    X_train = pos_train + neg_train\n",
    "    X_test = pos_test + neg_test\n",
    "\n",
    "    y_train = [1] * len(pos_train) + [0] * len(neg_train)\n",
    "    y_test = [1] * len(pos_test) + [0] * len(neg_test)\n",
    "    \n",
    "    f1_reg = []\n",
    "    for C in 10**(-lamdas):\n",
    "        logistic = LogisticRegression(C=C, penalty='l1', fit_intercept=True)\n",
    "        logistic.fit(X_train, y_train)\n",
    "        f1_reg.append(f1_score(y_train, logistic.predict(X_train)))\n",
    "        \n",
    "    C_max = 10**(-lamdas[np.argmax(f1_reg)])\n",
    "    logistic = LogisticRegression(C=C_max, penalty='l1', fit_intercept=True)\n",
    "    logistic.fit(X_train, y_train)\n",
    "    y_pred = logistic.predict(X_test)\n",
    "    y_score = logistic.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    precision_l1.append(precision_score(y_test, y_pred))\n",
    "    recall_l1.append(recall_score(y_test, y_pred))\n",
    "    f1_l1.append(f1_score(y_test, y_pred))\n",
    "    roc_l1.append(roc_auc_score(y_test, y_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision : 0.7816094768810605\n",
      "Average recall : 0.6848435595286468\n",
      "Average F1 score : 0.7293428063500023\n",
      "Average ROC AUC : 0.953059241388672\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Computing average performance over best model\"\"\"\n",
    "print('Average precision : {}'.format(np.mean(precision_l1)))\n",
    "print('Average recall : {}'.format(np.mean(recall_l1)))\n",
    "print('Average F1 score : {}'.format(np.mean(f1_l1)))\n",
    "print('Average ROC AUC : {}'.format(np.mean(roc_l1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
