{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ark_tweet_nlp import CMUTweetTagger\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "import pickle\n",
    "import copy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loading the tagged data\"\"\"\n",
    "data = pickle.load(open('./label_tag_data.p', 'rb'))\n",
    "! mkdir hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb68627bdf284ac7a55a297785c7bb96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11540), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A total of 2546 hastags\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Harvesting the hashtags\"\"\"\n",
    "hashtag_set = set()\n",
    "\n",
    "for k in tqdm(range(data.shape[0])):\n",
    "    local = data.tag_df.iloc[k]\n",
    "    for i in range(local.shape[0]):\n",
    "        if local.tag.iloc[i]=='#' or '#' in local.word.iloc[i]:\n",
    "            hashtag_set.add(local.word.iloc[i])\n",
    "            \n",
    "print('A total of {} hastags'.format(len(hashtag_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf13ac8835454eac93d8b9ee9be72ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Building corpus', max=11540), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e27ba0283e14025bf2881b50f020310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Removing concatenations of basics', max=11040), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"1. Building corpus\"\"\"\n",
    "corpus = set()\n",
    "\n",
    "for k in tqdm(range(data.shape[0]), desc='Building corpus'):\n",
    "    local = data.tag_df.iloc[k]\n",
    "    for i in range(local.shape[0]):\n",
    "        if any(str(k) in local.word.iloc[i] for k in range(10)):\n",
    "            continue\n",
    "        if '#' in local.word.iloc[i]:\n",
    "            continue\n",
    "        if local.tag.iloc[i] in ['#','U','&',',','O','$','D','!','^','@']:\n",
    "            continue\n",
    "        if \"'\" in local.word.iloc[i]:\n",
    "            continue\n",
    "        if '_' in local.word.iloc[i]:\n",
    "            continue\n",
    "        w = re.split('\\W+', local.word.iloc[i].replace(\"n't\",\" not\"))\n",
    "        corpus.update(set(w))\n",
    "        \n",
    "corpus.remove('')\n",
    "corpus = set([e for e in corpus])\n",
    "\n",
    "list_corpus = list(corpus)\n",
    "to_remove = []\n",
    "keepers = ['an','of','air','ways','lines','ing','lon','un','don'] # Some problematic tokens in the segmentation phase\n",
    "\n",
    "for i in tqdm(range(len(list_corpus)), desc='Removing concatenations of basics'):\n",
    "    w_i = list_corpus[i]\n",
    "    if w_i in keepers or len(w_i)<3:\n",
    "        continue\n",
    "    for j in range(i+1, len(list_corpus)):\n",
    "        w_j = list_corpus[j]\n",
    "        if w_j in keepers:\n",
    "            continue\n",
    "        if w_i+w_j in corpus:\n",
    "            to_remove.append(w_i+w_j)\n",
    "        if w_j+w_i in corpus:\n",
    "            to_remove.append(w_j+w_i)\n",
    "            \n",
    "corpus = set([e.replace(\"'\",'') for e in corpus if len(e)>1])\n",
    "corpus.remove('baagain') # Removing a problematic token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562580794aa24fe181d36e0fee6812f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Extracting valid splits', max=2546), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"2. Function to find the splits with only known words\"\"\"\n",
    "def valid_split(ht):\n",
    "    \n",
    "    if ht in corpus:\n",
    "        return [[ht]]\n",
    "    \n",
    "    components = [e for e in corpus if e in ht]\n",
    "    queue = [[ht]]\n",
    "    candidates = []\n",
    "    \n",
    "    while len(queue)>0:\n",
    "        # Dequeuing\n",
    "        next_c = []\n",
    "        base = queue[-1][:-1]\n",
    "        suite = queue[-1][-1]\n",
    "        del queue[-1]\n",
    "        \n",
    "        # Looking for news candidates\n",
    "        for c in components:\n",
    "            if c==suite[:len(c)] and len(c)<len(suite):\n",
    "                next_c.append(base + [suite[:len(c)], suite[len(c):]])\n",
    "                candidates.append(next_c[-1])\n",
    "            elif c==suite[:len(c)] and len(c)==len(suite):\n",
    "                # If no more characters in string add words\n",
    "                candidates.append(base + [suite[:len(c)]])\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        # Growing the queue\n",
    "        check = [e for e in next_c if not any(f not in corpus for f in e[:-1])]\n",
    "        queue += check\n",
    "        \n",
    "    return [e for e in candidates if e[-1] in corpus]\n",
    "\n",
    "hashtag_list = list(hashtag_set)\n",
    "candidate_lists = [valid_split(e.replace('#','')) for e in tqdm(hashtag_list, desc='Extracting valid splits')]\n",
    "\n",
    "\"\"\"Dropping duplicates in candidates\"\"\"\n",
    "candidate_lists = [[list(a) for a in list(set([tuple(e) for e in f]))] for f in candidate_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceed43836ef54267a065e1c2271b728d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Building the co-occurrence matrix', max=11540), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrg-researchlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n"
     ]
    }
   ],
   "source": [
    "\"\"\"3. Computing the cooccurrence matrix\"\"\"\n",
    "\n",
    "coocc = {}\n",
    "\n",
    "for k in tqdm(range(data.shape[0]), desc='Building the co-occurrence matrix'):\n",
    "    local = data.tag_df.iloc[k]\n",
    "    local = local[local.tag.isin(['#','U','&',',','O','$','D','!','^','@']).apply(lambda x: not x)]\n",
    "    local = local[local.word.apply(lambda x: '#' not in x)]\n",
    "    if local.shape[0]>1:\n",
    "        local = list(local.word)\n",
    "        for i in range(1, len(local)):\n",
    "            w1 = local[i-1]\n",
    "            w2 = local[i]\n",
    "            try:\n",
    "                coocc[w1][w2] += 1\n",
    "            except:\n",
    "                try:\n",
    "                    coocc[w1][w2] = 1\n",
    "                except:\n",
    "                    coocc[w1] = {w2: 1}\n",
    "                    \n",
    "coocc = pd.DataFrame(coocc).fillna(0.)\n",
    "relevant = list(corpus.intersection(list(coocc.index)))\n",
    "coocc = coocc.loc[relevant, relevant].fillna(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac650730ae547a784eacc36ec34d370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Computing EV for candidates', max=2546), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"4. Computing External Value for the various configurations as the product of \n",
    "left-side value and right-side value\"\"\"\n",
    "\n",
    "dict_hdf_ev = {}\n",
    "for k in tqdm(range(len(hashtag_list)), desc='Computing EV for candidates'):\n",
    "    dict_hdf_ev[hashtag_list[k]] = {}\n",
    "    for e in candidate_lists[k]:\n",
    "        ev = []\n",
    "        for w in e:\n",
    "            try:\n",
    "                ev.append(coocc[w].sum() * coocc.loc[w].sum())\n",
    "            except:\n",
    "                ev.append(0.)\n",
    "        dict_hdf_ev[hashtag_list[k]][tuple(e)] = ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285c845b8a2c4df69f044fcee52c76f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Computing the Mutual Information between characters', max=115…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrg-researchlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:32: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n"
     ]
    }
   ],
   "source": [
    "\"\"\"5. Computing Internal value for the various splits\"\"\"\n",
    "\n",
    "\"\"\"First, computing the Mutual Information scores of the splits\"\"\"\n",
    "\n",
    "mi_matrix = {}\n",
    "counts = {}\n",
    "\n",
    "for k in tqdm(range(data.shape[0]), desc='Computing the Mutual Information between characters'):\n",
    "    local = data.tag_df.iloc[k]\n",
    "    local = local[local.tag.isin(['#','U','&',',','O','$','D','!','^','@']).apply(lambda x: not x)]\n",
    "    local = local[local.word.apply(lambda x: '#' not in x)]\n",
    "    if local.shape[0]>1:\n",
    "        local = list(local.word)\n",
    "        for i in range(1, len(local)):\n",
    "            c1 = local[i-1][-1]\n",
    "            c2 = local[i][0]\n",
    "            # Updating counts\n",
    "            try:\n",
    "                counts[c1] += 1\n",
    "            except:\n",
    "                counts[c1] = 1\n",
    "            # Updating follow ups\n",
    "            try:\n",
    "                mi_matrix[c1][c2] += 1\n",
    "            except:\n",
    "                try:\n",
    "                    mi_matrix[c1][c2] = 1\n",
    "                except:\n",
    "                    mi_matrix[c1] = {c2: 1}\n",
    "                            \n",
    "mi_matrix = pd.DataFrame(mi_matrix)\n",
    "mi_matrix = mi_matrix.loc[list(mi_matrix.index), list(mi_matrix.index)]\n",
    "mi_df = (mi_matrix + mi_matrix.T)\n",
    "mi_df.fillna(0., inplace=True)        \n",
    "\n",
    "\"\"\"Computing the probabilities\"\"\"\n",
    "\n",
    "base_proba = pd.Series(counts)\n",
    "pair_proba = mi_df/np.sum(base_proba)\n",
    "base_proba /= np.sum(base_proba)\n",
    "\n",
    "\"\"\"Computing Mutual Information\"\"\"\n",
    "mi_matrix = pair_proba * pd.DataFrame(1/np.array(base_proba).reshape((-1,1)).dot(np.array(base_proba).reshape((1,-1))), index=list(base_proba.index), columns=list(base_proba.index))\n",
    "mi_matrix.fillna(0., inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46dd56cecd474ef7b6672e491935b735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Computing IV for candidates', max=2546), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Second for every candidate split, we compute the Boundary scores\"\"\"\n",
    "dict_hdf_iv = {}\n",
    "for k in tqdm(range(len(hashtag_list)), desc='Computing IV for candidates'):\n",
    "    dict_hdf_iv[hashtag_list[k]] = {}\n",
    "    for e in candidate_lists[k]:\n",
    "        if len(e)<=1:\n",
    "            dict_hdf_iv[hashtag_list[k]][tuple(e)] = []\n",
    "        else:\n",
    "            iv = []\n",
    "            for i in range(1, len(e)):\n",
    "                iv.append(mi_matrix.loc[e[i-1][-1], e[i][0]])\n",
    "            dict_hdf_iv[hashtag_list[k]][tuple(e)] = iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Computing the Word Rank score as the product of \n",
    "- the geometric mean of EV (intuition : high EV reflects high word importance in the \n",
    "cooccurrence graph)\n",
    "- a decreasing function of maximum IV (intuition : IV is large when split is very likely, \n",
    "large min IV indicates high probability of split validity)\"\"\"\n",
    "\n",
    "dict_EV = {}\n",
    "for ht in dict_hdf_ev.keys():\n",
    "    dict_EV[ht] = {}\n",
    "    if len(dict_hdf_ev[ht])>=1:\n",
    "        for c in dict_hdf_ev[ht]:\n",
    "            dict_EV[ht][c] = np.prod(dict_hdf_ev[ht][c])**(1./len(dict_hdf_ev[ht][c]))        \n",
    "\n",
    "dict_IV = {}\n",
    "for ht in dict_hdf_iv.keys():\n",
    "    dict_IV[ht] = {}\n",
    "    if len(dict_hdf_iv[ht])>=1:\n",
    "        for c in dict_hdf_iv[ht]:\n",
    "            if len(c)==1:\n",
    "                dict_IV[ht][c] = 100.\n",
    "            else:\n",
    "                dict_IV[ht][c] = np.min(dict_hdf_iv[ht][c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f290d9a6fb48b5952a01a8857ad30f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2546), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"6. Given the results, and the large presence of small letters words due to the errors\n",
    "in tweets, we'll two criteria to select the best split :\n",
    "1. Largest IV\n",
    "2. In case of ties, choose the split with smallest number of tokens\"\"\"\n",
    "\n",
    "split_hashtags = {}\n",
    "\n",
    "for k in tqdm(range(len(hashtag_list))):\n",
    "    ht = hashtag_list[k]\n",
    "    if len(list(dict_IV[ht].keys()))==0:\n",
    "        split_hashtags[ht] = ht.replace('#','')\n",
    "    elif len(list(dict_IV[ht].keys()))==1:\n",
    "        split_hashtags[ht] = ', '.join(list(dict_IV[ht].keys())[0])\n",
    "    else:\n",
    "        loc_dict = dict_IV[ht]\n",
    "        loc_iv = {', '.join(c): loc_dict[c] for c in loc_dict.keys()}\n",
    "        loc_iv_df = pd.DataFrame([pd.Series(loc_iv),\n",
    "                                  pd.Series([len(c.split(', ')) for c in pd.Series(loc_iv).index], index=pd.Series(loc_iv).index)],\n",
    "                                 index=['iv','len'])\n",
    "        loc_iv_df.sort_values(['iv','len'], ascending=[False,True], axis=1, inplace=True)\n",
    "        split_hashtags[ht] = loc_iv_df.columns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7902e62313a84e7086b5d161c8c327c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2546), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Tagging all the split hashtags\"\"\"\n",
    "hashtag_list = []\n",
    "tokenized_hashtags = []\n",
    "\n",
    "for ht in split_hashtags.keys():\n",
    "    hashtag_list.append(ht)\n",
    "    tokenized_hashtags.append(split_hashtags[ht].replace(',',''))\n",
    "\n",
    "def tagg(x):\n",
    "    return(CMUTweetTagger.runtagger_parse([x])[0])\n",
    "\n",
    "tagged_hashtags = [tagg(tht) for tht in tqdm(tokenized_hashtags)]\n",
    "\n",
    "tagged_hashtags = {hashtag_list[k]: tagged_hashtags[k] for k in range(len(hashtag_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(split_hashtags, open('./hashtags/split_hashtags.p','wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(tagged_hashtags, open('./hashtags/tagged_hashtags_last.p','wb'), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
