{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrg-researchlab/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/qrg-researchlab/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/qrg-researchlab/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/qrg-researchlab/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ark_tweet_nlp import CMUTweetTagger\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "import pickle\n",
    "import copy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loading the tagged data and the tagged \"\"\"\n",
    "data = pickle.load(open('./label_tag_data.p', 'rb'))\n",
    "tagged_hashtags = pickle.load(open('./hashtags/tagged_hashtags_last.p', 'rb'))\n",
    "! mkdir neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7433929fc8e34fcfacd6cec0aa5f3cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11540), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrg-researchlab/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Examining the type of patterns the verbs and adverbs appear in\"\"\"\n",
    "lmtzr = WordNetLemmatizer()\n",
    "actual_words_5 = []\n",
    "actual_words_7 = []\n",
    "actual_words_9 = []\n",
    "unique_verbs = set()\n",
    "\n",
    "for k in tqdm(range(data.shape[0])):\n",
    "    local = data.tag_df.iloc[k]\n",
    "    \n",
    "    \"\"\"Removing non interesting tags, tweet specific tags (emojis, existentials, numbers, URLs, the &, punctuation, unknown, @ mentions, determinants)\"\"\"\n",
    "    local = local[local.tag.isin(['U','&',',','$','D','!','^','L','M','X','Y','E','G']).apply(lambda x: not x)]\n",
    "    if local.shape[0]==0:\n",
    "        actual_words_5.append([])\n",
    "        actual_words_7.append([])\n",
    "        actual_words_9.append([])\n",
    "    else: \n",
    "        \"\"\"Lemmatizing the words to remove the verb and adverb tokens to be considered\"\"\"\n",
    "        local['word'] = local.T.apply(lambda x: lmtzr.lemmatize(x['word'].replace(\"'\",''), x['tag'].lower()) if x['tag'] in ['V','R','A','N'] else x['word'].replace(\"'\",''))\n",
    "\n",
    "        \"\"\"Extracting the verb and adverb patterns\"\"\"\n",
    "        local_words_5 = []\n",
    "        local_words_7 = []\n",
    "        local_words_9 = []\n",
    "        for i in range(local.shape[0]):\n",
    "            w = local.word.iloc[i]\n",
    "            if w.count('/')==1 and w.count('-')==0:\n",
    "                w = w.split('/')\n",
    "            if w.count('/')==0 and w.count('-')==1:\n",
    "                w = w.replace('-','')\n",
    "            if (local.tag.iloc[i] in ['V','R']) and ('#' not in w) and ('&' not in w) and not(any(str(k) in w for k in range(10))) and ('-' not in w) and ('/' not in w):\n",
    "                neighborhood_5 = local[['word','tag']].iloc[max(0,i-2):min(local.shape[0],i+3)]\n",
    "                neighborhood_5 = neighborhood_5[neighborhood_5.word.apply(lambda x: '#' not in x)]\n",
    "                local_words_5.append(list(neighborhood_5.word))\n",
    "                \n",
    "                neighborhood_7 = local[['word','tag']].iloc[max(0,i-3):min(local.shape[0],i+4)]\n",
    "                neighborhood_7 = neighborhood_7[neighborhood_7.word.apply(lambda x: '#' not in x)]\n",
    "                local_words_7.append(list(neighborhood_7.word))\n",
    "                \n",
    "                neighborhood_9 = local[['word','tag']].iloc[max(0,i-4):min(local.shape[0],i+5)]\n",
    "                neighborhood_9 = neighborhood_9[neighborhood_9.word.apply(lambda x: '#' not in x)]\n",
    "                local_words_9.append(list(neighborhood_9.word))\n",
    "                \n",
    "            \"\"\"If the word is a verb, add it to the bank of verbs\"\"\"\n",
    "            if (local.tag.iloc[i]=='V') and ('#' not in w) and ('&' not in w) and not(any(str(k) in w for k in range(10))) and ('-' not in w) and ('/' not in w):\n",
    "                if type(w)==list:\n",
    "                    unique_verbs.update(set(w))\n",
    "                else:\n",
    "                    unique_verbs.add(w)\n",
    "\n",
    "        actual_words_5.append(local_words_5)\n",
    "        actual_words_7.append(local_words_7)\n",
    "        actual_words_9.append(local_words_9)\n",
    "        \n",
    "    \"\"\"Check on length of neighborhoods'list\"\"\"\n",
    "    assert(len(actual_words_5)==k+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1cf07c501f14d9f9c8dc9fef9029294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11540), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Elementary cleaning step for normalization\"\"\"\n",
    "for k in tqdm(range(len(actual_words_5))):\n",
    "    actual_words_5[k] = [[w.replace(\"'\",'').replace(\"-\",'') for w in neighborhood] for neighborhood in actual_words_5[k]]\n",
    "    actual_words_7[k] = [[w.replace(\"'\",'').replace(\"-\",'') for w in neighborhood] for neighborhood in actual_words_7[k]]\n",
    "    actual_words_9[k] = [[w.replace(\"'\",'').replace(\"-\",'') for w in neighborhood] for neighborhood in actual_words_9[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Saving the neighborhoods\"\"\"\n",
    "pickle.dump(actual_words_5, open('./neighborhoods/actual_words_hashtag_free_5.p', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(actual_words_7, open('./neighborhoods/actual_words_hashtag_free_7.p', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "pickle.dump(actual_words_9, open('./neighborhoods/actual_words_hashtag_free_9.p', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3565849a9eb64009aac431d3931d6355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Adding the decomposition as neighborhoods to the previous one…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21aa979492f54962ba89a2eb2787310b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Removing leftover from tag file and removing underscores', ma…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Building the neighborhoods extracted from hashtag decomposition (obtained so far)\"\"\"\n",
    "tuple_hashtags = {ht: tuple(list(pd.DataFrame(tagged_hashtags[ht], columns=['word','tag','score'])['word'].apply(lambda x: x.replace(\"'\",'')))) for ht in tagged_hashtags.keys()}\n",
    "hashtag_words = []\n",
    "\n",
    "for k in tqdm(range(data.shape[0]), desc='Adding the decomposition as neighborhoods to the previous ones extracted'):\n",
    "    local = data.tag_df.iloc[k]\n",
    "    local_hashtag = []\n",
    "    for i in range(local.shape[0]):\n",
    "        if local.tag.iloc[i]=='#' or '#' in local.word.iloc[i]:\n",
    "            ht = local.word.iloc[i]\n",
    "            if ht in tuple_hashtags.keys():\n",
    "                local_hashtag.append(list(tuple_hashtags[ht]))\n",
    "    hashtag_words.append(local_hashtag)\n",
    "    \n",
    "for k in tqdm(range(len(hashtag_words)), desc='Removing leftover from tag file and removing underscores'):\n",
    "    hashtag_words[k] = [[w.replace(\"'\",'').replace(\"-\",'') for w in neighborhood] for neighborhood in hashtag_words[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(hashtag_words, open('./neighborhoods/hashtag_words.p','wb'), protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
