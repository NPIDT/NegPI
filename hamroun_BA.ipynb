{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ark_tweet_nlp import CMUTweetTagger\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "import pickle\n",
    "import copy\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processed data contains pre-processed text to search for patterns. The 'actual_words_5' contains 5-word neighborhoods, it is redundant as far as the text is concerned, however it contains hashtag decompositions. Which is why we also look into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loading the tagged data and the 5-words neighborhoods \"\"\"\n",
    "data = pickle.load(open('./processed_BA.b', 'rb'))\n",
    "actual_words_5, _, _ = pickle.load(open('./neighborhoods_ba.b', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Preparing the data\"\"\"\n",
    "data['tokens'] = data.text.apply(lambda x: word_tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd55d39b9c843e7afe8f0ebd1d14fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Going through the neighborhoods for 2 conditions', max=11684)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using 2 conditions : negative PI verb, negation word + positive PI verb with hashtags\n",
      "F1 score obtained through the fully supervised method : 0.6442775077330977\n",
      "Precision obtained through the fully supervised method : 0.6246786632390745\n",
      "Recall score obtained through the fully supervised method : 0.6651459854014599\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Pattern flagging\"\"\"\n",
    "\n",
    "\"\"\"Structuring the words into stand alone identifiers, negative paired identifiers, negative indicators, BA indicators, and other fluff\"\"\"\n",
    "stand_alone = ['boycott','bycott','boycot','bycot','avoid']\n",
    "neg_paired = ['fly','use','travel','choose','book','recommend']\n",
    "neg_ind = ['never','not','dont','shouldnt','no','wont','wouldnt']\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "\"\"\"Function to pick pattern\"\"\"\n",
    "def check_pattern(pat):\n",
    "    sa_ind = int(len(set(list(pat)).intersection(stand_alone))>0)\n",
    "    np_ind = int( (len(set(list(pat)).intersection(neg_paired))>0) & (len(set(list(pat)).intersection(neg_ind))>0))\n",
    "    never_again = int('never' in pat and 'again' in pat)\n",
    "    return [sa_ind, np_ind, never_again]\n",
    "\n",
    "ys = list(data.pi)\n",
    "\n",
    "pred_2 = []\n",
    "for k in tqdm(range(data.shape[0]), desc='Going through the neighborhoods for 2 conditions'):\n",
    "    label = False\n",
    "    for pat in actual_words_5[k]:\n",
    "        pf = check_pattern(pat)\n",
    "        \"\"\"Common conditions labeling\"\"\"\n",
    "        if (pf[0]==1 or pf[1]==1) and not label:\n",
    "            pred_2.append(1)\n",
    "            label = True\n",
    "            break\n",
    "    if label==False:\n",
    "        for i in range(len(data.tokens.iloc[k])):\n",
    "            if lmtzr.lemmatize(data.tokens.iloc[k][i],'v') in stand_alone:\n",
    "                label = True\n",
    "            if data.tokens.iloc[k][i] in neg_ind:\n",
    "                for j in range(i,min(len(data.tokens.iloc[k]),i+4)):\n",
    "                    if lmtzr.lemmatize(data.tokens.iloc[k][j],'v') in neg_paired:\n",
    "                        label = True\n",
    "                        break\n",
    "            if label:\n",
    "                pred_2.append(1)\n",
    "                break\n",
    "    \"\"\"Labeling as a non negative PI tweet if no condition is met\"\"\"\n",
    "    if not label:\n",
    "        pred_2.append(0)\n",
    "        \n",
    "print('Using 2 conditions : negative PI verb, negation word + positive PI verb with hashtags')\n",
    "print('F1 score obtained through the fully supervised method : {}'.format(f1_score(y_true=ys, y_pred=pred_2)))\n",
    "print('Precision obtained through the fully supervised method : {}'.format(precision_score(y_true=ys, y_pred=pred_2)))\n",
    "print('Recall score obtained through the fully supervised method : {}'.format(recall_score(y_true=ys, y_pred=pred_2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
